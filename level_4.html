<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Level 4: Data Management & Processing - Healthcare Industry Applications</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        :root {
            --primary-color: #0d6efd;
            --secondary-color: #6c757d;
            --success-color: #198754;
            --info-color: #0dcaf0;
            --warning-color: #ffc107;
            --danger-color: #dc3545;
            --light-color: #f8f9fa;
            --dark-color: #212529;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
        }

        .navbar {
            box-shadow: 0 2px 4px rgba(0,0,0,.1);
        }

        .hero-section {
            background: linear-gradient(135deg, #0d6efd, #6610f2);
            color: white;
            padding: 3rem 0;
            margin-bottom: 2rem;
        }

        .section-header {
            border-left: 4px solid var(--primary-color);
            padding-left: 1rem;
            margin-bottom: 1.5rem;
        }

        .component-card {
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            margin-bottom: 2rem;
            overflow: hidden;
        }

        .component-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0,0,0,.1);
        }

        .card-header {
            background-color: var(--primary-color);
            color: white;
            font-weight: 600;
            padding: 1rem 1.5rem;
        }

        .card-body {
            padding: 1.5rem;
        }

        .business-case {
            background-color: #e7f3ff;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
        }

        .problem-solved {
            background-color: #e7ffe7;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
        }

        .impact-absence {
            background-color: #fff0e7;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
        }

        .element-type {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.875rem;
            font-weight: 600;
            margin-right: 0.5rem;
        }

        .predefined {
            background-color: #d1ecf1;
            color: #0c5460;
        }

        .custom {
            background-color: #d4edda;
            color: #155724;
        }

        .relationship {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.875rem;
            font-weight: 600;
            margin-right: 0.5rem;
            background-color: #e2e3e5;
            color: #383d41;
        }

        pre {
            background-color: #2d2d2d;
            color: #f8f8f2;
            border-radius: 8px;
            padding: 1.5rem;
            overflow-x: auto;
            margin: 1.5rem 0;
        }

        code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
        }

        .code-explanation {
            background-color: #f8f9fa;
            border-left: 4px solid var(--primary-color);
            padding: 1rem;
            margin: 1rem 0;
        }

        .industry-example {
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border: 1px solid #dee2e6;
        }

        .industry-example h5 {
            color: var(--primary-color);
            margin-bottom: 1rem;
        }

        .toc {
            position: sticky;
            top: 100px;
            background-color: white;
            border-radius: 8px;
            padding: 1.5rem;
            box-shadow: 0 2px 4px rgba(0,0,0,.1);
        }

        .toc ul {
            list-style-type: none;
            padding-left: 0;
        }

        .toc li {
            margin-bottom: 0.5rem;
        }

        .toc a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.3s ease;
        }

        .toc a:hover {
            color: #0a58ca;
            text-decoration: underline;
        }

        .footer {
            background-color: var(--dark-color);
            color: white;
            padding: 2rem 0;
            margin-top: 3rem;
        }

        .highlight {
            background-color: #fff3cd;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
        }

        .table-of-contents {
            background-color: white;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,.1);
        }

        .nav-pills .nav-link.active {
            background-color: var(--primary-color);
        }

        .nav-pills .nav-link {
            color: var(--primary-color);
        }

        .nav-pills .nav-link:hover {
            background-color: rgba(13, 110, 253, 0.1);
        }

        .tech-detail {
            background-color: #f1f8ff;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #0366d6;
        }

        .business-implication {
            background-color: #fffde7;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #ffc107;
        }

        @media (max-width: 992px) {
            .toc {
                position: relative;
                top: 0;
                margin-bottom: 2rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark sticky-top">
        <div class="container">
            <a class="navbar-brand" href="#">Data Management & Processing</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="#data-ingestion">4.1 Data Ingestion</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#data-processing">4.2 Data Processing</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#data-transformation">4.3 Data Transformation</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="hero-section">
        <div class="container">
            <div class="row align-items-center">
                <div class="col-lg-8">
                    <h1 class="display-4 fw-bold">Level 4: Data Management & Processing</h1>
                    <p class="lead">Building on Levels 1-3 to handle and prepare your data for advanced healthcare applications</p>
                    <div class="mt-4">
                        <span class="badge bg-light text-dark me-2">Healthcare Industry</span>
                        <span class="badge bg-light text-dark me-2">Real-world Applications</span>
                        <span class="badge bg-light text-dark">Technical Implementation</span>
                    </div>
                </div>
                <div class="col-lg-4 text-center">
                    <i class="fas fa-database fa-10x opacity-75"></i>
                </div>
            </div>
        </div>
    </section>

    <div class="container">
        <div class="row">
            <!-- Table of Contents -->
            <div class="col-lg-3">
                <div class="toc">
                    <h5>Table of Contents</h5>
                    <ul>
                        <li><a href="#introduction">Introduction</a></li>
                        <li><a href="#data-ingestion">4.1 Data Ingestion</a>
                            <ul>
                                <li><a href="#document-loaders">Document Loaders</a></li>
                                <li><a href="#custom-document-loaders">Custom Document Loaders</a></li>
                            </ul>
                        </li>
                        <li><a href="#data-processing">4.2 Data Processing</a>
                            <ul>
                                <li><a href="#text-splitters">Text Splitters</a></li>
                                <li><a href="#message-processing">Message Processing</a></li>
                            </ul>
                        </li>
                        <li><a href="#data-transformation">4.3 Data Transformation</a>
                            <ul>
                                <li><a href="#embedding-models">Embedding Models</a></li>
                                <li><a href="#vector-representations">Vector Representations</a></li>
                                <li><a href="#embedding-caching">Embedding Caching</a></li>
                            </ul>
                        </li>
                        <li><a href="#conclusion">Conclusion</a></li>
                    </ul>
                </div>
            </div>

            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Introduction -->
                <section id="introduction" class="mb-5">
                    <h2 class="section-header">Introduction to Data Management & Processing in Healthcare</h2>
                    <p>Data Management & Processing forms the critical foundation for any advanced AI application in healthcare. In an industry where data comes in numerous formats, from structured patient records to unstructured clinical notes, medical images, and research papers, effective data handling is paramount. This level builds upon the foundational concepts from Levels 1-3, focusing specifically on how to ingest, process, and transform healthcare data to make it usable for AI applications.</p>
                    
                    <div class="industry-example">
                        <h5><i class="fas fa-hospital-alt me-2"></i>Healthcare Industry Context</h5>
                        <p>The healthcare industry generates enormous amounts of data daily. A single hospital might process thousands of patient records, lab results, imaging reports, and clinical notes. This data is often siloed across different departments and systems, stored in various formats, and subject to strict privacy regulations like HIPAA. Effective data management and processing enable healthcare organizations to:</p>
                        <ul>
                            <li>Consolidate patient information from disparate sources</li>
                            <li>Extract meaningful insights from unstructured clinical notes</li>
                            <li>Prepare data for AI-driven diagnostic tools</li>
                            <li>Enable research and population health analytics</li>
                            <li>Ensure compliance with data privacy regulations</li>
                        </ul>
                    </div>
                </section>

                <!-- 4.1 Data Ingestion -->
                <section id="data-ingestion" class="mb-5">
                    <h2 class="section-header">4.1 Data Ingestion</h2>
                    <p>Data ingestion is the first step in the data management pipeline, involving the acquisition and importation of data from various sources for processing or analysis. In healthcare, this means bringing together data from electronic health records (EHRs), medical imaging systems, lab information systems, and patient-generated data from wearables and other devices.</p>

                    <!-- Document Loaders -->
                    <div id="document-loaders" class="component-card">
                        <div class="card-header">
                            <h3 class="mb-0">Document Loaders</h3>
                        </div>
                        <div class="card-body">
                            <p>Document loaders are utilities designed to extract content from various file formats. In healthcare, they enable systems to read and process diverse document types containing patient information, clinical research, and medical knowledge.</p>
                            
                            <div class="business-case">
                                <h5><i class="fas fa-briefcase-medical me-2"></i>Detailed Business Use Cases</h5>
                                <p><strong>Patient Record Integration:</strong> A hospital network needs to consolidate patient records from multiple departments, including cardiology, radiology, and pathology. Each department stores patient information in different formats - PDF reports, CSV lab results, and HTML-formatted discharge summaries.</p>
                                
                                <p><strong>Clinical Research Analysis:</strong> A pharmaceutical company is conducting a multi-site clinical trial and needs to ingest research data from various hospitals, each providing data in different formats including JSON, Markdown, and Office documents.</p>
                                
                                <p><strong>Medical Knowledge Base Construction:</strong> A healthcare AI startup is building a knowledge base from medical literature, research papers, and clinical guidelines available in PDF, HTML, and Markdown formats.</p>
                            </div>
                            
                            <div class="problem-solved">
                                <h5><i class="fas fa-lightbulb me-2"></i>Problems Solved</h5>
                                <p>Document loaders address several critical challenges in healthcare data management:</p>
                                <ul>
                                    <li><strong>Data Silos:</strong> Healthcare data is often trapped in departmental systems with incompatible formats. Document loaders enable standardized extraction regardless of the source format.</li>
                                    <li><strong>Manual Data Entry:</strong> Without automated document loading, healthcare staff would need to manually transcribe information from one system to another, increasing the risk of errors and consuming valuable time.</li>
                                    <li><strong>Information Accessibility:</strong> Critical patient information might be inaccessible if it's locked in proprietary formats. Document loaders make this information available for analysis and decision support.</li>
                                </ul>
                            </div>
                            
                            <div class="impact-absence">
                                <h5><i class="fas fa-exclamation-triangle me-2"></i>Impact of Absence</h5>
                                <p>If document loaders didn't exist in healthcare systems:</p>
                                <ul>
                                    <li>Healthcare providers would spend excessive time manually transferring information between systems, reducing time available for patient care.</li>
                                    <li>Critical patient information might be overlooked if it's stored in a format that can't be easily accessed or integrated.</li>
                                    <li>Research and analytics initiatives would be severely hampered, as data scientists would need to develop custom extraction methods for each data source.</li>
                                    <li>Interoperability between healthcare systems would be nearly impossible, perpetuating the problem of fragmented patient records.</li>
                                </ul>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Element Types</h5>
                                <span class="element-type predefined">Predefined</span> <span class="element-type custom">Custom</span>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Relationship Keywords</h5>
                                <span class="relationship">Foundation</span> <span class="relationship">Interface</span>
                            </div>
                            
                            <div class="tech-detail">
                                <h5><i class="fas fa-cogs me-2"></i>In-depth Knowledge</h5>
                                <p>Document loaders use various techniques to extract content from different file formats:</p>
                                <ul>
                                    <li><strong>PDF Loaders:</strong> Use libraries like PyPDF2, pdfplumber, or PyMuPDF to extract text, handle complex layouts, and process both text-based and scanned PDFs (using OCR).</li>
                                    <li><strong>Web Loaders:</strong> Utilize tools like BeautifulSoup or Scrapy to parse HTML, extract relevant content, and handle JavaScript-rendered content.</li>
                                    <li><strong>CSV Loaders:</strong> Employ pandas or csv modules to read structured data, handle different delimiters, and manage encoding issues.</li>
                                    <li><strong>JSON Loaders:</strong> Parse JSON structures, handle nested objects, and convert to standardized formats.</li>
                                    <li><strong>Markdown Loaders:</strong> Extract content while preserving formatting, handle code blocks, and manage metadata.</li>
                                    <li><strong>Office File Loaders:</strong> Use libraries like python-docx, openpyxl, or python-pptx to extract content from Word, Excel, and PowerPoint files.</li>
                                </ul>
                            </div>
                            
                            <div class="business-implication">
                                <h5><i class="fas fa-chart-line me-2"></i>Business Implications</h5>
                                <p>Effective document loading capabilities enable healthcare organizations to:</p>
                                <ul>
                                    <li><strong>Reduce Operational Costs:</strong> Automating data extraction eliminates manual data entry, reducing labor costs and minimizing errors.</li>
                                    <li><strong>Improve Patient Care:</strong> Comprehensive patient records derived from multiple sources provide clinicians with a complete view of patient history.</li>
                                    <li><strong>Accelerate Research:</strong> Researchers can access and analyze data from multiple sources more quickly, speeding up medical discoveries.</li>
                                    <li><strong>Ensure Compliance:</strong> Standardized data extraction helps maintain data integrity and supports audit trails for regulatory compliance.</li>
                                </ul>
                            </div>
                            
                            <h5>Code Example</h5>
                            <pre><code class="language-python"># Document Loaders in Healthcare Context
from langchain.document_loaders import PyPDFLoader, CSVLoader, WebBaseLoader
from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredWordDocumentLoader
from langchain.document_loaders import JSONLoader
import os

# Example 1: Loading Patient Medical Records from PDF
def load_patient_medical_records(pdf_path):
    """
    Load patient medical records from PDF files
    
    Args:
        pdf_path: Path to the PDF file containing patient records
        
    Returns:
        List of Document objects containing the extracted text
    """
    try:
        # Initialize the PDF loader
        loader = PyPDFLoader(pdf_path)
        
        # Load the documents
        documents = loader.load()
        
        # Add metadata to identify the source
        for doc in documents:
            doc.metadata["source_type"] = "patient_medical_record"
            doc.metadata["source_file"] = os.path.basename(pdf_path)
            
        return documents
    except Exception as e:
        print(f"Error loading PDF: {e}")
        return []

# Example 2: Loading Lab Results from CSV
def load_lab_results(csv_path):
    """
    Load patient lab results from CSV files
    
    Args:
        csv_path: Path to the CSV file containing lab results
        
    Returns:
        List of Document objects containing the lab results
    """
    try:
        # Initialize the CSV loader
        loader = CSVLoader(csv_path)
        
        # Load the documents
        documents = loader.load()
        
        # Add metadata to identify the source
        for doc in documents:
            doc.metadata["source_type"] = "lab_results"
            doc.metadata["source_file"] = os.path.basename(csv_path)
            
        return documents
    except Exception as e:
        print(f"Error loading CSV: {e}")
        return []

# Example 3: Loading Clinical Guidelines from Web
def load_clinical_guidelines(urls):
    """
    Load clinical guidelines from web pages
    
    Args:
        urls: List of URLs containing clinical guidelines
        
    Returns:
        List of Document objects containing the extracted text
    """
    try:
        # Initialize the web loader
        loader = WebBaseLoader(urls)
        
        # Load the documents
        documents = loader.load()
        
        # Add metadata to identify the source
        for doc in documents:
            doc.metadata["source_type"] = "clinical_guidelines"
            
        return documents
    except Exception as e:
        print(f"Error loading web pages: {e}")
        return []

# Example 4: Loading Research Papers from Markdown
def load_research_papers(md_path):
    """
    Load medical research papers from Markdown files
    
    Args:
        md_path: Path to the Markdown file containing research papers
        
    Returns:
        List of Document objects containing the extracted text
    """
    try:
        # Initialize the Markdown loader
        loader = UnstructuredMarkdownLoader(md_path)
        
        # Load the documents
        documents = loader.load()
        
        # Add metadata to identify the source
        for doc in documents:
            doc.metadata["source_type"] = "research_paper"
            doc.metadata["source_file"] = os.path.basename(md_path)
            
        return documents
    except Exception as e:
        print(f"Error loading Markdown: {e}")
        return []

# Example 5: Loading Clinical Trial Data from JSON
def load_clinical_trial_data(json_path):
    """
    Load clinical trial data from JSON files
    
    Args:
        json_path: Path to the JSON file containing clinical trial data
        
    Returns:
        List of Document objects containing the extracted text
    """
    try:
        # Initialize the JSON loader
        loader = JSONLoader(
            json_path=json_path,
            jq_schema='.',  # Use jq syntax to extract specific parts of the JSON
            text_content=False  # Set to True if you want to extract only text content
        )
        
        # Load the documents
        documents = loader.load()
        
        # Add metadata to identify the source
        for doc in documents:
            doc.metadata["source_type"] = "clinical_trial_data"
            doc.metadata["source_file"] = os.path.basename(json_path)
            
        return documents
    except Exception as e:
        print(f"Error loading JSON: {e}")
        return []

# Example 6: Loading Discharge Summaries from Word Documents
def load_discharge_summaries(docx_path):
    """
    Load patient discharge summaries from Word documents
    
    Args:
        docx_path: Path to the Word document containing discharge summaries
        
    Returns:
        List of Document objects containing the extracted text
    """
    try:
        # Initialize the Word document loader
        loader = UnstructuredWordDocumentLoader(docx_path)
        
        # Load the documents
        documents = loader.load()
        
        # Add metadata to identify the source
        for doc in documents:
            doc.metadata["source_type"] = "discharge_summary"
            doc.metadata["source_file"] = os.path.basename(docx_path)
            
        return documents
    except Exception as e:
        print(f"Error loading Word document: {e}")
        return []

# Example 7: Consolidating Data from Multiple Sources
def consolidate_patient_data(patient_id):
    """
    Consolidate patient data from multiple sources
    
    Args:
        patient_id: The ID of the patient
        
    Returns:
        List of Document objects containing all patient data
    """
    all_documents = []
    
    # Define file paths based on patient ID
    pdf_path = f"data/patients/{patient_id}/medical_records.pdf"
    csv_path = f"data/patients/{patient_id}/lab_results.csv"
    docx_path = f"data/patients/{patient_id}/discharge_summary.docx"
    
    # Load data from each source
    if os.path.exists(pdf_path):
        medical_records = load_patient_medical_records(pdf_path)
        all_documents.extend(medical_records)
    
    if os.path.exists(csv_path):
        lab_results = load_lab_results(csv_path)
        all_documents.extend(lab_results)
    
    if os.path.exists(docx_path):
        discharge_summary = load_discharge_summaries(docx_path)
        all_documents.extend(discharge_summary)
    
    # Load clinical guidelines (same for all patients)
    guidelines_urls = [
        "https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance",
        "https://www.cdc.gov/coronavirus/2019-ncov/hcp/clinical-guidance-management-patients.html"
    ]
    guidelines = load_clinical_guidelines(guidelines_urls)
    all_documents.extend(guidelines)
    
    return all_documents

# Example usage
if __name__ == "__main__":
    # Load data for a specific patient
    patient_id = "PT-2023-0456"
    patient_data = consolidate_patient_data(patient_id)
    
    print(f"Loaded {len(patient_data)} documents for patient {patient_id}")
    
    # Print the first document as an example
    if patient_data:
        print("\nFirst document example:")
        print(f"Content type: {patient_data[0].metadata.get('source_type', 'Unknown')}")
        print(f"Content preview: {patient_data[0].page_content[:200]}...")
        print(f"Metadata: {patient_data[0].metadata}")
</code></pre>
                            
                            <div class="code-explanation">
                                <h5><i class="fas fa-code me-2"></i>Code Explanation</h5>
                                <p>This code demonstrates how document loaders can be used in a healthcare context to ingest data from various sources:</p>
                                <ol>
                                    <li><strong>PDF Loader:</strong> The <code>load_patient_medical_records</code> function uses PyPDFLoader to extract text from PDF files containing patient medical records. This is particularly useful for historical patient data that might be stored in PDF format.</li>
                                    <li><strong>CSV Loader:</strong> The <code>load_lab_results</code> function uses CSVLoader to import structured lab results from CSV files. This allows for easy integration with laboratory information systems.</li>
                                    <li><strong>Web Loader:</strong> The <code>load_clinical_guidelines</code> function uses WebBaseLoader to extract content from web pages containing clinical guidelines. This enables healthcare systems to stay updated with the latest medical knowledge.</li>
                                    <li><strong>Markdown Loader:</strong> The <code>load_research_papers</code> function uses UnstructuredMarkdownLoader to process research papers stored in Markdown format, which is common in academic and research settings.</li>
                                    <li><strong>JSON Loader:</strong> The <code>load_clinical_trial_data</code> function uses JSONLoader to import structured clinical trial data. The jq_schema parameter allows for precise extraction of specific data elements from complex JSON structures.</li>
                                    <li><strong>Word Document Loader:</strong> The <code>load_discharge_summaries</code> function uses UnstructuredWordDocumentLoader to process discharge summaries from Word documents, which is a common format in many healthcare settings.</li>
                                    <li><strong>Data Consolidation:</strong> The <code>consolidate_patient_data</code> function demonstrates how data from multiple sources can be combined to create a comprehensive patient record. This is crucial for providing clinicians with a complete view of a patient's medical history.</li>
                                </ol>
                                <p>Each loader function adds metadata to the extracted documents, identifying the source type and file name. This metadata is essential for tracking data provenance and ensuring proper handling of sensitive healthcare information.</p>
                            </div>
                            
                            <div class="industry-example">
                                <h5><i class="fas fa-hospital-alt me-2"></i>Real-world Healthcare Scenario</h5>
                                <p>Consider a regional hospital system that has recently acquired several smaller clinics. Each clinic uses different systems for storing patient information:</p>
                                <ul>
                                    <li>Clinic A uses a legacy system that exports patient records as PDF files</li>
                                    <li>Clinic B uses a modern EHR that can export data as CSV files</li>
                                    <li>Clinic C uses a cloud-based system that provides data through a web API</li>
                                </ul>
                                <p>The hospital needs to consolidate all this data into a unified system to provide seamless care to patients. Using document loaders, the hospital can:</p>
                                <ol>
                                    <li>Automatically ingest patient records from Clinic A's PDF exports</li>
                                    <li>Import lab results and structured data from Clinic B's CSV exports</li>
                                    <li>Extract data from Clinic C's web-based system using the web loader</li>
                                    <li>Combine all this data with the hospital's existing patient records</li>
                                    <li>Ensure that all data is properly tagged with metadata for audit and compliance purposes</li>
                                </ol>
                                <p>This approach eliminates the need for manual data entry, reduces errors, and provides clinicians with a comprehensive view of patient information regardless of which clinic originally generated the data.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Custom Document Loaders -->
                    <div id="custom-document-loaders" class="component-card">
                        <div class="card-header">
                            <h3 class="mb-0">Custom Document Loaders</h3>
                        </div>
                        <div class="card-body">
                            <p>While predefined document loaders handle common file formats, healthcare organizations often need to work with specialized or proprietary formats. Custom document loaders allow for the creation of tailored solutions to extract data from these unique sources.</p>
                            
                            <div class="business-case">
                                <h5><i class="fas fa-briefcase-medical me-2"></i>Detailed Business Use Cases</h5>
                                <p><strong>Legacy EHR Integration:</strong> A hospital uses an older electronic health record system that stores data in a proprietary binary format. A custom document loader is needed to extract patient information from this system.</p>
                                
                                <p><strong>Medical Imaging Reports:</strong> A radiology department stores imaging reports in a specialized format that combines structured data with narrative text. A custom loader is required to parse both elements correctly.</p>
                                
                                <p><strong>Wearable Device Data:</strong> A healthcare provider is integrating data from various patient wearable devices, each with its own data format. Custom loaders are needed to standardize this data for analysis.</p>
                                
                                <p><strong>Telehealth Session Records:</strong> A telemedicine platform stores session data in a unique XML format that includes video transcripts, vital signs, and clinical notes. A custom loader is needed to extract and structure this information.</p>
                            </div>
                            
                            <div class="problem-solved">
                                <h5><i class="fas fa-lightbulb me-2"></i>Problems Solved</h5>
                                <p>Custom document loaders address several specific challenges in healthcare data management:</p>
                                <ul>
                                    <li><strong>Proprietary Formats:</strong> Many healthcare systems use proprietary data formats that standard loaders cannot process. Custom loaders provide a bridge to these systems.</li>
                                    <li><strong>Specialized Data Structures:</strong> Healthcare data often has complex structures that combine multiple types of information. Custom loaders can be designed to understand and extract these structures correctly.</li>
                                    <li><strong>Legacy System Integration:</strong> Older healthcare systems may not support modern data export formats. Custom loaders enable integration with these legacy systems.</li>
                                    <li><strong>Regulatory Requirements:</strong> Healthcare data must be handled according to strict regulations. Custom loaders can incorporate necessary compliance checks and data handling procedures.</li>
                                </ul>
                            </div>
                            
                            <div class="impact-absence">
                                <h5><i class="fas fa-exclamation-triangle me-2"></i>Impact of Absence</h5>
                                <p>If custom document loaders didn't exist in healthcare systems:</p>
                                <ul>
                                    <li>Healthcare organizations would be locked into using only systems with standard data formats, limiting their ability to integrate with specialized or legacy systems.</li>
                                    <li>Valuable patient data stored in proprietary formats would remain inaccessible, leading to incomplete patient records and potentially affecting clinical decisions.</li>
                                    <li>Integration with new technologies like wearable devices or telehealth platforms would require significant custom development for each new device or platform.</li>
                                    <li>Healthcare providers would need to maintain manual processes for handling data from non-standard sources, increasing costs and the risk of errors.</li>
                                </ul>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Element Types</h5>
                                <span class="element-type custom">Custom</span>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Relationship Keywords</h5>
                                <span class="relationship">Interface</span> <span class="relationship">Foundation</span>
                            </div>
                            
                            <div class="tech-detail">
                                <h5><i class="fas fa-cogs me-2"></i>In-depth Knowledge</h5>
                                <p>Custom document loaders are typically built by extending base loader classes in frameworks like LangChain. They involve:</p>
                                <ul>
                                    <li><strong>Format Analysis:</strong> Understanding the structure and encoding of the target data format, which may require reverse engineering proprietary formats or working with vendor documentation.</li>
                                    <li><strong>Parsing Logic:</strong> Developing algorithms to extract meaningful content from the data structure, which may involve regular expressions, XML/HTML parsing, or binary data manipulation.</li>
                                    <li><strong>Metadata Extraction:</strong> Identifying and extracting relevant metadata from the source, such as patient identifiers, timestamps, or data provenance information.</li>
                                    <li><strong>Error Handling:</strong> Implementing robust error handling to manage malformed data, missing fields, or other data quality issues.</li>
                                    <li><strong>Compliance Integration:</strong> Incorporating checks for sensitive data (PHI) and ensuring proper handling according to regulations like HIPAA.</li>
                                </ul>
                                <p>Custom loaders often require collaboration between data engineers, who understand the technical aspects of data extraction, and domain experts, who understand the clinical significance of the data elements.</p>
                            </div>
                            
                            <div class="business-implication">
                                <h5><i class="fas fa-chart-line me-2"></i>Business Implications</h5>
                                <p>Custom document loaders enable healthcare organizations to:</p>
                                <ul>
                                    <li><strong>Extend System Lifespan:</strong> Continue using legacy systems that still provide value, even if they use proprietary data formats.</li>
                                    <li><strong>Innovate with New Technologies:</strong> Integrate cutting-edge devices and platforms without waiting for standard data formats to emerge.</li>
                                    <li><strong>Improve Data Completeness:</strong> Access data that would otherwise be locked in proprietary systems, leading to more comprehensive patient records.</li>
                                    <li><strong>Reduce Vendor Lock-in:</strong> Maintain flexibility in choosing healthcare IT solutions by not being limited to systems with standard export capabilities.</li>
                                </ul>
                            </div>
                            
                            <h5>Code Example</h5>
                            <pre><code class="language-python"># Custom Document Loaders for Healthcare Applications
from langchain.document_loaders.base import BaseLoader
from langchain.schema import Document
from typing import List, Optional, Dict, Any
import json
import xml.etree.ElementTree as ET
import re
import struct
import pandas as pd
from datetime import datetime
import base64
import binascii

# Example 1: Custom Loader for Legacy EHR System
class LegacyEHRLoader(BaseLoader):
    """
    Custom document loader for a legacy Electronic Health Record (EHR) system
    that stores data in a proprietary binary format.
    """
    
    def __init__(self, file_path: str):
        """
        Initialize the loader with the path to the legacy EHR file.
        
        Args:
            file_path: Path to the legacy EHR file
        """
        self.file_path = file_path
    
    def load(self) -> List[Document]:
        """
        Load and parse the legacy EHR file.
        
        Returns:
            List of Document objects containing patient data
        """
        documents = []
        
        try:
            with open(self.file_path, 'rb') as f:
                # Read the file header to understand the structure
                header = f.read(16)
                
                # Parse the header (this is a simplified example)
                # In a real implementation, this would be based on the actual format specification
                magic_number, version, record_count = struct.unpack('4sII', header)
                
                # Verify this is the correct file type
                if magic_number != b'EHR\x00':
                    raise ValueError("Not a valid legacy EHR file")
                
                # Read each record
                for _ in range(record_count):
                    # Read record header
                    record_header = f.read(12)
                    record_type, patient_id_length, data_length = struct.unpack('I4xII', record_header)
                    
                    # Read patient ID
                    patient_id = f.read(patient_id_length).decode('utf-8')
                    
                    # Read record data
                    record_data = f.read(data_length)
                    
                    # Parse the record data based on the record type
                    if record_type == 1:  # Patient demographics
                        content = self._parse_demographics(record_data)
                        doc_type = "patient_demographics"
                    elif record_type == 2:  # Clinical notes
                        content = self._parse_clinical_notes(record_data)
                        doc_type = "clinical_notes"
                    elif record_type == 3:  # Medications
                        content = self._parse_medications(record_data)
                        doc_type = "medications"
                    else:
                        # Skip unknown record types
                        continue
                    
                    # Create a document
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": self.file_path,
                            "patient_id": patient_id,
                            "record_type": doc_type,
                            "loader": "LegacyEHRLoader"
                        }
                    )
                    documents.append(doc)
                    
        except Exception as e:
            print(f"Error loading legacy EHR file: {e}")
            
        return documents
    
    def _parse_demographics(self, data: bytes) -> str:
        """Parse patient demographics data from binary format."""
        # This is a simplified example - real implementation would match the actual format
        fields = data.split(b'\x00')
        if len(fields) >= 4:
            name = fields[0].decode('utf-8')
            dob = fields[1].decode('utf-8')
            gender = fields[2].decode('utf-8')
            address = fields[3].decode('utf-8')
            
            return f"Patient: {name}\nDate of Birth: {dob}\nGender: {gender}\nAddress: {address}"
        return "Invalid demographics data"
    
    def _parse_clinical_notes(self, data: bytes) -> str:
        """Parse clinical notes data from binary format."""
        # In a real implementation, this would handle the specific binary format
        try:
            return data.decode('utf-8')
        except UnicodeDecodeError:
            # Handle cases where the data might be encoded differently
            return data.decode('latin-1')
    
    def _parse_medications(self, data: bytes) -> str:
        """Parse medications data from binary format."""
        # This is a simplified example - real implementation would match the actual format
        try:
            # Assume the data is JSON for this example
            return json.loads(data.decode('utf-8'))
        except:
            return "Invalid medications data"

# Example 2: Custom Loader for Medical Imaging Reports
class ImagingReportLoader(BaseLoader):
    """
    Custom document loader for medical imaging reports in a specialized format
    that combines structured data with narrative text.
    """
    
    def __init__(self, file_path: str):
        """
        Initialize the loader with the path to the imaging report file.
        
        Args:
            file_path: Path to the imaging report file
        """
        self.file_path = file_path
    
    def load(self) -> List[Document]:
        """
        Load and parse the imaging report file.
        
        Returns:
            List of Document objects containing imaging report data
        """
        documents = []
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Parse the specialized format (this is a simplified example)
            # In a real implementation, this would match the actual format specification
            
            # Extract structured data and narrative text
            structured_data = {}
            narrative_text = ""
            
            # Use regex to extract structured fields
            patterns = {
                "patient_id": r"Patient ID:\s*(\w+)",
                "exam_date": r"Exam Date:\s*(\d{4}-\d{2}-\d{2})",
                "modality": r"Modality:\s*([A-Z]+)",
                "body_part": r"Body Part:\s*([A-Za-z\s]+)",
                "radiologist": r"Radiologist:\s*([A-Za-z\s,]+)"
            }
            
            for field, pattern in patterns.items():
                match = re.search(pattern, content)
                if match:
                    structured_data[field] = match.group(1)
            
            # Extract narrative text (everything after "FINDINGS:")
            findings_match = re.search(r"FINDINGS:\s*(.*)", content, re.DOTALL)
            if findings_match:
                narrative_text = findings_match.group(1)
            
            # Create a document with both structured data and narrative text
            doc_content = f"Imaging Report\n\n"
            doc_content += f"Patient ID: {structured_data.get('patient_id', 'Unknown')}\n"
            doc_content += f"Exam Date: {structured_data.get('exam_date', 'Unknown')}\n"
            doc_content += f"Modality: {structured_data.get('modality', 'Unknown')}\n"
            doc_content += f"Body Part: {structured_data.get('body_part', 'Unknown')}\n"
            doc_content += f"Radiologist: {structured_data.get('radiologist', 'Unknown')}\n\n"
            doc_content += "FINDINGS:\n"
            doc_content += narrative_text
            
            doc = Document(
                page_content=doc_content,
                metadata={
                    "source": self.file_path,
                    "patient_id": structured_data.get("patient_id"),
                    "exam_date": structured_data.get("exam_date"),
                    "modality": structured_data.get("modality"),
                    "body_part": structured_data.get("body_part"),
                    "radiologist": structured_data.get("radiologist"),
                    "loader": "ImagingReportLoader"
                }
            )
            documents.append(doc)
            
        except Exception as e:
            print(f"Error loading imaging report: {e}")
            
        return documents

# Example 3: Custom Loader for Wearable Device Data
class WearableDeviceLoader(BaseLoader):
    """
    Custom document loader for data from wearable health devices.
    Handles multiple device types and data formats.
    """
    
    def __init__(self, file_path: str, device_type: str):
        """
        Initialize the loader with the path to the wearable device data file
        and the type of device.
        
        Args:
            file_path: Path to the wearable device data file
            device_type: Type of device (e.g., "fitness_tracker", "glucose_monitor", "ecg_monitor")
        """
        self.file_path = file_path
        self.device_type = device_type
    
    def load(self) -> List[Document]:
        """
        Load and parse the wearable device data file.
        
        Returns:
            List of Document objects containing wearable device data
        """
        documents = []
        
        try:
            if self.device_type == "fitness_tracker":
                documents = self._load_fitness_tracker_data()
            elif self.device_type == "glucose_monitor":
                documents = self._load_glucose_monitor_data()
            elif self.device_type == "ecg_monitor":
                documents = self._load_ecg_monitor_data()
            else:
                print(f"Unsupported device type: {self.device_type}")
                
        except Exception as e:
            print(f"Error loading wearable device data: {e}")
            
        return documents
    
    def _load_fitness_tracker_data(self) -> List[Document]:
        """Load data from a fitness tracker device."""
        documents = []
        
        # Assume the data is in a CSV format with specific columns
        try:
            df = pd.read_csv(self.file_path)
            
            # Group data by patient and date
            for (patient_id, date), group in df.groupby(['patient_id', 'date']):
                # Create a summary of the day's activity
                steps = group['steps'].sum() if 'steps' in group.columns else 0
                distance = group['distance'].sum() if 'distance' in group.columns else 0
                calories = group['calories'].sum() if 'calories' in group.columns else 0
                
                # Calculate average heart rate if available
                avg_heart_rate = 0
                if 'heart_rate' in group.columns:
                    avg_heart_rate = group['heart_rate'].mean()
                
                # Create content for the document
                content = f"Fitness Tracker Data for Patient {patient_id} on {date}\n\n"
                content += f"Total Steps: {steps}\n"
                content += f"Total Distance: {distance} km\n"
                content += f"Total Calories: {calories}\n"
                content += f"Average Heart Rate: {avg_heart_rate:.1f} bpm\n"
                
                # Add detailed data if available
                if len(group) > 1:
                    content += "\nDetailed Activity:\n"
                    for _, row in group.iterrows():
                        content += f"{row['time']}: "
                        if 'steps' in row:
                            content += f"Steps: {row['steps']}, "
                        if 'distance' in row:
                            content += f"Distance: {row['distance']} km, "
                        if 'heart_rate' in row:
                            content += f"Heart Rate: {row['heart_rate']} bpm, "
                        if 'calories' in row:
                            content += f"Calories: {row['calories']}, "
                        content += "\n"
                
                doc = Document(
                    page_content=content,
                    metadata={
                        "source": self.file_path,
                        "patient_id": patient_id,
                        "date": date,
                        "device_type": "fitness_tracker",
                        "total_steps": steps,
                        "total_distance": distance,
                        "total_calories": calories,
                        "avg_heart_rate": avg_heart_rate,
                        "loader": "WearableDeviceLoader"
                    }
                )
                documents.append(doc)
                
        except Exception as e:
            print(f"Error loading fitness tracker data: {e}")
            
        return documents
    
    def _load_glucose_monitor_data(self) -> List[Document]:
        """Load data from a glucose monitor device."""
        documents = []
        
        # Assume the data is in a JSON format
        try:
            with open(self.file_path, 'r') as f:
                data = json.load(f)
            
            # Group data by patient and date
            patient_data = {}
            for reading in data.get('readings', []):
                patient_id = reading.get('patient_id')
                timestamp = reading.get('timestamp')
                
                if patient_id not in patient_data:
                    patient_data[patient_id] = {}
                
                # Extract date from timestamp
                date = timestamp.split('T')[0]
                
                if date not in patient_data[patient_id]:
                    patient_data[patient_id][date] = []
                
                patient_data[patient_id][date].append(reading)
            
            # Create a document for each patient and date
            for patient_id, dates in patient_data.items():
                for date, readings in dates.items():
                    # Calculate statistics
                    glucose_values = [r.get('glucose_value') for r in readings]
                    avg_glucose = sum(glucose_values) / len(glucose_values) if glucose_values else 0
                    min_glucose = min(glucose_values) if glucose_values else 0
                    max_glucose = max(glucose_values) if glucose_values else 0
                    
                    # Create content for the document
                    content = f"Glucose Monitor Data for Patient {patient_id} on {date}\n\n"
                    content += f"Average Glucose: {avg_glucose:.1f} mg/dL\n"
                    content += f"Minimum Glucose: {min_glucose} mg/dL\n"
                    content += f"Maximum Glucose: {max_glucose} mg/dL\n"
                    content += f"Number of Readings: {len(readings)}\n\n"
                    
                    content += "Detailed Readings:\n"
                    for reading in readings:
                        timestamp = reading.get('timestamp')
                        glucose_value = reading.get('glucose_value')
                        notes = reading.get('notes', '')
                        
                        content += f"{timestamp}: {glucose_value} mg/dL"
                        if notes:
                            content += f" - {notes}"
                        content += "\n"
                    
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": self.file_path,
                            "patient_id": patient_id,
                            "date": date,
                            "device_type": "glucose_monitor",
                            "avg_glucose": avg_glucose,
                            "min_glucose": min_glucose,
                            "max_glucose": max_glucose,
                            "num_readings": len(readings),
                            "loader": "WearableDeviceLoader"
                        }
                    )
                    documents.append(doc)
                    
        except Exception as e:
            print(f"Error loading glucose monitor data: {e}")
            
        return documents
    
    def _load_ecg_monitor_data(self) -> List[Document]:
        """Load data from an ECG monitor device."""
        documents = []
        
        # Assume the data is in a binary format with a specific structure
        try:
            with open(self.file_path, 'rb') as f:
                # Read the file header
                header = f.read(20)
                magic_number, version, num_records = struct.unpack('4sII', header)
                
                # Verify this is the correct file type
                if magic_number != b'ECG\x00':
                    raise ValueError("Not a valid ECG monitor file")
                
                # Read each record
                for _ in range(num_records):
                    # Read record header
                    record_header = f.read(16)
                    patient_id_length, timestamp_length, data_length = struct.unpack('III4x', record_header)
                    
                    # Read patient ID
                    patient_id = f.read(patient_id_length).decode('utf-8')
                    
                    # Read timestamp
                    timestamp = f.read(timestamp_length).decode('utf-8')
                    
                    # Read ECG data
                    ecg_data = f.read(data_length)
                    
                    # Decode the ECG data (this is a simplified example)
                    # In a real implementation, this would handle the specific encoding
                    try:
                        # Assume the data is base64 encoded
                        decoded_data = base64.b64decode(ecg_data)
                        
                        # Convert to a list of integers representing the ECG signal
                        ecg_values = list(struct.unpack(f'{len(decoded_data)//2}h', decoded_data))
                        
                        # Calculate basic statistics
                        avg_value = sum(ecg_values) / len(ecg_values) if ecg_values else 0
                        min_value = min(ecg_values) if ecg_values else 0
                        max_value = max(ecg_values) if ecg_values else 0
                        
                        # Create content for the document
                        content = f"ECG Monitor Data for Patient {patient_id}\n"
                        content += f"Timestamp: {timestamp}\n\n"
                        content += f"Signal Statistics:\n"
                        content += f"Average Value: {avg_value:.2f}\n"
                        content += f"Minimum Value: {min_value}\n"
                        content += f"Maximum Value: {max_value}\n"
                        content += f"Number of Samples: {len(ecg_values)}\n\n"
                        
                        # Include a sample of the ECG data (first 100 values)
                        content += "ECG Signal Sample (first 100 values):\n"
                        for i, value in enumerate(ecg_values[:100]):
                            content += f"{i}: {value}\n"
                        
                        # Extract date from timestamp for metadata
                        date = timestamp.split('T')[0]
                        
                        doc = Document(
                            page_content=content,
                            metadata={
                                "source": self.file_path,
                                "patient_id": patient_id,
                                "timestamp": timestamp,
                                "date": date,
                                "device_type": "ecg_monitor",
                                "avg_value": avg_value,
                                "min_value": min_value,
                                "max_value": max_value,
                                "num_samples": len(ecg_values),
                                "loader": "WearableDeviceLoader"
                            }
                        )
                        documents.append(doc)
                        
                    except Exception as e:
                        print(f"Error decoding ECG data: {e}")
                        continue
                    
        except Exception as e:
            print(f"Error loading ECG monitor data: {e}")
            
        return documents

# Example 4: Custom Loader for Telehealth Session Records
class TelehealthSessionLoader(BaseLoader):
    """
    Custom document loader for telehealth session records in a specialized XML format
    that includes video transcripts, vital signs, and clinical notes.
    """
    
    def __init__(self, file_path: str):
        """
        Initialize the loader with the path to the telehealth session record file.
        
        Args:
            file_path: Path to the telehealth session record file
        """
        self.file_path = file_path
    
    def load(self) -> List[Document]:
        """
        Load and parse the telehealth session record file.
        
        Returns:
            List of Document objects containing telehealth session data
        """
        documents = []
        
        try:
            # Parse the XML file
            tree = ET.parse(self.file_path)
            root = tree.getroot()
            
            # Extract session information
            session_id = root.findtext('session_id')
            patient_id = root.findtext('patient_id')
            provider_id = root.findtext('provider_id')
            start_time = root.findtext('start_time')
            end_time = root.findtext('end_time')
            
            # Extract transcript
            transcript_element = root.find('transcript')
            transcript = ""
            if transcript_element is not None:
                for utterance in transcript_element.findall('utterance'):
                    speaker = utterance.get('speaker')
                    text = utterance.text or ""
                    timestamp = utterance.get('timestamp')
                    transcript += f"[{timestamp}] {speaker}: {text}\n"
            
            # Extract vital signs
            vital_signs = {}
            vital_signs_element = root.find('vital_signs')
            if vital_signs_element is not None:
                for vital in vital_signs_element:
                    vital_signs[vital.tag] = vital.text
            
            # Extract clinical notes
            clinical_notes = root.findtext('clinical_notes', "")
            
            # Create content for the document
            content = f"Telehealth Session Record\n\n"
            content += f"Session ID: {session_id}\n"
            content += f"Patient ID: {patient_id}\n"
            content += f"Provider ID: {provider_id}\n"
            content += f"Start Time: {start_time}\n"
            content += f"End Time: {end_time}\n\n"
            
            content += "Vital Signs:\n"
            for vital, value in vital_signs.items():
                content += f"{vital.replace('_', ' ').title()}: {value}\n"
            
            content += "\nTranscript:\n"
            content += transcript
            
            content += "\nClinical Notes:\n"
            content += clinical_notes
            
            # Extract date from start_time for metadata
            date = start_time.split('T')[0] if start_time else "Unknown"
            
            doc = Document(
                page_content=content,
                metadata={
                    "source": self.file_path,
                    "session_id": session_id,
                    "patient_id": patient_id,
                    "provider_id": provider_id,
                    "start_time": start_time,
                    "end_time": end_time,
                    "date": date,
                    "vital_signs": vital_signs,
                    "loader": "TelehealthSessionLoader"
                }
            )
            documents.append(doc)
            
        except Exception as e:
            print(f"Error loading telehealth session record: {e}")
            
        return documents

# Example 5: Custom Loader for Pharmacy Prescription Data
class PharmacyPrescriptionLoader(BaseLoader):
    """
    Custom document loader for pharmacy prescription data in a specialized format
    that includes medication details, dosage instructions, and prescribing information.
    """
    
    def __init__(self, file_path: str):
        """
        Initialize the loader with the path to the pharmacy prescription data file.
        
        Args:
            file_path: Path to the pharmacy prescription data file
        """
        self.file_path = file_path
    
    def load(self) -> List[Document]:
        """
        Load and parse the pharmacy prescription data file.
        
        Returns:
            List of Document objects containing pharmacy prescription data
        """
        documents = []
        
        try:
            # Assume the data is in a delimited text format with specific sections
            with open(self.file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Split into sections
            sections = re.split(r'---+SECTION---+', content)
            
            # Process each section
            for section in sections:
                if not section.strip():
                    continue
                
                # Extract prescription details
                prescription = {}
                
                # Use regex to extract fields
                patterns = {
                    "prescription_id": r"Prescription ID:\s*(\w+)",
                    "patient_id": r"Patient ID:\s*(\w+)",
                    "provider_id": r"Provider ID:\s*(\w+)",
                    "medication_name": r"Medication Name:\s*([A-Za-z0-9\s\-]+)",
                    "strength": r"Strength:\s*([A-Za-z0-9\s\-]+)",
                    "dosage_form": r"Dosage Form:\s*([A-Za-z\s]+)",
                    "quantity": r"Quantity:\s*(\d+)",
                    "refills": r"Refills:\s*(\d+)",
                    "date_prescribed": r"Date Prescribed:\s*(\d{4}-\d{2}-\d{2})",
                    "instructions": r"Instructions:\s*(.*?)\n\n",
                    "notes": r"Notes:\s*(.*?)$"
                }
                
                for field, pattern in patterns.items():
                    match = re.search(pattern, section, re.DOTALL)
                    if match:
                        prescription[field] = match.group(1).strip()
                
                # Create content for the document
                doc_content = f"Pharmacy Prescription\n\n"
                doc_content += f"Prescription ID: {prescription.get('prescription_id', 'Unknown')}\n"
                doc_content += f"Patient ID: {prescription.get('patient_id', 'Unknown')}\n"
                doc_content += f"Provider ID: {prescription.get('provider_id', 'Unknown')}\n"
                doc_content += f"Medication: {prescription.get('medication_name', 'Unknown')}\n"
                doc_content += f"Strength: {prescription.get('strength', 'Unknown')}\n"
                doc_content += f"Dosage Form: {prescription.get('dosage_form', 'Unknown')}\n"
                doc_content += f"Quantity: {prescription.get('quantity', 'Unknown')}\n"
                doc_content += f"Refills: {prescription.get('refills', 'Unknown')}\n"
                doc_content += f"Date Prescribed: {prescription.get('date_prescribed', 'Unknown')}\n\n"
                
                doc_content += "Instructions:\n"
                doc_content += f"{prescription.get('instructions', 'No instructions provided')}\n\n"
                
                doc_content += "Notes:\n"
                doc_content += f"{prescription.get('notes', 'No notes')}\n"
                
                # Extract date from date_prescribed for metadata
                date = prescription.get('date_prescribed', 'Unknown')
                
                doc = Document(
                    page_content=doc_content,
                    metadata={
                        "source": self.file_path,
                        "prescription_id": prescription.get("prescription_id"),
                        "patient_id": prescription.get("patient_id"),
                        "provider_id": prescription.get("provider_id"),
                        "medication_name": prescription.get("medication_name"),
                        "strength": prescription.get("strength"),
                        "dosage_form": prescription.get("dosage_form"),
                        "quantity": prescription.get("quantity"),
                        "refills": prescription.get("refills"),
                        "date_prescribed": prescription.get("date_prescribed"),
                        "date": date,
                        "loader": "PharmacyPrescriptionLoader"
                    }
                )
                documents.append(doc)
                
        except Exception as e:
            print(f"Error loading pharmacy prescription data: {e}")
            
        return documents

# Example usage
if __name__ == "__main__":
    # Example 1: Load data from a legacy EHR system
    legacy_ehr_loader = LegacyEHRLoader("data/legacy_ehr/Patient123.bin")
    legacy_documents = legacy_ehr_loader.load()
    print(f"Loaded {len(legacy_documents)} documents from legacy EHR system")
    
    # Example 2: Load imaging reports
    imaging_loader = ImagingReportLoader("data/imaging/CT_Scan_Report.txt")
    imaging_documents = imaging_loader.load()
    print(f"Loaded {len(imaging_documents)} imaging reports")
    
    # Example 3: Load wearable device data
    fitness_loader = WearableDeviceLoader("data/wearable/fitness_tracker.csv", "fitness_tracker")
    fitness_documents = fitness_loader.load()
    print(f"Loaded {len(fitness_documents)} fitness tracker documents")
    
    glucose_loader = WearableDeviceLoader("data/wearable/glucose_monitor.json", "glucose_monitor")
    glucose_documents = glucose_loader.load()
    print(f"Loaded {len(glucose_documents)} glucose monitor documents")
    
    ecg_loader = WearableDeviceLoader("data/wearable/ecg_monitor.dat", "ecg_monitor")
    ecg_documents = ecg_loader.load()
    print(f"Loaded {len(ecg_documents)} ECG monitor documents")
    
    # Example 4: Load telehealth session records
    telehealth_loader = TelehealthSessionLoader("data/telehealth/session_456.xml")
    telehealth_documents = telehealth_loader.load()
    print(f"Loaded {len(telehealth_documents)} telehealth session records")
    
    # Example 5: Load pharmacy prescription data
    pharmacy_loader = PharmacyPrescriptionLoader("data/pharmacy/prescriptions.txt")
    pharmacy_documents = pharmacy_loader.load()
    print(f"Loaded {len(pharmacy_documents)} pharmacy prescriptions")
</code></pre>
                            
                            <div class="code-explanation">
                                <h5><i class="fas fa-code me-2"></i>Code Explanation</h5>
                                <p>This code demonstrates several custom document loaders designed for healthcare applications:</p>
                                <ol>
                                    <li><strong>LegacyEHRLoader:</strong> This loader handles data from a legacy electronic health record system that stores data in a proprietary binary format. It reads the file header to understand the structure, then processes each record based on its type (demographics, clinical notes, or medications). The loader includes specific parsing methods for each record type, demonstrating how to handle binary data formats.</li>
                                    <li><strong>ImagingReportLoader:</strong> This loader processes medical imaging reports that combine structured data with narrative text. It uses regular expressions to extract structured fields (patient ID, exam date, modality, etc.) and separates the narrative findings section. This approach is common when dealing with semi-structured healthcare documents.</li>
                                    <li><strong>WearableDeviceLoader:</strong> This is a versatile loader that handles data from different types of wearable health devices. It includes specific methods for fitness trackers, glucose monitors, and ECG monitors, each with their own data formats and processing requirements. The loader demonstrates how to handle CSV, JSON, and binary data formats from different device types.</li>
                                    <li><strong>TelehealthSessionLoader:</strong> This loader processes telehealth session records stored in XML format. It extracts session information, conversation transcripts, vital signs, and clinical notes. The XML parsing approach demonstrates how to handle structured healthcare data with nested elements.</li>
                                    <li><strong>PharmacyPrescriptionLoader:</strong> This loader handles pharmacy prescription data in a delimited text format with specific sections. It uses regular expressions to extract various fields and creates comprehensive documents with all prescription details.</li>
                                </ol>
                                <p>Each custom loader extends the BaseLoader class from LangChain and implements the required load() method. They all add metadata to the extracted documents, including source information, patient identifiers, dates, and loader type. This metadata is crucial for data provenance and proper handling of healthcare information.</p>
                                <p>The loaders also demonstrate robust error handling, which is essential when dealing with real-world healthcare data that may be incomplete, malformed, or in unexpected formats.</p>
                            </div>
                            
                            <div class="industry-example">
                                <h5><i class="fas fa-hospital-alt me-2"></i>Real-world Healthcare Scenario</h5>
                                <p>Consider a large healthcare system that is implementing a comprehensive patient data platform. The system needs to integrate data from multiple sources, including:</p>
                                <ul>
                                    <li>A legacy EHR system that's been in use for 20 years and stores data in a proprietary binary format</li>
                                    <li>Various medical imaging systems that generate reports in specialized formats</li>
                                    <li>Patient wearable devices from multiple manufacturers, each with their own data formats</li>
                                    <li>A telehealth platform that stores session data in XML format</li>
                                    <li>Pharmacy systems that provide prescription data in a custom text format</li>
                                </ul>
                                <p>Without custom document loaders, the healthcare system would need to manually convert data from each source or develop one-off integration scripts. By implementing the custom loaders demonstrated in the code, the system can:</p>
                                <ol>
                                    <li>Automatically ingest data from the legacy EHR system without replacing it, preserving the investment in the existing system</li>
                                    <li>Extract both structured data and narrative text from imaging reports, enabling more comprehensive analysis</li>
                                    <li>Standardize data from various wearable devices, allowing clinicians to view all patient-generated health data in a unified format</li>
                                    <li>Process telehealth session records to include conversation transcripts alongside clinical data, providing richer context for care decisions</li>
                                    <li>Integrate pharmacy data to ensure medication information is complete and up-to-date</li>
                                </ol>
                                <p>This approach enables the healthcare system to create a comprehensive patient data platform without requiring all source systems to be replaced or modified, significantly reducing implementation costs and timelines while improving data completeness and quality.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- 4.2 Data Processing -->
                <section id="data-processing" class="mb-5">
                    <h2 class="section-header">4.2 Data Processing</h2>
                    <p>Once data is ingested from various sources, it needs to be processed to make it suitable for analysis and AI applications. Data processing involves cleaning, structuring, and preparing the data for further transformation and use. In healthcare, this step is critical due to the sensitive nature of patient information and the need for accurate, reliable data for clinical decision-making.</p>

                    <!-- Text Splitters -->
                    <div id="text-splitters" class="component-card">
                        <div class="card-header">
                            <h3 class="mb-0">Text Splitters</h3>
                        </div>
                        <div class="card-body">
                            <p>Text splitters break down large documents into smaller, manageable chunks while preserving context and meaning. In healthcare, this is essential for processing lengthy medical records, research papers, and clinical notes that exceed the context limits of AI models.</p>
                            
                            <div class="business-case">
                                <h5><i class="fas fa-briefcase-medical me-2"></i>Detailed Business Use Cases</h5>
                                <p><strong>Clinical Note Analysis:</strong> A hospital wants to analyze clinical notes to identify patterns in patient symptoms and treatments. Clinical notes can be lengthy, containing multiple patient encounters, lab results, and treatment plans. Text splitters break these notes into smaller sections that can be processed individually while maintaining the context of each section.</p>
                                
                                <p><strong>Medical Literature Review:</strong> A research institution is conducting a systematic review of medical literature on a specific disease. Research papers are often long and contain multiple sections (abstract, introduction, methods, results, discussion). Text splitters allow the system to process each section separately, enabling more targeted analysis.</p>
                                
                                <p><strong>Medical Record Summarization:</strong> A healthcare provider wants to generate summaries of patient medical records for quick review by clinicians. Patient records can span years of treatment and contain thousands of pages. Text splitters break these records into manageable chunks that can be summarized individually and then combined into a comprehensive overview.</p>
                                
                                <p><strong>Drug Information Extraction:</strong> A pharmaceutical company needs to extract information about drug interactions from medical literature and regulatory documents. These documents are often lengthy and contain multiple sections with different types of information. Text splitters enable the system to process each section according to its content type.</p>
                            </div>
                            
                            <div class="problem-solved">
                                <h5><i class="fas fa-lightbulb me-2"></i>Problems Solved</h5>
                                <p>Text splitters address several critical challenges in healthcare data processing:</p>
                                <ul>
                                    <li><strong>Context Limitations:</strong> AI models have limits on the amount of text they can process at once. Text splitters break down large documents to fit within these limits while preserving as much context as possible.</li>
                                    <li><strong>Content Segmentation:</strong> Healthcare documents often contain multiple types of information that need to be processed differently. Text splitters can segment documents based on content type, enabling more targeted analysis.</li>
                                    <li><strong>Information Retrieval:</strong> When searching for specific information in large documents, text splitters enable more granular retrieval, improving the relevance of search results.</li>
                                    <li><strong>Processing Efficiency:</strong> Processing smaller text chunks is more computationally efficient than processing entire large documents, especially when dealing with millions of medical records.</li>
                                </ul>
                            </div>
                            
                            <div class="impact-absence">
                                <h5><i class="fas fa-exclamation-triangle me-2"></i>Impact of Absence</h5>
                                <p>If text splitters didn't exist in healthcare systems:</p>
                                <ul>
                                    <li>Large medical documents would exceed the context limits of AI models, making it impossible to process them in their entirety.</li>
                                    <li>Healthcare organizations would need to manually segment documents, a time-consuming and error-prone process that would significantly delay analysis.</li>
                                    <li>Information retrieval from large documents would be less precise, potentially missing critical patient information or research findings.</li>
                                    <li>Processing large medical documents would be computationally expensive, limiting the scalability of healthcare AI applications.</li>
                                    <li>Analysis of medical literature would be less effective, as the system couldn't focus on specific sections of papers based on their content.</li>
                                </ul>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Element Types</h5>
                                <span class="element-type predefined">Predefined</span> <span class="element-type custom">Custom</span>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Relationship Keywords</h5>
                                <span class="relationship">Processing</span> <span class="relationship">Foundation</span>
                            </div>
                            
                            <div class="tech-detail">
                                <h5><i class="fas fa-cogs me-2"></i>In-depth Knowledge</h5>
                                <p>Text splitters use various strategies to divide text while preserving context:</p>
                                <ul>
                                    <li><strong>Recursive Character Splitting:</strong> Attempts to split text at natural boundaries like paragraphs, sentences, and words. This is often the most effective approach for general text.</li>
                                    <li><strong>Character-based Splitting:</strong> Divides text based on a fixed number of characters, which is simple but may break sentences or words.</li>
                                    <li><strong>Semantic Splitting:</strong> Uses embeddings to identify semantic boundaries in the text, creating chunks that are coherent in meaning. This is particularly useful for complex medical texts.</li>
                                    <li><strong>Token-based Splitting:</strong> Divides text based on a fixed number of tokens, which is important when working with models that have token limits.</li>
                                    <li><strong>Structure-aware Splitting:</strong> Recognizes document structure (headings, lists, tables) and splits accordingly. This is valuable for structured medical documents.</li>
                                    <li><strong>Specialized Splitters:</strong> Designed for specific formats like HTML, Markdown, JSON, or code, which respect the syntax and structure of those formats.</li>
                                </ul>
                                <p>In healthcare applications, choosing the right splitting strategy is crucial. Medical texts often contain critical information that must not be split across chunks, such as lab results with reference ranges or medication dosages with instructions. Specialized splitters can be designed to recognize and preserve these important units of information.</p>
                            </div>
                            
                            <div class="business-implication">
                                <h5><i class="fas fa-chart-line me-2"></i>Business Implications</h5>
                                <p>Effective text splitting enables healthcare organizations to:</p>
                                <ul>
                                    <li><strong>Improve Clinical Decision Support:</strong> By processing medical records in manageable chunks, AI systems can provide more relevant and timely insights to clinicians.</li>
                                    <li><strong>Accelerate Medical Research:</strong> Researchers can analyze large volumes of medical literature more efficiently, speeding up the discovery process.</li>
                                    <li><strong>Enhance Patient Care:</strong> Comprehensive analysis of patient records leads to more personalized and effective treatment plans.</li>
                                    <li><strong>Ensure Regulatory Compliance:</strong> Properly segmented data makes it easier to audit and verify AI systems' decision-making processes.</li>
                                </ul>
                            </div>
                            
                            <h5>Code Example</h5>
                            <pre><code class="language-python"># Text Splitters in Healthcare Context
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.text_splitter import SemanticTextSplitter, TokenTextSplitter
from langchain.text_splitter import HTMLHeaderTextSplitter, MarkdownHeaderTextSplitter
from langchain.text_splitter import PythonCodeTextSplitter, JSONTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
import re
import json

# Example 1: Recursive Character Text Splitter for Clinical Notes
def split_clinical_notes(clinical_notes):
    """
    Split clinical notes using recursive character text splitter.
    
    Args:
        clinical_notes: String containing clinical notes
        
    Returns:
        List of split text chunks
    """
    # Initialize the recursive character text splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # Maximum size of each chunk
        chunk_overlap=200,  # Overlap between chunks to maintain context
        length_function=len,  # Function to measure text length
        separators=["\n\n", "\n", ". ", " ", ""]  # Order of preferred split points
    )
    
    # Split the clinical notes
    chunks = text_splitter.split_text(clinical_notes)
    
    return chunks

# Example 2: Semantic Text Splitter for Medical Literature
def split_medical_literature(medical_text, embeddings_model):
    """
    Split medical literature using semantic text splitter.
    
    Args:
        medical_text: String containing medical literature
        embeddings_model: Embeddings model to use for semantic splitting
        
    Returns:
        List of split text chunks
    """
    # Initialize the semantic text splitter
    text_splitter = SemanticTextSplitter(
        embeddings_model=embeddings_model,
        breakpoint_threshold_amount=0.5,  # Threshold for determining split points
        buffer_size=1,  # Number of sentences to include around split points
        chunk_size=1000,
        chunk_overlap=200
    )
    
    # Split the medical literature
    chunks = text_splitter.split_text(medical_text)
    
    return chunks

# Example 3: Token-based Splitter for AI Model Processing
def split_by_tokens(text, model_name="gpt-3.5-turbo"):
    """
    Split text based on token count for AI model processing.
    
    Args:
        text: String to split
        model_name: Name of the model to determine token limits
        
    Returns:
        List of split text chunks
    """
    # Determine chunk size based on model
    if model_name == "gpt-3.5-turbo":
        chunk_size = 3000  # Approximate token limit
    elif model_name == "gpt-4":
        chunk_size = 7000  # Approximate token limit
    else:
        chunk_size = 3000  # Default
    
    # Initialize the token text splitter
    text_splitter = TokenTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=100,
        encoding_name="cl100k_base"  # Encoding for the model
    )
    
    # Split the text
    chunks = text_splitter.split_text(text)
    
    return chunks

# Example 4: HTML-aware Splitter for Medical Web Content
def split_medical_html(html_content):
    """
    Split medical HTML content while preserving structure.
    
    Args:
        html_content: String containing HTML content
        
    Returns:
        List of split text chunks
    """
    # Define headers to split on
    headers_to_split_on = [
        ("h1", "Header 1"),
        ("h2", "Header 2"),
        ("h3", "Header 3"),
        ("h4", "Header 4"),
    ]
    
    # Initialize the HTML header text splitter
    html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    
    # Split the HTML content
    html_chunks = html_splitter.split_text(html_content)
    
    # Convert to plain text chunks
    chunks = []
    for chunk in html_chunks:
        # Extract the text content
        text_content = chunk.page_content
        
        # Add header context if available
        header_context = ""
        for header_name, header_value in chunk.metadata.items():
            if header_name.startswith("Header"):
                header_context += f"{header_name}: {header_value}\n"
        
        if header_context:
            text_content = f"{header_context}\n{text_content}"
        
        chunks.append(text_content)
    
    return chunks

# Example 5: Markdown-aware Splitter for Medical Documents
def split_medical_markdown(markdown_content):
    """
    Split medical Markdown content while preserving structure.
    
    Args:
        markdown_content: String containing Markdown content
        
    Returns:
        List of split text chunks
    """
    # Define headers to split on
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
        ("####", "Header 4"),
    ]
    
    # Initialize the Markdown header text splitter
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    
    # Split the Markdown content
    md_chunks = markdown_splitter.split_text(markdown_content)
    
    # Convert to plain text chunks
    chunks = []
    for chunk in md_chunks:
        # Extract the text content
        text_content = chunk.page_content
        
        # Add header context if available
        header_context = ""
        for header_name, header_value in chunk.metadata.items():
            if header_name.startswith("Header"):
                header_context += f"{header_name}: {header_value}\n"
        
        if header_context:
            text_content = f"{header_context}\n{text_content}"
        
        chunks.append(text_content)
    
    return chunks

# Example 6: JSON-aware Splitter for Medical Data
def split_medical_json(json_content):
    """
    Split medical JSON content while preserving structure.
    
    Args:
        json_content: String containing JSON content
        
    Returns:
        List of split text chunks
    """
    # Initialize the JSON text splitter
    json_splitter = JSONTextSplitter(
        chunk_size=1000,
        chunk_overlap=0  # No overlap for structured data
    )
    
    # Split the JSON content
    chunks = json_splitter.split_text(json_content)
    
    return chunks

# Example 7: Custom Medical Text Splitter
class MedicalTextSplitter:
    """
    Custom text splitter designed specifically for medical documents.
    Preserves important medical units like lab results and medication information.
    """
    
    def __init__(self, chunk_size=1000, chunk_overlap=200):
        """
        Initialize the medical text splitter.
        
        Args:
            chunk_size: Maximum size of each chunk
            chunk_overlap: Overlap between chunks to maintain context
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # Define patterns for important medical units that should not be split
        self.medical_patterns = [
            # Lab results pattern (e.g., "Glucose: 100 mg/dL (70-100)")
            r"[A-Za-z\s]+:\s*\d+(?:\.\d+)?\s*[A-Za-z/%]+\s*\(\d+(?:\.\d+)?\s*-\s*\d+(?:\.\d+)?\)",
            # Medication pattern (e.g., "Metformin 500 mg twice daily")
            r"[A-Za-z]+\s*\d+(?:\.\d+)?\s*[A-Za-z]+\s*[A-Za-z\s]+",
            # Vital signs pattern (e.g., "BP: 120/80 mmHg")
            r"(?:BP|Blood Pressure):\s*\d+/\d+\s*mmHg",
            # Date pattern (e.g., "01/15/2023")
            r"\d{1,2}/\d{1,2}/\d{4}",
            # Time pattern (e.g., "10:30 AM")
            r"\d{1,2}:\d{2}\s*(?:AM|PM|am|pm)"
        ]
    
    def split_text(self, text):
        """
        Split medical text while preserving important medical units.
        
        Args:
            text: String containing medical text
            
        Returns:
            List of split text chunks
        """
        # Find all important medical units
        medical_units = []
        for pattern in self.medical_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                medical_units.append((match.start(), match.end(), match.group()))
        
        # Sort medical units by start position
        medical_units.sort(key=lambda x: x[0])
        
        # Initialize chunks
        chunks = []
        current_chunk = ""
        current_size = 0
        
        # Iterate through the text
        i = 0
        while i < len(text):
            # Check if we're at a medical unit
            at_medical_unit = False
            for start, end, unit in medical_units:
                if i == start:
                    # We're at the start of a medical unit
                    at_medical_unit = True
                    
                    # Check if adding this unit would exceed the chunk size
                    if current_size + (end - i) > self.chunk_size and current_chunk:
                        # Save the current chunk
                        chunks.append(current_chunk)
                        
                        # Start a new chunk with overlap
                        overlap_start = max(0, len(current_chunk) - self.chunk_overlap)
                        current_chunk = current_chunk[overlap_start:]
                        current_size = len(current_chunk)
                    
                    # Add the medical unit to the current chunk
                    current_chunk += unit
                    current_size += (end - i)
                    
                    # Move to the end of the medical unit
                    i = end
                    break
            
            if not at_medical_unit:
                # We're not at a medical unit, add the character
                current_chunk += text[i]
                current_size += 1
                
                # Check if we've reached the chunk size
                if current_size >= self.chunk_size:
                    # Find a good split point (preferably at a sentence or paragraph break)
                    split_point = self._find_split_point(current_chunk, self.chunk_size - self.chunk_overlap)
                    
                    # Split the chunk
                    chunk_to_save = current_chunk[:split_point]
                    chunks.append(chunk_to_save)
                    
                    # Start a new chunk with overlap
                    current_chunk = current_chunk[split_point:]
                    current_size = len(current_chunk)
                
                # Move to the next character
                i += 1
        
        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _find_split_point(self, text, max_position):
        """
        Find a good split point in the text.
        
        Args:
            text: Text to find split point in
            max_position: Maximum position to consider
            
        Returns:
            Index of the split point
        """
        # Limit the search to the max_position
        search_text = text[:max_position]
        
        # Try to split at paragraph breaks first
        paragraphs = search_text.split("\n\n")
        if len(paragraphs) > 1:
            # Find the last paragraph break
            split_point = 0
            for i in range(len(paragraphs) - 1):
                split_point += len(paragraphs[i]) + 2  # +2 for the paragraph break
            return split_point
        
        # Try to split at sentence breaks
        sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', search_text)
        if len(sentences) > 1:
            # Find the last sentence break
            split_point = 0
            for i in range(len(sentences) - 1):
                split_point += len(sentences[i]) + 1  # +1 for the space after the sentence
            return split_point
        
        # If no good split point is found, just split at the max_position
        return max_position

# Example 8: Processing Patient Records with Multiple Splitters
def process_patient_record(patient_record, embeddings_model=None):
    """
    Process a patient record using appropriate text splitters based on content type.
    
    Args:
        patient_record: Dictionary containing patient record data
        embeddings_model: Embeddings model for semantic splitting (optional)
        
    Returns:
        Dictionary containing processed chunks by content type
    """
    processed_chunks = {}
    
    # Process clinical notes
    if "clinical_notes" in patient_record:
        clinical_notes = patient_record["clinical_notes"]
        
        # Use the custom medical text splitter for clinical notes
        medical_splitter = MedicalTextSplitter(chunk_size=1500, chunk_overlap=300)
        clinical_chunks = medical_splitter.split_text(clinical_notes)
        
        processed_chunks["clinical_notes"] = clinical_chunks
    
    # Process medical history
    if "medical_history" in patient_record:
        medical_history = patient_record["medical_history"]
        
        # Use recursive character splitter for medical history
        history_chunks = split_clinical_notes(medical_history)
        
        processed_chunks["medical_history"] = history_chunks
    
    # Process lab results
    if "lab_results" in patient_record:
        lab_results = patient_record["lab_results"]
        
        # If lab results are in JSON format, use JSON splitter
        if isinstance(lab_results, str) and lab_results.strip().startswith('{'):
            lab_chunks = split_medical_json(lab_results)
        else:
            # Otherwise, use the custom medical text splitter
            medical_splitter = MedicalTextSplitter(chunk_size=800, chunk_overlap=100)
            lab_chunks = medical_splitter.split_text(str(lab_results))
        
        processed_chunks["lab_results"] = lab_chunks
    
    # Process medical literature (if available)
    if "medical_literature" in patient_record and embeddings_model:
        medical_literature = patient_record["medical_literature"]
        
        # Use semantic splitter for medical literature
        literature_chunks = split_medical_literature(medical_literature, embeddings_model)
        
        processed_chunks["medical_literature"] = literature_chunks
    
    # Process HTML content (if available)
    if "html_content" in patient_record:
        html_content = patient_record["html_content"]
        
        # Use HTML-aware splitter
        html_chunks = split_medical_html(html_content)
        
        processed_chunks["html_content"] = html_chunks
    
    # Process Markdown content (if available)
    if "markdown_content" in patient_record:
        markdown_content = patient_record["markdown_content"]
        
        # Use Markdown-aware splitter
        md_chunks = split_medical_markdown(markdown_content)
        
        processed_chunks["markdown_content"] = md_chunks
    
    return processed_chunks

# Example usage
if __name__ == "__main__":
    # Example clinical notes
    clinical_notes = """
    Patient: John Doe
    Date: 01/15/2023
    
    CHIEF COMPLAINT:
    Chest pain for 2 days.
    
    HISTORY OF PRESENT ILLNESS:
    Patient is a 55-year-old male with a history of hypertension who presents with chest pain for the past 2 days. The pain is described as a pressure-like sensation in the center of the chest, rated 6/10 in severity. The pain is exacerbated by exertion and relieved by rest. Patient reports associated shortness of breath but no nausea or diaphoresis. Patient took aspirin at home without significant relief.
    
    PAST MEDICAL HISTORY:
    1. Hypertension - diagnosed 5 years ago, controlled with medication
    2. Hyperlipidemia - diagnosed 3 years ago
    3. Type 2 Diabetes - diagnosed 2 years ago, controlled with metformin 500 mg twice daily
    
    MEDICATIONS:
    1. Lisinopril 10 mg daily
    2. Atorvastatin 20 mg daily
    3. Metformin 500 mg twice daily
    4. Aspirin 81 mg daily
    
    ALLERGIES:
    Penicillin - causes rash
    
    SOCIAL HISTORY:
    Patient is a former smoker, quit 10 years ago with a 20 pack-year history. Drinks alcohol socially. Works as an office manager, sedentary lifestyle.
    
    FAMILY HISTORY:
    Father had myocardial infarction at age 60. Mother has type 2 diabetes.
    
    PHYSICAL EXAMINATION:
    Vital Signs: BP: 145/90 mmHg, HR: 85 bpm, RR: 18/min, Temp: 98.6F, O2 Sat: 98% on room air
    
    General: Alert and oriented, in no acute distress
    HEENT: Normocephalic, atraumatic. Pupils equal, round, reactive to light. Mucous membranes moist.
    Neck: Supple, no jugular venous distention, no carotid bruits
    Heart: Regular rate and rhythm, no murmurs, rubs, or gallops
    Lungs: Clear to auscultation bilaterally, good air entry
    Abdomen: Soft, non-tender, no hepatosplenomegaly
    Extremities: No edema, pulses 2+ and symmetric
    
    LABORATORY RESULTS:
    Glucose: 105 mg/dL (70-100)
    Total Cholesterol: 210 mg/dL (<200)
    LDL: 140 mg/dL (<100)
    HDL: 35 mg/dL (>40)
    Triglycerides: 175 mg/dL (<150)
    Troponin: 0.02 ng/mL (<0.04)
    BNP: 45 pg/mL (<100)
    
    ASSESSMENT AND PLAN:
    1. Chest pain - likely stable angina given risk factors and exertional nature
       - Will admit for observation
       - Serial cardiac enzymes
       - Cardiology consult
       - Stress test in the morning
    
    2. Hypertension
       - Continue home regimen
       - Monitor BP closely
    
    3. Hyperlipidemia
       - Continue atorvastatin
       - Consider increasing dose if LDL remains elevated
    
    4. Type 2 Diabetes
       - Continue metformin
       - Monitor glucose levels
    
    Patient education on diet, exercise, and medication compliance provided.
    """
    
    # Example 1: Split clinical notes using recursive character splitter
    print("Example 1: Recursive Character Splitter")
    clinical_chunks = split_clinical_notes(clinical_notes)
    print(f"Split into {len(clinical_chunks)} chunks")
    for i, chunk in enumerate(clinical_chunks[:2]):  # Print first 2 chunks
        print(f"\nChunk {i+1}:")
        print(chunk[:200] + "...")
    
    # Example 2: Split clinical notes using custom medical text splitter
    print("\n\nExample 2: Custom Medical Text Splitter")
    medical_splitter = MedicalTextSplitter(chunk_size=1000, chunk_overlap=200)
    medical_chunks = medical_splitter.split_text(clinical_notes)
    print(f"Split into {len(medical_chunks)} chunks")
    for i, chunk in enumerate(medical_chunks[:2]):  # Print first 2 chunks
        print(f"\nChunk {i+1}:")
        print(chunk[:200] + "...")
    
    # Example 3: Split by tokens
    print("\n\nExample 3: Token-based Splitter")
    token_chunks = split_by_tokens(clinical_notes)
    print(f"Split into {len(token_chunks)} chunks")
    for i, chunk in enumerate(token_chunks[:2]):  # Print first 2 chunks
        print(f"\nChunk {i+1}:")
        print(chunk[:200] + "...")
    
    # Example 4: Process HTML content
    html_content = """
    <html>
    <head>
        <title>Patient Information</title>
    </head>
    <body>
        <h1>Patient Medical Record</h1>
        <h2>Demographics</h2>
        <p>Name: John Doe</p>
        <p>Age: 55</p>
        <p>Gender: Male</p>
        
        <h2>Medical History</h2>
        <p>Hypertension - diagnosed 5 years ago</p>
        <p>Hyperlipidemia - diagnosed 3 years ago</p>
        <p>Type 2 Diabetes - diagnosed 2 years ago</p>
        
        <h2>Current Medications</h2>
        <p>Lisinopril 10 mg daily</p>
        <p>Atorvastatin 20 mg daily</p>
        <p>Metformin 500 mg twice daily</p>
        <p>Aspirin 81 mg daily</p>
        
        <h2>Lab Results</h2>
        <p>Glucose: 105 mg/dL (70-100)</p>
        <p>Total Cholesterol: 210 mg/dL (<200)</p>
        <p>LDL: 140 mg/dL (<100)</p>
        <p>HDL: 35 mg/dL (>40)</p>
    </body>
    </html>
    """
    
    print("\n\nExample 4: HTML-aware Splitter")
    html_chunks = split_medical_html(html_content)
    print(f"Split into {len(html_chunks)} chunks")
    for i, chunk in enumerate(html_chunks):
        print(f"\nChunk {i+1}:")
        print(chunk)
    
    # Example 5: Process Markdown content
    markdown_content = """
    # Patient Medical Record
    
    ## Demographics
    - Name: John Doe
    - Age: 55
    - Gender: Male
    
    ## Medical History
    - Hypertension - diagnosed 5 years ago
    - Hyperlipidemia - diagnosed 3 years ago
    - Type 2 Diabetes - diagnosed 2 years ago
    
    ## Current Medications
    - Lisinopril 10 mg daily
    - Atorvastatin 20 mg daily
    - Metformin 500 mg twice daily
    - Aspirin 81 mg daily
    
    ## Lab Results
    - Glucose: 105 mg/dL (70-100)
    - Total Cholesterol: 210 mg/dL (<200)
    - LDL: 140 mg/dL (<100)
    - HDL: 35 mg/dL (>40)
    """
    
    print("\n\nExample 5: Markdown-aware Splitter")
    md_chunks = split_medical_markdown(markdown_content)
    print(f"Split into {len(md_chunks)} chunks")
    for i, chunk in enumerate(md_chunks):
        print(f"\nChunk {i+1}:")
        print(chunk)
    
    # Example 6: Process JSON content
    json_content = """
    {
        "patient_id": "PT-2023-0456",
        "name": "John Doe",
        "age": 55,
        "gender": "Male",
        "medical_history": [
            {
                "condition": "Hypertension",
                "diagnosed_date": "2018-01-15",
                "status": "Active"
            },
            {
                "condition": "Hyperlipidemia",
                "diagnosed_date": "2020-03-22",
                "status": "Active"
            },
            {
                "condition": "Type 2 Diabetes",
                "diagnosed_date": "2021-07-10",
                "status": "Active"
            }
        ],
        "medications": [
            {
                "name": "Lisinopril",
                "dosage": "10 mg",
                "frequency": "daily"
            },
            {
                "name": "Atorvastatin",
                "dosage": "20 mg",
                "frequency": "daily"
            },
            {
                "name": "Metformin",
                "dosage": "500 mg",
                "frequency": "twice daily"
            },
            {
                "name": "Aspirin",
                "dosage": "81 mg",
                "frequency": "daily"
            }
        ],
        "lab_results": {
            "Glucose": {
                "value": 105,
                "unit": "mg/dL",
                "reference_range": "70-100"
            },
            "Total Cholesterol": {
                "value": 210,
                "unit": "mg/dL",
                "reference_range": "<200"
            },
            "LDL": {
                "value": 140,
                "unit": "mg/dL",
                "reference_range": "<100"
            },
            "HDL": {
                "value": 35,
                "unit": "mg/dL",
                "reference_range": ">40"
            }
        }
    }
    """
    
    print("\n\nExample 6: JSON-aware Splitter")
    json_chunks = split_medical_json(json_content)
    print(f"Split into {len(json_chunks)} chunks")
    for i, chunk in enumerate(json_chunks):
        print(f"\nChunk {i+1}:")
        print(chunk)
    
    # Example 7: Process a complete patient record
    print("\n\nExample 7: Processing Complete Patient Record")
    patient_record = {
        "clinical_notes": clinical_notes,
        "medical_history": "Hypertension, Hyperlipidemia, Type 2 Diabetes",
        "lab_results": json_content,
        "html_content": html_content,
        "markdown_content": markdown_content
    }
    
    # Initialize embeddings model for semantic splitting (using a mock for this example)
    class MockEmbeddings:
        def embed_documents(self, texts):
            return [[0.1] * 1536 for _ in texts]
    
    embeddings_model = MockEmbeddings()
    
    processed_record = process_patient_record(patient_record, embeddings_model)
    
    print("Processed patient record:")
    for content_type, chunks in processed_record.items():
        print(f"\n{content_type}: {len(chunks)} chunks")
        if chunks:
            print(f"First chunk preview: {chunks[0][:100]}...")
</code></pre>
                            
                            <div class="code-explanation">
                                <h5><i class="fas fa-code me-2"></i>Code Explanation</h5>
                                <p>This code demonstrates various text splitting techniques specifically designed for healthcare applications:</p>
                                <ol>
                                    <li><strong>RecursiveCharacterTextSplitter:</strong> The <code>split_clinical_notes</code> function uses this splitter to divide clinical notes at natural boundaries like paragraphs, sentences, and words. This approach preserves context while ensuring chunks are of a manageable size.</li>
                                    <li><strong>SemanticTextSplitter:</strong> The <code>split_medical_literature</code> function uses semantic splitting to divide medical literature based on meaning rather than just character count. This is particularly useful for complex medical texts where maintaining semantic coherence is important.</li>
                                    <li><strong>TokenTextSplitter:</strong> The <code>split_by_tokens</code> function divides text based on token count, which is essential when working with AI models that have token limitations. The function adjusts the chunk size based on the specific model being used.</li>
                                    <li><strong>HTMLHeaderTextSplitter:</strong> The <code>split_medical_html</code> function processes HTML content while preserving its structure. It splits at header boundaries, ensuring that each chunk maintains its contextual relationship to the document hierarchy.</li>
                                    <li><strong>MarkdownHeaderTextSplitter:</strong> The <code>split_medical_markdown</code> function handles Markdown content similarly to the HTML splitter, preserving the document structure based on headers.</li>
                                    <li><strong>JSONTextSplitter:</strong> The <code>split_medical_json</code> function processes JSON data while maintaining its structure. This is important for structured healthcare data like patient records and lab results.</li>
                                    <li><strong>MedicalTextSplitter:</strong> This custom class is specifically designed for medical documents. It recognizes and preserves important medical units like lab results, medication information, vital signs, and dates. The splitter uses regular expressions to identify these units and ensures they are not split across chunks, maintaining their integrity.</li>
                                    <li><strong>process_patient_record:</strong> This function demonstrates how to process a complete patient record using the appropriate splitter for each type of content. It shows how different parts of a patient record might require different splitting strategies based on their content and structure.</li>
                                </ol>
                                <p>The code includes comprehensive examples with sample healthcare data, showing how each splitter handles different types of medical content. The examples demonstrate the importance of choosing the right splitting strategy for different healthcare data types to ensure that critical information is preserved and context is maintained.</p>
                            </div>
                            
                            <div class="industry-example">
                                <h5><i class="fas fa-hospital-alt me-2"></i>Real-world Healthcare Scenario</h5>
                                <p>Consider a large hospital system implementing an AI-powered clinical decision support system. The system needs to process various types of medical documents to provide insights to clinicians:</p>
                                <ul>
                                    <li>Clinical notes from electronic health records, which contain narrative text about patient encounters</li>
                                    <li>Lab results in JSON format, which contain structured data with reference ranges</li>
                                    <li>Medical literature in HTML format from online journals and databases</li>
                                    <li>Clinical guidelines in Markdown format from professional organizations</li>
                                    <li>Patient-generated health data from wearable devices</li>
                                </ul>
                                <p>Each of these document types requires a different approach to text splitting:</p>
                                <ol>
                                    <li>Clinical notes need to be split while preserving important medical units like medication dosages and lab results. The custom MedicalTextSplitter ensures that these critical pieces of information are not split across chunks.</li>
                                    <li>Lab results in JSON format need to maintain their structure to preserve the relationship between values, units, and reference ranges. The JSONTextSplitter handles this by respecting the JSON structure.</li>
                                    <li>Medical literature in HTML format needs to be split based on document structure (sections, subsections) to maintain context. The HTMLHeaderTextSplitter handles this by splitting at header boundaries.</li>
                                    <li>Clinical guidelines in Markdown format similarly need to maintain their hierarchical structure. The MarkdownHeaderTextSplitter preserves this structure during the splitting process.</li>
                                    <li>Patient-generated health data might be best processed using semantic splitting to group related readings and measurements together, even if they span a larger number of characters.</li>
                                </ol>
                                <p>By using the appropriate text splitter for each type of medical document, the hospital system can ensure that the AI receives properly formatted, contextually relevant chunks of information. This leads to more accurate insights and better clinical decision support, ultimately improving patient care and outcomes.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Message Processing -->
                    <div id="message-processing" class="component-card">
                        <div class="card-header">
                            <h3 class="mb-0">Message Processing</h3>
                        </div>
                        <div class="card-body">
                            <p>Message processing involves manipulating conversation messages to optimize them for AI analysis. In healthcare, this includes trimming conversations to focus on relevant parts, filtering out unnecessary information, and merging consecutive messages to maintain context. This is particularly important for clinical conversations, telehealth sessions, and patient-provider communications.</p>
                            
                            <div class="business-case">
                                <h5><i class="fas fa-briefcase-medical me-2"></i>Detailed Business Use Cases</h5>
                                <p><strong>Telehealth Session Analysis:</strong> A healthcare provider wants to analyze telehealth sessions to identify key medical information discussed. Sessions can be lengthy, with significant portions devoted to pleasantries, technical issues, or administrative matters. Message processing trims these sessions to focus on clinically relevant content.</p>
                                
                                <p><strong>Clinical Decision Support:</strong> An AI system assists clinicians by analyzing patient-provider conversations to extract symptoms, treatments, and follow-up plans. Message processing filters out irrelevant content and merges related messages to provide a coherent picture of the clinical discussion.</p>
                                
                                <p><strong>Patient Triage:</strong> A healthcare system uses AI to analyze patient messages through a patient portal to determine urgency and appropriate routing. Message processing filters out non-medical content and focuses on symptoms and concerns to make accurate triage decisions.</p>
                                
                                <p><strong>Medical Documentation:</strong> A hospital uses AI to generate clinical notes from patient-provider conversations. Message processing merges consecutive messages from the same speaker and filters out irrelevant content to create a coherent narrative for documentation.</p>
                            </div>
                            
                            <div class="problem-solved">
                                <h5><i class="fas fa-lightbulb me-2"></i>Problems Solved</h5>
                                <p>Message processing addresses several critical challenges in healthcare communication:</p>
                                <ul>
                                    <li><strong>Information Overload:</strong> Healthcare conversations often contain significant amounts of non-clinical content. Message processing filters out this noise to focus on medically relevant information.</li>
                                    <li><strong>Context Fragmentation:</strong> Conversations can be broken into many small messages that lose context when analyzed individually. Merging consecutive messages maintains the flow and context of the conversation.</li>
                                    <li><strong>Privacy Concerns:</strong> Patient conversations may contain sensitive information that needs to be handled carefully. Message processing can identify and appropriately handle sensitive content.</li>
                                    <li><strong>Efficiency in Analysis:</strong> Processing entire conversations can be computationally expensive. Trimming conversations to the most relevant parts makes analysis more efficient.</li>
                                </ul>
                            </div>
                            
                            <div class="impact-absence">
                                <h5><i class="fas fa-exclamation-triangle me-2"></i>Impact of Absence</h5>
                                <p>If message processing didn't exist in healthcare systems:</p>
                                <ul>
                                    <li>AI systems would need to process entire conversations, including irrelevant content, leading to less accurate analysis and higher computational costs.</li>
                                    <li>Clinical decision support systems might miss important medical information buried in lengthy conversations, potentially affecting patient care.</li>
                                    <li>Patient triage systems would make decisions based on incomplete or noisy information, potentially misrouting urgent cases.</li>
                                    <li>Medical documentation generated from conversations would be verbose and unfocused, making it less useful for clinical decision-making.</li>
                                    <li>Analysis of healthcare communications would be less efficient, limiting the scalability of AI applications in healthcare settings.</li>
                                </ul>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Element Types</h5>
                                <span class="element-type predefined">Predefined</span> <span class="element-type custom">Custom</span>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Relationship Keywords</h5>
                                <span class="relationship">Processing</span> <span class="relationship">Control</span>
                            </div>
                            
                            <div class="tech-detail">
                                <h5><i class="fas fa-cogs me-2"></i>In-depth Knowledge</h5>
                                <p>Message processing involves several techniques to optimize conversation data:</p>
                                <ul>
                                    <li><strong>Message Trimming:</strong> Reduces the length of conversations by removing less relevant parts. This can be based on various criteria such as time (keeping only the most recent messages), relevance (keeping only messages that contain medical keywords), or importance (keeping only messages from healthcare providers).</li>
                                    <li><strong>Message Filtering:</strong> Removes messages that don't meet certain criteria. This can include filtering out messages that are too short, contain only pleasantries, or don't contain medical information.</li>
                                    <li><strong>Merging Consecutive Messages:</strong> Combines messages from the same speaker that occur consecutively. This helps maintain context and makes the conversation flow more naturally when analyzed.</li>
                                    <li><strong>Speaker Identification:</strong> Identifies and tags messages by speaker (patient, provider, caregiver, etc.). This is crucial for understanding the context and importance of each message.</li>
                                    <li><strong>Medical Entity Recognition:</strong> Identifies and highlights medical entities (symptoms, medications, procedures) within messages. This helps focus analysis on clinically relevant content.</li>
                                </ul>
                                <p>In healthcare applications, message processing must be carefully designed to preserve critical medical information while filtering out noise. This often requires domain knowledge to determine what constitutes relevant medical content in different contexts.</p>
                            </div>
                            
                            <div class="business-implication">
                                <h5><i class="fas fa-chart-line me-2"></i>Business Implications</h5>
                                <p>Effective message processing enables healthcare organizations to:</p>
                                <ul>
                                    <li><strong>Improve Clinical Decision Support:</strong> By focusing on medically relevant content, AI systems can provide more accurate and timely insights to clinicians.</li>
                                    <li><strong>Enhance Patient Triage:</strong> More accurate analysis of patient communications leads to better triage decisions and more appropriate care routing.</li>
                                    <li><strong>Reduce Documentation Burden:</strong> Streamlined processing of conversations enables more efficient generation of clinical notes, reducing the administrative burden on healthcare providers.</li>
                                    <li><strong>Increase Operational Efficiency:</strong> Optimized message processing reduces computational requirements, allowing healthcare AI systems to scale more effectively.</li>
                                </ul>
                            </div>
                            
                            <h5>Code Example</h5>
                            <pre><code class="language-python"># Message Processing in Healthcare Context
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from typing import List, Dict, Any, Optional, Tuple
import re
import json
from datetime import datetime, timedelta

# Example 1: Message Trimming for Telehealth Sessions
def trim_telehealth_session(messages: List[Dict[str, Any]], max_messages: int = 50, 
                           focus_keywords: List[str] = None) -> List[Dict[str, Any]]:
    """
    Trim a telehealth session to focus on clinically relevant content.
    
    Args:
        messages: List of message dictionaries with 'speaker', 'content', and 'timestamp'
        max_messages: Maximum number of messages to keep
        focus_keywords: List of keywords to prioritize when trimming
        
    Returns:
        List of trimmed message dictionaries
    """
    if focus_keywords is None:
        focus_keywords = [
            "pain", "symptom", "medication", "treatment", "diagnosis", 
            "fever", "cough", "breathing", "chest", "headache", "nausea",
            "prescription", "referral", "test", "result", "follow-up"
        ]
    
    # If the session is already under the limit, return as is
    if len(messages) <= max_messages:
        return messages
    
    # Score each message based on relevance
    scored_messages = []
    for msg in messages:
        score = 0
        
        # Higher score for provider messages
        if msg.get("speaker") == "provider":
            score += 2
        
        # Score based on keywords
        content_lower = msg.get("content", "").lower()
        for keyword in focus_keywords:
            if keyword in content_lower:
                score += 1
        
        # Higher score for longer messages (likely more substantive)
        if len(content_lower) > 50:
            score += 1
        
        scored_messages.append((score, msg))
    
    # Sort by score (descending) and take the top max_messages
    scored_messages.sort(key=lambda x: x[0], reverse=True)
    trimmed_messages = [msg for score, msg in scored_messages[:max_messages]]
    
    # Sort by timestamp to maintain chronological order
    trimmed_messages.sort(key=lambda x: x.get("timestamp", ""))
    
    return trimmed_messages

# Example 2: Message Filtering for Patient Triage
def filter_patient_messages(messages: List[Dict[str, Any]], 
                           min_length: int = 10,
                           medical_keywords: List[str] = None) -> List[Dict[str, Any]]:
    """
    Filter patient messages to focus on medically relevant content for triage.
    
    Args:
        messages: List of message dictionaries
        min_length: Minimum length of message content to consider
        medical_keywords: List of medical keywords to look for
        
    Returns:
        List of filtered message dictionaries
    """
    if medical_keywords is None:
        medical_keywords = [
            "pain", "ache", "hurt", "symptom", "fever", "cough", "cold", "flu",
            "breathing", "breath", "chest", "heart", "headache", "nausea", "vomit",
            "dizzy", "dizziness", "medication", "medicine", "pill", "prescription",
            "emergency", "urgent", "worry", "concern", "doctor", "appointment"
        ]
    
    filtered_messages = []
    
    for msg in messages:
        content = msg.get("content", "")
        
        # Skip messages that are too short
        if len(content) < min_length:
            continue
        
        # Skip messages that are just pleasantries
        if content.lower() in ["hello", "hi", "thanks", "thank you", "ok", "okay"]:
            continue
        
        # Check if message contains medical keywords
        content_lower = content.lower()
        has_medical_content = any(keyword in content_lower for keyword in medical_keywords)
        
        # Include message if it has medical content or is from a provider
        if has_medical_content or msg.get("speaker") == "provider":
            filtered_messages.append(msg)
    
    return filtered_messages

# Example 3: Merging Consecutive Messages
def merge_consecutive_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Merge consecutive messages from the same speaker.
    
    Args:
        messages: List of message dictionaries
        
    Returns:
        List of merged message dictionaries
    """
    if not messages:
        return []
    
    merged_messages = []
    current_speaker = messages[0].get("speaker")
    current_content = messages[0].get("content", "")
    current_timestamp = messages[0].get("timestamp")
    
    for msg in messages[1:]:
        speaker = msg.get("speaker")
        content = msg.get("content", "")
        timestamp = msg.get("timestamp")
        
        if speaker == current_speaker:
            # Merge with current message
            current_content += " " + content
            # Update timestamp to the latest one
            current_timestamp = timestamp
        else:
            # Save current message and start a new one
            merged_messages.append({
                "speaker": current_speaker,
                "content": current_content,
                "timestamp": current_timestamp
            })
            current_speaker = speaker
            current_content = content
            current_timestamp = timestamp
    
    # Add the last message
    merged_messages.append({
        "speaker": current_speaker,
        "content": current_content,
        "timestamp": current_timestamp
    })
    
    return merged_messages

# Example 4: Medical Entity Recognition in Messages
def extract_medical_entities(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Extract medical entities from messages and add them to the message metadata.
    
    Args:
        messages: List of message dictionaries
        
    Returns:
        List of message dictionaries with extracted medical entities
    """
    # Define patterns for medical entities
    patterns = {
        "symptoms": [
            r"(?:pain|ache|hurt|sore)\s+(?:in|of)\s+([A-Za-z\s]+)",
            r"(?:fever|temperature)\s+(?:of\s+)?(\d+(?:\.\d+)?)\s*(?:degrees?)?",
            r"(?:cough|coughing)\s+(?:for\s+)?(\d+)\s+(?:days?|weeks?|months?)",
            r"(?:difficulty|trouble)\s+(?:breathing|breath)",
            r"(?:nausea|vomiting|dizzy|dizziness)",
            r"(?:headache|migraine)"
        ],
        "medications": [
            r"([A-Za-z]+)\s+(\d+(?:\.\d+)?)\s*(?:mg|mcg|g|ml)",
            r"([A-Za-z]+)\s+(?:once|twice|three times)\s+a\s+day",
            r"([A-Za-z]+)\s+(?:every|each)\s+(\d+)\s+(?:hours?|days?)"
        ],
        "procedures": [
            r"(?:MRI|CT|X-ray|scan|ultrasound)\s+(?:of|for)\s+([A-Za-z\s]+)",
            r"(?:blood test|lab work|labs?)\s+(?:for|to check)\s+([A-Za-z\s]+)",
            r"(?:surgery|operation)\s+(?:on|for)\s+([A-Za-z\s]+)"
        ],
        "vitals": [
            r"(?:BP|blood pressure)\s+(?:of\s+)?(\d+)\/(\d+)",
            r"(?:heart rate|pulse)\s+(?:of\s+)?(\d+)\s*(?:bpm)?",
            r"(?:oxygen|O2|sat)\s+(?:of\s+)?(\d+)%"
        ]
    }
    
    processed_messages = []
    
    for msg in messages:
        content = msg.get("content", "")
        entities = {"symptoms": [], "medications": [], "procedures": [], "vitals": []}
        
        # Extract entities using patterns
        for entity_type, entity_patterns in patterns.items():
            for pattern in entity_patterns:
                matches = re.finditer(pattern, content, re.IGNORECASE)
                for match in matches:
                    entities[entity_type].append(match.group(0))
        
        # Create a new message with entities
        processed_msg = msg.copy()
        processed_msg["medical_entities"] = entities
        processed_messages.append(processed_msg)
    
    return processed_messages

# Example 5: Conversation Summarization for Clinical Documentation
def summarize_conversation(messages: List[Dict[str, Any]]) -> str:
    """
    Generate a summary of a conversation for clinical documentation.
    
    Args:
        messages: List of message dictionaries
        
    Returns:
        String containing the conversation summary
    """
    # First, merge consecutive messages
    merged_messages = merge_consecutive_messages(messages)
    
    # Extract medical entities
    messages_with_entities = extract_medical_entities(merged_messages)
    
    # Initialize summary sections
    summary = {
        "chief_complaint": "",
        "history_of_present_illness": "",
        "medications": [],
        "assessment_and_plan": ""
    }
    
    # Process messages to populate summary sections
    for msg in messages_with_entities:
        speaker = msg.get("speaker")
        content = msg.get("content", "")
        entities = msg.get("medical_entities", {})
        
        if speaker == "patient":
            # Look for chief complaint in early patient messages
            if not summary["chief_complaint"] and entities["symptoms"]:
                summary["chief_complaint"] = content
            
            # Add to history of present illness
            if entities["symptoms"]:
                summary["history_of_present_illness"] += f"Patient reports: {content}\n"
        
        elif speaker == "provider":
            # Extract medications mentioned by provider
            if entities["medications"]:
                summary["medications"].extend(entities["medications"])
            
            # Look for assessment and plan in later provider messages
            if "plan" in content.lower() or "assessment" in content.lower():
                summary["assessment_and_plan"] = content
    
    # Format the summary
    formatted_summary = "CLINICAL NOTE SUMMARY\n\n"
    formatted_summary += f"CHIEF COMPLAINT: {summary['chief_complaint']}\n\n"
    formatted_summary += f"HISTORY OF PRESENT ILLNESS:\n{summary['history_of_present_illness']}\n"
    
    if summary["medications"]:
        formatted_summary += f"\nMEDICATIONS:\n"
        for med in set(summary["medications"]):  # Remove duplicates
            formatted_summary += f"- {med}\n"
    
    formatted_summary += f"\nASSESSMENT AND PLAN:\n{summary['assessment_and_plan']}\n"
    
    return formatted_summary

# Example 6: Time-based Message Processing
def process_messages_by_time(messages: List[Dict[str, Any]], 
                            time_window_minutes: int = 30) -> List[Dict[str, Any]]:
    """
    Process messages by grouping them into time windows.
    
    Args:
        messages: List of message dictionaries with timestamps
        time_window_minutes: Size of each time window in minutes
        
    Returns:
        List of processed message groups
    """
    if not messages:
        return []
    
    # Sort messages by timestamp
    sorted_messages = sorted(messages, key=lambda x: x.get("timestamp", ""))
    
    # Group messages into time windows
    message_groups = []
    current_group = []
    window_start = None
    
    for msg in sorted_messages:
        timestamp_str = msg.get("timestamp", "")
        
        # Parse timestamp (assuming format like "2023-01-15T10:30:00")
        try:
            timestamp = datetime.fromisoformat(timestamp_str)
        except:
            # If parsing fails, use current time
            timestamp = datetime.now()
        
        # Initialize window start if needed
        if window_start is None:
            window_start = timestamp
            current_group.append(msg)
            continue
        
        # Check if message is within current time window
        time_diff = timestamp - window_start
        if time_diff <= timedelta(minutes=time_window_minutes):
            current_group.append(msg)
        else:
            # Save current group and start a new one
            if current_group:
                message_groups.append({
                    "start_time": window_start.isoformat(),
                    "end_time": timestamp.isoformat(),
                    "messages": current_group
                })
            
            current_group = [msg]
            window_start = timestamp
    
    # Add the last group
    if current_group:
        message_groups.append({
            "start_time": window_start.isoformat(),
            "end_time": datetime.now().isoformat(),
            "messages": current_group
        })
    
    return message_groups

# Example 7: Privacy-aware Message Processing
def process_messages_with_privacy(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Process messages while identifying and handling sensitive information.
    
    Args:
        messages: List of message dictionaries
        
    Returns:
        List of processed message dictionaries with privacy considerations
    """
    # Define patterns for potentially sensitive information
    privacy_patterns = {
        "name": r"\b[A-Z][a-z]+\s+[A-Z][a-z]+\b",
        "phone": r"\b\d{3}[-.\s]?\d{3}[-.\s]?\d{4}\b",
        "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
        "address": r"\d+\s+[A-Za-z\s]+,\s*[A-Za-z\s]+,\s*[A-Z]{2}\s*\d{5}",
        "ssn": r"\b\d{3}[-.\s]?\d{2}[-.\s]?\d{4}\b",
        "medical_record": r"\b(?:MRN|Patient ID)\s*[:\-]?\s*([A-Z0-9]+)\b"
    }
    
    processed_messages = []
    
    for msg in messages:
        content = msg.get("content", "")
        privacy_concerns = []
        
        # Check for privacy concerns
        for concern_type, pattern in privacy_patterns.items():
            matches = re.finditer(pattern, content)
            for match in matches:
                privacy_concerns.append({
                    "type": concern_type,
                    "text": match.group(0),
                    "start": match.start(),
                    "end": match.end()
                })
        
        # Create a new message with privacy information
        processed_msg = msg.copy()
        processed_msg["privacy_concerns"] = privacy_concerns
        
        # If there are privacy concerns, flag the message
        if privacy_concerns:
            processed_msg["privacy_flag"] = True
        else:
            processed_msg["privacy_flag"] = False
        
        processed_messages.append(processed_msg)
    
    return processed_messages

# Example 8: Comprehensive Message Processing Pipeline
def process_healthcare_conversation(messages: List[Dict[str, Any]], 
                                   use_case: str = "clinical_documentation") -> Dict[str, Any]:
    """
    Comprehensive pipeline for processing healthcare conversations.
    
    Args:
        messages: List of message dictionaries
        use_case: Processing goal ("clinical_documentation", "triage", "analysis")
        
    Returns:
        Dictionary containing processed conversation data
    """
    # Step 1: Merge consecutive messages
    merged_messages = merge_consecutive_messages(messages)
    
    # Step 2: Extract medical entities
    messages_with_entities = extract_medical_entities(merged_messages)
    
    # Step 3: Process based on use case
    if use_case == "clinical_documentation":
        # Trim to focus on clinically relevant content
        trimmed_messages = trim_telehealth_session(messages_with_entities, max_messages=100)
        
        # Generate summary for documentation
        summary = summarize_conversation(trimmed_messages)
        
        # Group by time for better organization
        time_groups = process_messages_by_time(trimmed_messages, time_window_minutes=15)
        
        result = {
            "original_message_count": len(messages),
            "processed_message_count": len(trimmed_messages),
            "summary": summary,
            "time_groups": time_groups,
            "messages": trimmed_messages
        }
    
    elif use_case == "triage":
        # Filter to focus on urgent medical content
        filtered_messages = filter_patient_messages(messages_with_entities)
        
        # Check for urgent keywords
        urgent_keywords = [
            "chest pain", "difficulty breathing", "severe pain", "bleeding",
            "emergency", "urgent", "immediately", "right away"
        ]
        
        urgency_score = 0
        for msg in filtered_messages:
            content = msg.get("content", "").lower()
            for keyword in urgent_keywords:
                if keyword in content:
                    urgency_score += 1
        
        # Determine triage level based on urgency score
        if urgency_score >= 3:
            triage_level = "Emergency"
        elif urgency_score >= 1:
            triage_level = "Urgent"
        else:
            triage_level = "Routine"
        
        result = {
            "original_message_count": len(messages),
            "filtered_message_count": len(filtered_messages),
            "urgency_score": urgency_score,
            "triage_level": triage_level,
            "messages": filtered_messages
        }
    
    elif use_case == "analysis":
        # Process with privacy considerations
        privacy_processed = process_messages_with_privacy(messages_with_entities)
        
        # Group by time for analysis
        time_groups = process_messages_by_time(privacy_processed, time_window_minutes=30)
        
        # Extract key medical information
        key_info = {
            "symptoms": [],
            "medications": [],
            "procedures": [],
            "vitals": []
        }
        
        for msg in privacy_processed:
            entities = msg.get("medical_entities", {})
            for entity_type, entity_list in entities.items():
                key_info[entity_type].extend(entity_list)
        
        # Remove duplicates
        for entity_type in key_info:
            key_info[entity_type] = list(set(key_info[entity_type]))
        
        result = {
            "original_message_count": len(messages),
            "processed_message_count": len(privacy_processed),
            "key_medical_info": key_info,
            "time_groups": time_groups,
            "messages": privacy_processed
        }
    
    else:
        # Default processing
        result = {
            "original_message_count": len(messages),
            "processed_message_count": len(messages_with_entities),
            "messages": messages_with_entities
        }
    
    return result

# Example usage
if __name__ == "__main__":
    # Example telehealth session messages
    telehealth_messages = [
        {
            "speaker": "patient",
            "content": "Hello doctor, thanks for seeing me today.",
            "timestamp": "2023-01-15T10:00:00"
        },
        {
            "speaker": "provider",
            "content": "Hello, you're welcome. How can I help you today?",
            "timestamp": "2023-01-15T10:00:30"
        },
        {
            "speaker": "patient",
            "content": "I've been having chest pain for the past two days. It's a pressure-like sensation in the center of my chest.",
            "timestamp": "2023-01-15T10:01:00"
        },
        {
            "speaker": "patient",
            "content": "The pain gets worse when I exert myself and gets better when I rest.",
            "timestamp": "2023-01-15T10:01:30"
        },
        {
            "speaker": "provider",
            "content": "I see. On a scale of 1 to 10, how would you rate the pain?",
            "timestamp": "2023-01-15T10:02:00"
        },
        {
            "speaker": "patient",
            "content": "I would say it's about a 6 or 7 when it's at its worst.",
            "timestamp": "2023-01-15T10:02:30"
        },
        {
            "speaker": "patient",
            "content": "I've also been feeling a bit short of breath when the pain occurs.",
            "timestamp": "2023-01-15T10:03:00"
        },
        {
            "speaker": "provider",
            "content": "Thank you for that information. Have you taken any medication for the pain?",
            "timestamp": "2023-01-15T10:03:30"
        },
        {
            "speaker": "patient",
            "content": "I took some aspirin at home, but it didn't help much.",
            "timestamp": "2023-01-15T10:04:00"
        },
        {
            "speaker": "provider",
            "content": "I understand. Based on your symptoms, I'm concerned about possible angina. I'd like to run some tests.",
            "timestamp": "2023-01-15T10:04:30"
        },
        {
            "speaker": "provider",
            "content": "I'm going to order an ECG and some blood work to check your heart.",
            "timestamp": "2023-01-15T10:05:00"
        },
        {
            "speaker": "patient",
            "content": "Okay, that sounds good. Should I be worried?",
            "timestamp": "2023-01-15T10:05:30"
        },
        {
            "speaker": "provider",
            "content": "It's always better to be cautious with chest pain. We'll get the tests done quickly and go from there.",
            "timestamp": "2023-01-15T10:06:00"
        },
        {
            "speaker": "patient",
            "content": "Thank you, doctor. I appreciate your help.",
            "timestamp": "2023-01-15T10:06:30"
        },
        {
            "speaker": "provider",
            "content": "You're welcome. We'll get you scheduled for those tests right away.",
            "timestamp": "2023-01-15T10:07:00"
        }
    ]
    
    # Example 1: Trim telehealth session
    print("Example 1: Trimming Telehealth Session")
    trimmed_messages = trim_telehealth_session(telehealth_messages, max_messages=8)
    print(f"Original messages: {len(telehealth_messages)}")
    print(f"Trimmed messages: {len(trimmed_messages)}")
    for msg in trimmed_messages:
        print(f"{msg['speaker']}: {msg['content']}")
    
    # Example 2: Filter patient messages for triage
    print("\n\nExample 2: Filtering Patient Messages for Triage")
    filtered_messages = filter_patient_messages(telehealth_messages)
    print(f"Original messages: {len(telehealth_messages)}")
    print(f"Filtered messages: {len(filtered_messages)}")
    for msg in filtered_messages:
        print(f"{msg['speaker']}: {msg['content']}")
    
    # Example 3: Merge consecutive messages
    print("\n\nExample 3: Merging Consecutive Messages")
    merged_messages = merge_consecutive_messages(telehealth_messages)
    print(f"Original messages: {len(telehealth_messages)}")
    print(f"Merged messages: {len(merged_messages)}")
    for msg in merged_messages:
        print(f"{msg['speaker']}: {msg['content']}")
    
    # Example 4: Extract medical entities
    print("\n\nExample 4: Extracting Medical Entities")
    messages_with_entities = extract_medical_entities(telehealth_messages)
    for msg in messages_with_entities:
        print(f"{msg['speaker']}: {msg['content']}")
        print(f"Medical entities: {msg['medical_entities']}")
        print()
    
    # Example 5: Summarize conversation
    print("\n\nExample 5: Summarizing Conversation")
    summary = summarize_conversation(telehealth_messages)
    print(summary)
    
    # Example 6: Process messages by time
    print("\n\nExample 6: Processing Messages by Time")
    time_groups = process_messages_by_time(telehealth_messages, time_window_minutes=2)
    for i, group in enumerate(time_groups):
        print(f"\nTime Group {i+1}: {group['start_time']} to {group['end_time']}")
        for msg in group['messages']:
            print(f"  {msg['speaker']}: {msg['content']}")
    
    # Example 7: Process with privacy considerations
    print("\n\nExample 7: Processing with Privacy Considerations")
    # Add some messages with potential privacy concerns
    privacy_test_messages = telehealth_messages.copy()
    privacy_test_messages.extend([
        {
            "speaker": "patient",
            "content": "My name is John Smith and my phone number is 555-123-4567.",
            "timestamp": "2023-01-15T10:08:00"
        },
        {
            "speaker": "provider",
            "content": "Thanks John, I'll call you at that number if needed. Your MRN is PT-2023-0456.",
            "timestamp": "2023-01-15T10:08:30"
        }
    ])
    
    privacy_processed = process_messages_with_privacy(privacy_test_messages)
    for msg in privacy_processed:
        print(f"{msg['speaker']}: {msg['content']}")
        print(f"Privacy flag: {msg['privacy_flag']}")
        if msg['privacy_concerns']:
            print(f"Privacy concerns: {msg['privacy_concerns']}")
        print()
    
    # Example 8: Comprehensive processing pipeline
    print("\n\nExample 8: Comprehensive Processing Pipeline")
    
    # Clinical documentation use case
    print("\nClinical Documentation Use Case:")
    clinical_result = process_healthcare_conversation(telehealth_messages, use_case="clinical_documentation")
    print(f"Original messages: {clinical_result['original_message_count']}")
    print(f"Processed messages: {clinical_result['processed_message_count']}")
    print("\nSummary:")
    print(clinical_result['summary'])
    
    # Triage use case
    print("\n\nTriage Use Case:")
    triage_result = process_healthcare_conversation(telehealth_messages, use_case="triage")
    print(f"Original messages: {triage_result['original_message_count']}")
    print(f"Filtered messages: {triage_result['filtered_message_count']}")
    print(f"Urgency score: {triage_result['urgency_score']}")
    print(f"Triage level: {triage_result['triage_level']}")
    
    # Analysis use case
    print("\n\nAnalysis Use Case:")
    analysis_result = process_healthcare_conversation(telehealth_messages, use_case="analysis")
    print(f"Original messages: {analysis_result['original_message_count']}")
    print(f"Processed messages: {analysis_result['processed_message_count']}")
    print("\nKey medical information:")
    for entity_type, entities in analysis_result['key_medical_info'].items():
        print(f"{entity_type}: {entities}")
</code></pre>
                            
                            <div class="code-explanation">
                                <h5><i class="fas fa-code me-2"></i>Code Explanation</h5>
                                <p>This code demonstrates various message processing techniques specifically designed for healthcare communications:</p>
                                <ol>
                                    <li><strong>trim_telehealth_session:</strong> This function reduces the length of telehealth sessions by scoring messages based on relevance and keeping only the most important ones. It prioritizes messages from healthcare providers and those containing medical keywords, ensuring that clinically relevant content is preserved.</li>
                                    <li><strong>filter_patient_messages:</strong> This function filters patient messages to focus on medically relevant content for triage purposes. It removes very short messages, pleasantries, and messages that don't contain medical keywords, ensuring that the triage system focuses on substantive medical concerns.</li>
                                    <li><strong>merge_consecutive_messages:</strong> This function combines consecutive messages from the same speaker, which helps maintain context and makes conversations flow more naturally. This is particularly useful for clinical documentation and analysis.</li>
                                    <li><strong>extract_medical_entities:</strong> This function identifies and extracts medical entities (symptoms, medications, procedures, vitals) from messages using regular expressions. It adds this information to the message metadata, enabling more targeted analysis and documentation.</li>
                                    <li><strong>summarize_conversation:</strong> This function generates a structured summary of a conversation for clinical documentation. It identifies key sections like chief complaint, history of present illness, medications, and assessment and plan, creating a format familiar to healthcare providers.</li>
                                    <li><strong>process_messages_by_time:</strong> This function groups messages into time windows, which can be useful for analyzing the progression of a conversation or identifying when specific topics were discussed.</li>
                                    <li><strong>process_messages_with_privacy:</strong> This function identifies potentially sensitive information in messages (names, phone numbers, email addresses, etc.) and flags messages that contain privacy concerns. This is crucial for healthcare applications where patient privacy must be protected.</li>
                                    <li><strong>process_healthcare_conversation:</strong> This comprehensive pipeline combines multiple processing techniques based on the specific use case (clinical documentation, triage, or analysis). It demonstrates how different processing strategies can be applied depending on the goal of the analysis.</li>
                                </ol>
                                <p>The code includes a detailed example of a telehealth session between a patient and provider, demonstrating how each processing technique affects the conversation data. The examples show how message processing can extract clinically relevant information while filtering out noise, maintaining privacy, and organizing the data for different analytical purposes.</p>
                            </div>
                            
                            <div class="industry-example">
                                <h5><i class="fas fa-hospital-alt me-2"></i>Real-world Healthcare Scenario</h5>
                                <p>Consider a large healthcare system implementing an AI-powered platform to analyze patient-provider communications across multiple channels:</p>
                                <ul>
                                    <li>Telehealth sessions conducted through the hospital's virtual care platform</li>
                                    <li>Secure messaging between patients and providers through the patient portal</li>
                                    <li>Phone call transcripts from the nurse advice line</li>
                                    <li>In-person visit notes dictated by providers</li>
                                </ul>
                                <p>The healthcare system wants to use this data for multiple purposes:</p>
                                <ol>
                                    <li><strong>Clinical Documentation:</strong> Automatically generate clinical notes from patient-provider conversations to reduce the administrative burden on providers.</li>
                                    <li><strong>Patient Triage:</strong> Analyze incoming patient messages to determine urgency and route them to the appropriate care team.</li>
                                    <li><strong>Care Quality Analysis:</strong> Identify patterns in patient-provider communications to assess the quality of care and identify areas for improvement.</li>
                                </ol>
                                <p>Using the message processing techniques demonstrated in the code, the healthcare system can:</p>
                                <ul>
                                    <li>Trim telehealth sessions to focus on clinically relevant content, making it easier to generate concise clinical notes.</li>
                                    <li>Filter patient messages to identify urgent medical concerns, ensuring that critical issues are addressed promptly.</li>
                                    <li>Merge consecutive messages to maintain context, improving the accuracy of both documentation and analysis.</li>
                                    <li>Extract medical entities to populate structured fields in electronic health records, reducing manual data entry.</li>
                                    <li>Generate conversation summaries that follow standard clinical documentation formats, making them immediately useful for providers.</li>
                                    <li>Process messages with privacy considerations to ensure compliance with HIPAA and other regulations.</li>
                                </ul>
                                <p>By implementing these message processing techniques, the healthcare system can improve the efficiency of clinical documentation, enhance patient triage, and gain valuable insights into the quality of care being delivered. This ultimately leads to better patient outcomes, reduced administrative burden on providers, and more efficient use of healthcare resources.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- 4.3 Data Transformation -->
                <section id="data-transformation" class="mb-5">
                    <h2 class="section-header">4.3 Data Transformation</h2>
                    <p>Data transformation converts processed data into formats suitable for analysis, machine learning, and other applications. In healthcare, this involves creating vector representations of medical text, embedding clinical concepts, and caching these representations for efficient retrieval. This step is crucial for enabling advanced AI applications like semantic search, clinical decision support, and medical knowledge discovery.</p>

                    <!-- Embedding Models -->
                    <div id="embedding-models" class="component-card">
                        <div class="card-header">
                            <h3 class="mb-0">Embedding Models</h3>
                        </div>
                        <div class="card-body">
                            <p>Embedding models convert text into numerical vectors that capture semantic meaning. In healthcare, these models are trained or fine-tuned to understand medical terminology, relationships between clinical concepts, and the context of patient information. This enables AI systems to comprehend and process complex medical language.</p>
                            
                            <div class="business-case">
                                <h5><i class="fas fa-briefcase-medical me-2"></i>Detailed Business Use Cases</h5>
                                <p><strong>Clinical Decision Support:</strong> A hospital wants to implement an AI system that can provide relevant medical literature and similar patient cases to clinicians as they make treatment decisions. Embedding models convert clinical notes and medical literature into vectors that can be compared to find semantically similar content.</p>
                                
                                <p><strong>Medical Research Acceleration:</strong> A pharmaceutical company needs to analyze thousands of research papers to identify potential drug candidates and understand disease mechanisms. Embedding models enable semantic search across this literature, finding connections that might not be apparent through keyword searches alone.</p>
                                
                                <p><strong>Patient Similarity Analysis:</strong> A healthcare provider wants to identify patients with similar conditions and treatment responses to personalize care plans. Embedding models convert patient records into vectors that can be compared to find clinically similar patients.</p>
                                
                                <p><strong>Clinical Trial Matching:</strong> A research hospital needs to identify eligible patients for clinical trials based on their medical history and current conditions. Embedding models enable matching of patient records with trial criteria using semantic similarity rather than exact keyword matches.</p>
                            </div>
                            
                            <div class="problem-solved">
                                <h5><i class="fas fa-lightbulb me-2"></i>Problems Solved</h5>
                                <p>Embedding models address several critical challenges in healthcare data processing:</p>
                                <ul>
                                    <li><strong>Medical Language Complexity:</strong> Healthcare terminology is complex, with synonyms, abbreviations, and context-dependent meanings. Embedding models capture these nuances, enabling more accurate understanding of medical text.</li>
                                    <li><strong>Semantic Search Limitations:</strong> Traditional keyword searches miss semantically related content. Embedding models enable finding content based on meaning rather than exact words, which is crucial for medical applications.</li>
                                    <li><strong>Data Integration Challenges:</strong> Healthcare data comes from diverse sources with different terminologies. Embedding models provide a common vector space where related concepts from different sources are positioned close together.</li>
                                    <li><strong>Clinical Decision Support:</strong> To provide relevant insights, AI systems need to understand the context and meaning of clinical information. Embedding models encode this meaning in a way that can be used by machine learning algorithms.</li>
                                </ul>
                            </div>
                            
                            <div class="impact-absence">
                                <h5><i class="fas fa-exclamation-triangle me-2"></i>Impact of Absence</h5>
                                <p>If embedding models didn't exist in healthcare systems:</p>
                                <ul>
                                    <li>AI systems would struggle to understand the meaning and context of medical text, leading to less accurate clinical decision support and potentially affecting patient care.</li>
                                    <li>Medical research would be limited to keyword searches, missing important connections and insights that could lead to new treatments or understanding of diseases.</li>
                                    <li>Patient similarity analysis would be based on simple matching of structured data fields, missing the nuanced clinical similarities that are important for personalized care.</li>
                                    <li>Clinical trial matching would rely on exact matches of inclusion/exclusion criteria, potentially missing eligible patients whose records use different terminology.</li>
                                    <li>Healthcare organizations would need to manually map terminology between systems, a time-consuming and error-prone process that would hinder data integration efforts.</li>
                                </ul>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Element Types</h5>
                                <span class="element-type predefined">Predefined</span> <span class="element-type custom">Custom</span>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Relationship Keywords</h5>
                                <span class="relationship">Transformation</span> <span class="relationship">Foundation</span>
                            </div>
                            
                            <div class="tech-detail">
                                <h5><i class="fas fa-cogs me-2"></i>In-depth Knowledge</h5>
                                <p>Embedding models for healthcare applications involve several technical considerations:</p>
                                <ul>
                                    <li><strong>Model Architecture:</strong> Common architectures include BERT, BioBERT, ClinicalBERT, GPT variants, and specialized medical transformers. These models are typically pre-trained on large corpora of medical text and may be fine-tuned for specific healthcare applications.</li>
                                    <li><strong>Training Data:</strong> Medical embedding models are trained on diverse healthcare texts including electronic health records, medical literature, clinical guidelines, and medical textbooks. The quality and relevance of this training data significantly impact model performance.</li>
                                    <li><strong>Fine-tuning:</strong> Pre-trained models are often fine-tuned on domain-specific data to adapt them to particular healthcare settings or applications. This process adjusts the model weights to better capture the nuances of the target domain.</li>
                                    <li><strong>Vector Dimensions:</strong> Embedding vectors typically range from 300 to 768+ dimensions, with higher dimensions potentially capturing more nuanced relationships but requiring more computational resources.</li>
                                    <li><strong>Normalization:</strong> Embedding vectors are often normalized (scaled to unit length) to enable more effective similarity comparisons using cosine similarity.</li>
                                </ul>
                                <p>In healthcare applications, embedding models must be carefully evaluated for their ability to capture medical concepts accurately. This often involves testing against medical ontologies like SNOMED CT, MeSH, or UMLS to ensure that clinically related concepts are positioned close together in the vector space.</p>
                            </div>
                            
                            <div class="business-implication">
                                <h5><i class="fas fa-chart-line me-2"></i>Business Implications</h5>
                                <p>Effective embedding models enable healthcare organizations to:</p>
                                <ul>
                                    <li><strong>Improve Clinical Decision Support:</strong> By understanding the meaning and context of clinical information, AI systems can provide more relevant and timely insights to clinicians.</li>
                                    <li><strong>Accelerate Medical Research:</strong> Semantic search capabilities enable researchers to find relevant literature and identify connections more quickly, speeding up the discovery process.</li>
                                    <li><strong>Personalize Patient Care:</strong> Patient similarity analysis based on semantic understanding enables more personalized treatment plans and better outcomes.</li>
                                    <li><strong>Optimize Clinical Trials:</strong> More accurate patient matching for clinical trials accelerates enrollment and improves trial success rates.</li>
                                </ul>
                            </div>
                            
                            <h5>Code Example</h5>
                            <pre><code class="language-python"># Embedding Models in Healthcare Context
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.embeddings import LlamaCppEmbeddings
from langchain.schema import Document
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
import json
import os
from sklearn.metrics.pairwise import cosine_similarity
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer, losses
from torch.utils.data import DataLoader
import pickle

# Example 1: Using Pre-trained Embedding Models for Medical Text
class MedicalEmbeddingManager:
    """
    Manager class for handling medical text embeddings using various pre-trained models.
    """
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2", 
                 model_type: str = "sentence_transformers"):
        """
        Initialize the embedding manager with a specific model.
        
        Args:
            model_name: Name of the pre-trained model
            model_type: Type of model ("sentence_transformers", "huggingface", "openai")
        """
        self.model_name = model_name
        self.model_type = model_type
        
        # Initialize the appropriate model based on type
        if model_type == "sentence_transformers":
            self.model = SentenceTransformer(model_name)
        elif model_type == "huggingface":
            self.model = HuggingFaceEmbeddings(model_name=model_name)
        elif model_type == "openai":
            self.model = OpenAIEmbeddings()
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
        
        # Cache for embeddings to avoid recomputation
        self.embedding_cache = {}
    
    def embed_text(self, text: str) -> List[float]:
        """
        Generate embeddings for a single text.
        
        Args:
            text: Input text to embed
            
        Returns:
            List of floats representing the embedding vector
        """
        # Check cache first
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        # Generate embedding
        if self.model_type == "sentence_transformers":
            embedding = self.model.encode(text).tolist()
        elif self.model_type == "huggingface":
            embedding = self.model.embed_query(text)
        elif self.model_type == "openai":
            embedding = self.model.embed_query(text)
        
        # Cache the embedding
        self.embedding_cache[text] = embedding
        
        return embedding
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for multiple texts.
        
        Args:
            texts: List of input texts to embed
            
        Returns:
            List of embedding vectors
        """
        # Filter out texts that are already in cache
        uncached_texts = [text for text in texts if text not in self.embedding_cache]
        
        # Generate embeddings for uncached texts
        if uncached_texts:
            if self.model_type == "sentence_transformers":
                uncached_embeddings = self.model.encode(uncached_texts).tolist()
            elif self.model_type == "huggingface":
                uncached_embeddings = self.model.embed_documents(uncached_texts)
            elif self.model_type == "openai":
                uncached_embeddings = self.model.embed_documents(uncached_texts)
            
            # Cache the new embeddings
            for text, embedding in zip(uncached_texts, uncached_embeddings):
                self.embedding_cache[text] = embedding
        
        # Return embeddings for all texts (from cache or newly generated)
        return [self.embedding_cache[text] for text in texts]
    
    def find_similar_texts(self, query: str, texts: List[str], top_k: int = 5) -> List[Tuple[str, float]]:
        """
        Find texts most similar to the query.
        
        Args:
            query: Query text
            texts: List of texts to search through
            top_k: Number of top results to return
            
        Returns:
            List of tuples containing (text, similarity_score)
        """
        # Embed the query
        query_embedding = np.array(self.embed_text(query)).reshape(1, -1)
        
        # Embed all texts
        text_embeddings = np.array(self.embed_documents(texts))
        
        # Calculate cosine similarity
        similarities = cosine_similarity(query_embedding, text_embeddings)[0]
        
        # Get top-k results
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        return [(texts[i], similarities[i]) for i in top_indices]
    
    def save_cache(self, file_path: str):
        """
        Save the embedding cache to a file.
        
        Args:
            file_path: Path to save the cache file
        """
        with open(file_path, 'wb') as f:
            pickle.dump(self.embedding_cache, f)
    
    def load_cache(self, file_path: str):
        """
        Load the embedding cache from a file.
        
        Args:
            file_path: Path to the cache file
        """
        if os.path.exists(file_path):
            with open(file_path, 'rb') as f:
                self.embedding_cache = pickle.load(f)

# Example 2: Fine-tuning Embedding Models for Medical Domain
class MedicalEmbeddingFineTuner:
    """
    Class for fine-tuning embedding models on medical text.
    """
    
    def __init__(self, base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        Initialize the fine-tuner with a base model.
        
        Args:
            base_model_name: Name of the base model to fine-tune
        """
        self.base_model_name = base_model_name
        self.model = SentenceTransformer(base_model_name)
    
    def prepare_training_data(self, medical_texts: List[str], 
                              related_pairs: List[Tuple[int, int]] = None,
                              unrelated_pairs: List[Tuple[int, int]] = None):
        """
        Prepare training data for fine-tuning.
        
        Args:
            medical_texts: List of medical texts
            related_pairs: List of tuples containing indices of related texts
            unrelated_pairs: List of tuples containing indices of unrelated texts
        
        Returns:
            DataLoader for training
        """
        from sentence_transformers.readers import InputExample
        
        # If no pairs are provided, create some based on text similarity
        if related_pairs is None and unrelated_pairs is None:
            # Create embeddings for all texts
            embeddings = self.model.encode(medical_texts)
            
            # Calculate similarities
            similarities = cosine_similarity(embeddings)
            
            # Create related and unrelated pairs
            related_pairs = []
            unrelated_pairs = []
            
            for i in range(len(medical_texts)):
                for j in range(i+1, len(medical_texts)):
                    if similarities[i, j] > 0.7:  # Related threshold
                        related_pairs.append((i, j))
                    elif similarities[i, j] < 0.3:  # Unrelated threshold
                        unrelated_pairs.append((i, j))
        
        # Create training examples
        train_examples = []
        
        # Add related pairs with label 1
        for i, j in related_pairs:
            train_examples.append(InputExample(
                texts=[medical_texts[i], medical_texts[j]],
                label=1.0
            ))
        
        # Add unrelated pairs with label 0
        for i, j in unrelated_pairs:
            train_examples.append(InputExample(
                texts=[medical_texts[i], medical_texts[j]],
                label=0.0
            ))
        
        # Create DataLoader
        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
        
        return train_dataloader
    
    def fine_tune_model(self, medical_texts: List[str], 
                       output_path: str,
                       epochs: int = 3,
                       related_pairs: List[Tuple[int, int]] = None,
                       unrelated_pairs: List[Tuple[int, int]] = None):
        """
        Fine-tune the model on medical text.
        
        Args:
            medical_texts: List of medical texts
            output_path: Path to save the fine-tuned model
            epochs: Number of training epochs
            related_pairs: List of tuples containing indices of related texts
            unrelated_pairs: List of tuples containing indices of unrelated texts
        """
        # Prepare training data
        train_dataloader = self.prepare_training_data(
            medical_texts, related_pairs, unrelated_pairs
        )
        
        # Define the loss function
        train_loss = losses.CosineSimilarityLoss(self.model)
        
        # Fine-tune the model
        self.model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            epochs=epochs,
            warmup_steps=100,
            output_path=output_path
        )
        
        print(f"Model fine-tuned and saved to {output_path}")
    
    def evaluate_model(self, test_texts: List[str], 
                      test_pairs: List[Tuple[int, int]],
                      test_labels: List[float]) -> Dict[str, float]:
        """
        Evaluate the fine-tuned model.
        
        Args:
            test_texts: List of test texts
            test_pairs: List of tuples containing indices of text pairs
            test_labels: List of labels (1 for related, 0 for unrelated)
            
        Returns:
            Dictionary containing evaluation metrics
        """
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support
        
        # Generate embeddings for all texts
        embeddings = self.model.encode(test_texts)
        
        # Calculate similarities for test pairs
        predictions = []
        for i, j in test_pairs:
            similarity = cosine_similarity(
                embeddings[i].reshape(1, -1), 
                embeddings[j].reshape(1, -1)
            )[0, 0]
            # Convert similarity to binary prediction
            predictions.append(1 if similarity > 0.5 else 0)
        
        # Calculate metrics
        accuracy = accuracy_score(test_labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            test_labels, predictions, average='binary'
        )
        
        return {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }

# Example 3: Custom Medical Embedding Model
class CustomMedicalEmbeddingModel(nn.Module):
    """
    Custom embedding model designed specifically for medical text.
    """
    
    def __init__(self, vocab_size: int, embedding_dim: int = 300, 
                 hidden_dim: int = 512, num_layers: int = 2):
        """
        Initialize the custom medical embedding model.
        
        Args:
            vocab_size: Size of the vocabulary
            embedding_dim: Dimension of the embeddings
            hidden_dim: Dimension of the hidden layers
            num_layers: Number of LSTM layers
        """
        super(CustomMedicalEmbeddingModel, self).__init__()
        
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # Bidirectional LSTM
        self.lstm = nn.LSTM(
            embedding_dim, 
            hidden_dim, 
            num_layers=num_layers,
            bidirectional=True,
            batch_first=True
        )
        
        # Attention mechanism
        self.attention = nn.Linear(hidden_dim * 2, 1)
        
        # Output layer
        self.output = nn.Linear(hidden_dim * 2, embedding_dim)
        
        # Medical knowledge integration layer
        self.medical_knowledge = nn.Linear(embedding_dim, embedding_dim)
    
    def forward(self, input_ids, medical_knowledge_vectors=None):
        """
        Forward pass of the model.
        
        Args:
            input_ids: Input token IDs
            medical_knowledge_vectors: Optional medical knowledge vectors
            
        Returns:
            Embedding vectors
        """
        # Embed input tokens
        embedded = self.embedding(input_ids)
        
        # Pass through LSTM
        lstm_out, _ = self.lstm(embedded)
        
        # Apply attention
        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)
        attended = torch.sum(attention_weights * lstm_out, dim=1)
        
        # Generate output embeddings
        output = self.output(attended)
        
        # Integrate medical knowledge if provided
        if medical_knowledge_vectors is not None:
            medical_output = self.medical_knowledge(medical_knowledge_vectors)
            output = output + medical_output
        
        # Normalize the output
        output = torch.nn.functional.normalize(output, p=2, dim=1)
        
        return output

# Example 4: Medical Knowledge Integration
class MedicalKnowledgeIntegrator:
    """
    Class for integrating medical knowledge into embeddings.
    """
    
    def __init__(self, medical_ontology_path: str = None):
        """
        Initialize the medical knowledge integrator.
        
        Args:
            medical_ontology_path: Path to medical ontology file
        """
        self.medical_ontology = {}
        
        if medical_ontology_path and os.path.exists(medical_ontology_path):
            with open(medical_ontology_path, 'r') as f:
                self.medical_ontology = json.load(f)
    
    def get_medical_knowledge_vector(self, text: str, embedding_manager: MedicalEmbeddingManager) -> List[float]:
        """
        Generate a medical knowledge vector for the given text.
        
        Args:
            text: Input text
            embedding_manager: Embedding manager for generating embeddings
            
        Returns:
            Medical knowledge vector
        """
        # Extract medical entities from text
        medical_entities = self._extract_medical_entities(text)
        
        if not medical_entities:
            # Return zero vector if no medical entities found
            return [0.0] * 384  # Assuming 384-dimensional embeddings
        
        # Generate embeddings for medical entities
        entity_embeddings = embedding_manager.embed_documents(medical_entities)
        
        # Calculate the average embedding
        knowledge_vector = np.mean(entity_embeddings, axis=0).tolist()
        
        return knowledge_vector
    
    def _extract_medical_entities(self, text: str) -> List[str]:
        """
        Extract medical entities from text.
        
        Args:
            text: Input text
            
        Returns:
            List of medical entities
        """
        # Simple keyword-based extraction (in a real implementation, this would use NER)
        medical_keywords = [
            "pain", "fever", "cough", "breathing", "chest", "headache", "nausea",
            "diabetes", "hypertension", "asthma", "cancer", "heart disease",
            "aspirin", "ibuprofen", "metformin", "lisinopril", "atorvastatin",
            "MRI", "CT scan", "X-ray", "blood test", "ECG", "ultrasound"
        ]
        
        entities = []
        text_lower = text.lower()
        
        for keyword in medical_keywords:
            if keyword in text_lower:
                entities.append(keyword)
        
        return entities

# Example 5: Healthcare Application Embedding Pipeline
class HealthcareEmbeddingPipeline:
    """
    Complete pipeline for generating and using embeddings in healthcare applications.
    """
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 use_medical_knowledge: bool = True,
                 medical_ontology_path: str = None):
        """
        Initialize the healthcare embedding pipeline.
        
        Args:
            model_name: Name of the embedding model
            use_medical_knowledge: Whether to integrate medical knowledge
            medical_ontology_path: Path to medical ontology file
        """
        # Initialize embedding manager
        self.embedding_manager = MedicalEmbeddingManager(model_name)
        
        # Initialize medical knowledge integrator if needed
        self.use_medical_knowledge = use_medical_knowledge
        if use_medical_knowledge:
            self.knowledge_integrator = MedicalKnowledgeIntegrator(medical_ontology_path)
        
        # Storage for document embeddings
        self.document_embeddings = {}
        self.document_metadata = {}
    
    def process_documents(self, documents: List[Document]) -> Dict[str, Any]:
        """
        Process a list of documents and generate embeddings.
        
        Args:
            documents: List of Document objects
            
        Returns:
            Dictionary containing processing results
        """
        # Extract text from documents
        texts = [doc.page_content for doc in documents]
        
        # Generate embeddings
        embeddings = self.embedding_manager.embed_documents(texts)
        
        # Integrate medical knowledge if enabled
        if self.use_medical_knowledge:
            knowledge_vectors = [
                self.knowledge_integrator.get_medical_knowledge_vector(text, self.embedding_manager)
                for text in texts
            ]
            
            # Combine embeddings with knowledge vectors
            combined_embeddings = []
            for embedding, knowledge in zip(embeddings, knowledge_vectors):
                # Simple weighted combination
                combined = [0.7 * e + 0.3 * k for e, k in zip(embedding, knowledge)]
                combined_embeddings.append(combined)
            
            embeddings = combined_embeddings
        
        # Store embeddings and metadata
        for i, doc in enumerate(documents):
            doc_id = f"doc_{i}"
            self.document_embeddings[doc_id] = embeddings[i]
            self.document_metadata[doc_id] = doc.metadata
        
        return {
            "processed_count": len(documents),
            "embedding_dimension": len(embeddings[0]) if embeddings else 0,
            "used_medical_knowledge": self.use_medical_knowledge
        }
    
    def search_similar_documents(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Search for documents similar to the query.
        
        Args:
            query: Query text
            top_k: Number of top results to return
            
        Returns:
            List of dictionaries containing document information and similarity scores
        """
        if not self.document_embeddings:
            return []
        
        # Generate query embedding
        query_embedding = self.embedding_manager.embed_text(query)
        
        # Integrate medical knowledge if enabled
        if self.use_medical_knowledge:
            knowledge_vector = self.knowledge_integrator.get_medical_knowledge_vector(
                query, self.embedding_manager
            )
            # Combine with knowledge vector
            query_embedding = [0.7 * e + 0.3 * k for e, k in zip(query_embedding, knowledge_vector)]
        
        # Calculate similarities
        similarities = {}
        query_np = np.array(query_embedding).reshape(1, -1)
        
        for doc_id, doc_embedding in self.document_embeddings.items():
            doc_np = np.array(doc_embedding).reshape(1, -1)
            similarity = cosine_similarity(query_np, doc_np)[0, 0]
            similarities[doc_id] = similarity
        
        # Get top-k results
        top_docs = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        # Format results
        results = []
        for doc_id, similarity in top_docs:
            result = {
                "document_id": doc_id,
                "similarity_score": similarity,
                "metadata": self.document_metadata.get(doc_id, {})
            }
            results.append(result)
        
        return results
    
    def find_similar_patients(self, patient_record: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Find patients with similar medical records.
        
        Args:
            patient_record: Patient record text
            top_k: Number of top results to return
            
        Returns:
            List of dictionaries containing patient information and similarity scores
        """
        # Filter for patient documents
        patient_docs = {
            doc_id: embedding for doc_id, embedding in self.document_embeddings.items()
            if self.document_metadata.get(doc_id, {}).get("document_type") == "patient_record"
        }
        
        if not patient_docs:
            return []
        
        # Generate patient record embedding
        patient_embedding = self.embedding_manager.embed_text(patient_record)
        
        # Integrate medical knowledge if enabled
        if self.use_medical_knowledge:
            knowledge_vector = self.knowledge_integrator.get_medical_knowledge_vector(
                patient_record, self.embedding_manager
            )
            # Combine with knowledge vector
            patient_embedding = [0.7 * e + 0.3 * k for e, k in zip(patient_embedding, knowledge_vector)]
        
        # Calculate similarities
        similarities = {}
        patient_np = np.array(patient_embedding).reshape(1, -1)
        
        for doc_id, doc_embedding in patient_docs.items():
            doc_np = np.array(doc_embedding).reshape(1, -1)
            similarity = cosine_similarity(patient_np, doc_np)[0, 0]
            similarities[doc_id] = similarity
        
        # Get top-k results (excluding the patient themselves if they're in the database)
        top_patients = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        # Format results
        results = []
        for doc_id, similarity in top_patients:
            result = {
                "patient_id": self.document_metadata.get(doc_id, {}).get("patient_id", "Unknown"),
                "similarity_score": similarity,
                "metadata": self.document_metadata.get(doc_id, {})
            }
            results.append(result)
        
        return results
    
    def cluster_documents(self, n_clusters: int = 5) -> Dict[str, List[str]]:
        """
        Cluster documents based on their embeddings.
        
        Args:
            n_clusters: Number of clusters to create
            
        Returns:
            Dictionary mapping cluster IDs to lists of document IDs
        """
        from sklearn.cluster import KMeans
        
        if not self.document_embeddings:
            return {}
        
        # Prepare embeddings for clustering
        embeddings_list = list(self.document_embeddings.values())
        doc_ids = list(self.document_embeddings.keys())
        
        # Perform clustering
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(embeddings_list)
        
        # Organize documents by cluster
        clusters = {}
        for doc_id, label in zip(doc_ids, cluster_labels):
            cluster_id = f"cluster_{label}"
            if cluster_id not in clusters:
                clusters[cluster_id] = []
            clusters[cluster_id].append(doc_id)
        
        return clusters

# Example usage
if __name__ == "__main__":
    # Example 1: Using MedicalEmbeddingManager
    print("Example 1: Using MedicalEmbeddingManager")
    
    # Initialize embedding manager
    embedding_manager = MedicalEmbeddingManager(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_type="sentence_transformers"
    )
    
    # Example medical texts
    medical_texts = [
        "Patient presents with chest pain and shortness of breath. ECG shows ST elevation.",
        "Patient has a history of hypertension and diabetes. Currently on metformin and lisinopril.",
        "Patient complains of headache and nausea. No fever or neck stiffness.",
        "Patient underwent successful cardiac catheterization. No complications reported.",
        "Patient with chronic obstructive pulmonary disease experiencing exacerbation."
    ]
    
    # Generate embeddings
    embeddings = embedding_manager.embed_documents(medical_texts)
    print(f"Generated {len(embeddings)} embeddings")
    print(f"Embedding dimension: {len(embeddings[0])}")
    
    # Find similar texts
    query = "Patient with heart problems and difficulty breathing"
    similar_texts = embedding_manager.find_similar_texts(query, medical_texts, top_k=3)
    print(f"\nTexts similar to '{query}':")
    for text, similarity in similar_texts:
        print(f"  Similarity {similarity:.4f}: {text}")
    
    # Example 2: Fine-tuning a model
    print("\n\nExample 2: Fine-tuning a Model")
    
    # Initialize fine-tuner
    fine_tuner = MedicalEmbeddingFineTuner()
    
    # For demonstration, we'll use the same medical texts
    # In a real scenario, you would have a larger dataset
    fine_tuner.fine_tune_model(
        medical_texts=medical_texts,
        output_path="./fine_tuned_medical_model",
        epochs=1  # Using 1 epoch for demonstration
    )
    
    # Example 3: Custom Medical Embedding Model
    print("\n\nExample 3: Custom Medical Embedding Model")
    
    # This is a conceptual example - in practice, you would need to train this model
    # For demonstration, we'll just show the model structure
    vocab_size = 10000  # Example vocabulary size
    custom_model = CustomMedicalEmbeddingModel(vocab_size=vocab_size)
    print(f"Custom model created with vocab size {vocab_size}")
    print(f"Model structure:\n{custom_model}")
    
    # Example 4: Medical Knowledge Integration
    print("\n\nExample 4: Medical Knowledge Integration")
    
    # Initialize knowledge integrator
    knowledge_integrator = MedicalKnowledgeIntegrator()
    
    # Get medical knowledge vector for a text
    text = "Patient with diabetes and hypertension experiencing chest pain"
    knowledge_vector = knowledge_integrator.get_medical_knowledge_vector(text, embedding_manager)
    print(f"Medical knowledge vector dimension: {len(knowledge_vector)}")
    print(f"Extracted medical entities: {knowledge_integrator._extract_medical_entities(text)}")
    
    # Example 5: Healthcare Embedding Pipeline
    print("\n\nExample 5: Healthcare Embedding Pipeline")
    
    # Initialize pipeline
    pipeline = HealthcareEmbeddingPipeline(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        use_medical_knowledge=True
    )
    
    # Create example documents
    documents = [
        Document(
            page_content="Patient presents with chest pain and shortness of breath. ECG shows ST elevation.",
            metadata={"document_type": "clinical_note", "patient_id": "PT001", "date": "2023-01-15"}
        ),
        Document(
            page_content="Patient has a history of hypertension and diabetes. Currently on metformin and lisinopril.",
            metadata={"document_type": "patient_record", "patient_id": "PT002", "date": "2023-01-16"}
        ),
        Document(
            page_content="Patient complains of headache and nausea. No fever or neck stiffness.",
            metadata={"document_type": "clinical_note", "patient_id": "PT003", "date": "2023-01-17"}
        ),
        Document(
            page_content="Patient underwent successful cardiac catheterization. No complications reported.",
            metadata={"document_type": "procedure_note", "patient_id": "PT001", "date": "2023-01-18"}
        ),
        Document(
            page_content="Patient with chronic obstructive pulmonary disease experiencing exacerbation.",
            metadata={"document_type": "clinical_note", "patient_id": "PT004", "date": "2023-01-19"}
        )
    ]
    
    # Process documents
    processing_result = pipeline.process_documents(documents)
    print(f"Processed {processing_result['processed_count']} documents")
    print(f"Embedding dimension: {processing_result['embedding_dimension']}")
    print(f"Used medical knowledge: {processing_result['used_medical_knowledge']}")
    
    # Search for similar documents
    query = "heart problems breathing difficulties"
    similar_docs = pipeline.search_similar_documents(query, top_k=3)
    print(f"\nDocuments similar to '{query}':")
    for doc in similar_docs:
        print(f"  Document ID: {doc['document_id']}, Similarity: {doc['similarity_score']:.4f}")
        print(f"  Metadata: {doc['metadata']}")
    
    # Find similar patients
    patient_record = "55-year-old male with history of hypertension and diabetes. Complains of chest pain."
    similar_patients = pipeline.find_similar_patients(patient_record, top_k=2)
    print(f"\nPatients similar to the given record:")
    for patient in similar_patients:
        print(f"  Patient ID: {patient['patient_id']}, Similarity: {patient['similarity_score']:.4f}")
        print(f"  Metadata: {patient['metadata']}")
    
    # Cluster documents
    clusters = pipeline.cluster_documents(n_clusters=3)
    print(f"\nDocument clusters:")
    for cluster_id, doc_ids in clusters.items():
        print(f"  {cluster_id}: {doc_ids}")
</code></pre>
                            
                            <div class="code-explanation">
                                <h5><i class="fas fa-code me-2"></i>Code Explanation</h5>
                                <p>This code demonstrates various approaches to creating and using embedding models for healthcare applications:</p>
                                <ol>
                                    <li><strong>MedicalEmbeddingManager:</strong> This class provides a high-level interface for working with pre-trained embedding models. It supports different model types (Sentence Transformers, Hugging Face, OpenAI) and includes functionality for embedding text, finding similar texts, and caching embeddings to improve performance.</li>
                                    <li><strong>MedicalEmbeddingFineTuner:</strong> This class handles the fine-tuning of embedding models on medical text. It prepares training data by identifying related and unrelated text pairs, then fine-tunes the model using contrastive learning. This approach adapts general-purpose embedding models to better understand medical terminology and relationships.</li>
                                    <li><strong>CustomMedicalEmbeddingModel:</strong> This class defines a custom neural network architecture specifically designed for medical text embeddings. It includes an embedding layer, bidirectional LSTM, attention mechanism, and a medical knowledge integration layer. This architecture is designed to capture the complex relationships and context in medical text.</li>
                                    <li><strong>MedicalKnowledgeIntegrator:</strong> This class integrates structured medical knowledge into the embedding process. It extracts medical entities from text and generates knowledge vectors that can be combined with text embeddings to enhance their medical relevance.</li>
                                    <li><strong>HealthcareEmbeddingPipeline:</strong> This comprehensive class combines all the previous components into a complete pipeline for healthcare applications. It can process documents, search for similar content, find similar patients, and cluster documents based on their embeddings. This pipeline demonstrates how embedding models can be used in real healthcare scenarios.</li>
                                </ol>
                                <p>The code includes examples of how to use each component, from basic text embedding to complex healthcare applications like finding similar patients or clustering medical documents. The examples show how embedding models can capture the semantic meaning of medical text and enable sophisticated analysis that goes beyond simple keyword matching.</p>
                            </div>
                            
                            <div class="industry-example">
                                <h5><i class="fas fa-hospital-alt me-2"></i>Real-world Healthcare Scenario</h5>
                                <p>Consider a large hospital system implementing an AI-powered platform to improve clinical decision support and patient care. The system needs to process and understand various types of medical text:</p>
                                <ul>
                                    <li>Clinical notes from electronic health records</li>
                                    <li>Medical literature and research papers</li>
                                    <li>Clinical guidelines and best practices</li>
                                    <li>Patient-generated health data from wearables and patient portals</li>
                                </ul>
                                <p>The hospital system wants to use this data for multiple purposes:</p>
                                <ol>
                                    <li><strong>Clinical Decision Support:</strong> Provide clinicians with relevant medical literature and similar patient cases as they make treatment decisions.</li>
                                    <li><strong>Patient Similarity Analysis:</strong> Identify patients with similar conditions and treatment responses to personalize care plans.</li>
                                    <li><strong>Medical Research:</strong> Accelerate research by enabling semantic search across medical literature and patient records.</li>
                                    <li><strong>Clinical Trial Matching:</strong> Identify eligible patients for clinical trials based on semantic similarity between their records and trial criteria.</li>
                                </ol>
                                <p>Using the embedding models demonstrated in the code, the hospital system can:</p>
                                <ul>
                                    <li>Convert all medical text into vector representations that capture semantic meaning, enabling more sophisticated analysis than keyword-based approaches.</li>
                                    <li>Fine-tune general-purpose embedding models on the hospital's specific medical text, improving their understanding of local terminology and practices.</li>
                                    <li>Integrate structured medical knowledge (from ontologies like SNOMED CT) with text embeddings to enhance their medical relevance.</li>
                                    <li>Find similar patients based on the semantic content of their records, not just structured data fields, enabling more personalized care.</li>
                                    <li>Cluster medical documents to identify patterns and themes in patient populations, supporting population health management initiatives.</li>
                                </ul>
                                <p>By implementing these embedding techniques, the hospital system can provide more accurate clinical decision support, personalize patient care, accelerate medical research, and optimize clinical trial matching. This ultimately leads to better patient outcomes, more efficient use of healthcare resources, and advancements in medical knowledge.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Vector Representations -->
                    <div id="vector-representations" class="component-card">
                        <div class="card-header">
                            <h3 class="mb-0">Vector Representations</h3>
                        </div>
                        <div class="card-body">
                            <p>Vector representations are the numerical outputs of embedding models that capture the semantic meaning of text. In healthcare, these vectors enable sophisticated analysis of medical text, including similarity comparisons, clustering, and classification. They form the foundation for many advanced AI applications in medicine.</p>
                            
                            <div class="business-case">
                                <h5><i class="fas fa-briefcase-medical me-2"></i>Detailed Business Use Cases</h5>
                                <p><strong>Medical Literature Search:</strong> A research hospital wants to enable researchers to find relevant medical literature based on semantic similarity rather than just keywords. Vector representations allow researchers to find papers that discuss similar concepts even if they use different terminology.</p>
                                
                                <p><strong>Clinical Trial Matching:</strong> A pharmaceutical company needs to identify eligible patients for clinical trials based on their medical records. Vector representations enable matching of patient records with trial criteria using semantic similarity, identifying eligible patients that might be missed by traditional keyword-based approaches.</p>
                                
                                <p><strong>Pharmacovigilance:</strong> A regulatory agency wants to detect potential adverse drug reactions by analyzing patient reports and social media posts. Vector representations enable grouping of similar reports and identification of emerging patterns that might indicate safety concerns.</p>
                                
                                <p><strong>Clinical Decision Support:</strong> A healthcare provider wants to provide clinicians with relevant treatment guidelines and similar patient cases. Vector representations enable matching of current patient cases with historical cases and guidelines based on semantic similarity.</p>
                            </div>
                            
                            <div class="problem-solved">
                                <h5><i class="fas fa-lightbulb me-2"></i>Problems Solved</h5>
                                <p>Vector representations address several critical challenges in healthcare data analysis:</p>
                                <ul>
                                    <li><strong>Terminology Variability:</strong> Healthcare professionals use different terms to describe the same concepts. Vector representations capture semantic meaning, enabling analysis that is robust to terminology differences.</li>
                                    <li><strong>Unstructured Data Analysis:</strong> Much of healthcare data is in unstructured text form. Vector representations enable quantitative analysis of this text, facilitating machine learning applications.</li>
                                    <li><strong>Knowledge Discovery:</strong> Traditional methods struggle to identify non-obvious relationships in medical data. Vector representations enable the discovery of subtle patterns and connections that might lead to new medical insights.</li>
                                    <li><strong>Data Integration:</strong> Healthcare data comes from diverse sources with different formats and terminologies. Vector representations provide a common framework for integrating and analyzing this heterogeneous data.</li>
                                </ul>
                            </div>
                            
                            <div class="impact-absence">
                                <h5><i class="fas fa-exclamation-triangle me-2"></i>Impact of Absence</h5>
                                <p>If vector representations didn't exist in healthcare systems:</p>
                                <ul>
                                    <li>Medical literature search would be limited to keyword matching, missing relevant papers that use different terminology to describe the same concepts.</li>
                                    <li>Clinical trial matching would rely on exact matches of structured data fields, potentially missing eligible patients whose records use different terminology.</li>
                                    <li>Pharmacovigilance systems would struggle to detect emerging adverse drug reaction patterns, as they wouldn't be able to group semantically similar reports.</li>
                                    <li>Clinical decision support systems would provide less relevant recommendations, as they couldn't match cases based on semantic similarity.</li>
                                    <li>Healthcare organizations would need to manually map terminology between systems, a time-consuming and error-prone process that would hinder data integration and analysis.</li>
                                </ul>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Element Types</h5>
                                <span class="element-type predefined">Predefined</span> <span class="element-type custom">Custom</span>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Relationship Keywords</h5>
                                <span class="relationship">Transformation</span> <span class="relationship">Foundation</span>
                            </div>
                            
                            <div class="tech-detail">
                                <h5><i class="fas fa-cogs me-2"></i>In-depth Knowledge</h5>
                                <p>Vector representations in healthcare involve several technical considerations:</p>
                                <ul>
                                    <li><strong>Dimensionality:</strong> Medical text vectors typically range from 300 to 768+ dimensions, with higher dimensions potentially capturing more nuanced relationships but requiring more computational resources.</li>
                                    <li><strong>Similarity Metrics:</strong> Cosine similarity is commonly used to compare medical text vectors, as it measures the angle between vectors rather than their magnitude, which is appropriate for semantic similarity.</li>
                                    <li><strong>Vector Operations:</strong> Mathematical operations like addition, subtraction, and averaging can be performed on medical text vectors to explore relationships between concepts (e.g., "king - man + woman = queen" in general domains, or "hypertension - medication + lifestyle" in medical domains).</li>
                                    <li><strong>Dimensionality Reduction:</strong> Techniques like PCA, t-SNE, or UMAP can be applied to medical text vectors to visualize high-dimensional relationships in 2D or 3D space, aiding in pattern discovery.</li>
                                    <li><strong>Vector Databases:</strong> Specialized databases like Pinecone, Weaviate, or Milvus are designed to efficiently store and query high-dimensional vectors, enabling scalable similarity search in healthcare applications.</li>
                                </ul>
                                <p>In healthcare applications, vector representations must be carefully validated to ensure they accurately capture medical concepts. This often involves testing against medical ontologies and evaluating their performance in domain-specific tasks like clinical decision support or literature search.</p>
                            </div>
                            
                            <div class="business-implication">
                                <h5><i class="fas fa-chart-line me-2"></i>Business Implications</h5>
                                <p>Effective vector representations enable healthcare organizations to:</p>
                                <ul>
                                    <li><strong>Accelerate Medical Research:</strong> Semantic search capabilities enable researchers to find relevant literature more quickly and identify connections that might lead to new discoveries.</li>
                                    <li><strong>Improve Clinical Trials:</strong> More accurate patient matching for clinical trials accelerates enrollment and improves trial success rates.</li>
                                    <li><strong>Enhance Patient Safety:</strong> Pharmacovigilance systems can detect potential adverse drug reactions earlier, improving patient safety.</li>
                                    <li><strong>Personalize Patient Care:</strong> Clinical decision support systems can provide more relevant recommendations based on semantic similarity to previous cases.</li>
                                </ul>
                            </div>
                            
                            <h5>Code Example</h5>
                            <pre><code class="language-python"># Vector Representations in Healthcare Context
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, DBSCAN
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.schema import Document
import json
import os
import pickle
from datetime import datetime

# Example 1: Healthcare Vector Database
class HealthcareVectorDatabase:
    """
    Vector database specifically designed for healthcare applications.
    """
    
    def __init__(self, embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        Initialize the healthcare vector database.
        
        Args:
            embedding_model: Name of the embedding model to use
        """
        # Initialize embedding model
        self.embedding_model = HuggingFaceEmbeddings(model_name=embedding_model)
        
        # Storage for vectors and metadata
        self.vectors = {}  # Dictionary mapping document IDs to vectors
        self.metadata = {}  # Dictionary mapping document IDs to metadata
        self.document_texts = {}  # Dictionary mapping document IDs to original text
        
        # Index for efficient search
        self.vector_index = None
        self.index_to_id = []
    
    def add_document(self, document_id: str, text: str, metadata: Dict[str, Any] = None):
        """
        Add a document to the vector database.
        
        Args:
            document_id: Unique identifier for the document
            text: Text content of the document
            metadata: Optional metadata dictionary
        """
        # Generate embedding
        vector = self.embedding_model.embed_query(text)
        
        # Store vector, metadata, and text
        self.vectors[document_id] = np.array(vector)
        self.metadata[document_id] = metadata or {}
        self.document_texts[document_id] = text
        
        # Invalidate index (will be rebuilt on next search)
        self.vector_index = None
    
    def add_documents(self, documents: List[Tuple[str, str, Dict[str, Any]]]):
        """
        Add multiple documents to the vector database.
        
        Args:
            documents: List of tuples containing (document_id, text, metadata)
        """
        for doc_id, text, metadata in documents:
            self.add_document(doc_id, text, metadata)
    
    def _build_index(self):
        """
        Build the vector index for efficient search.
        """
        if not self.vectors:
            return
        
        # Convert vectors to matrix
        self.index_to_id = list(self.vectors.keys())
        vector_matrix = np.array([self.vectors[doc_id] for doc_id in self.index_to_id])
        
        # Normalize vectors for cosine similarity
        norms = np.linalg.norm(vector_matrix, axis=1, keepdims=True)
        normalized_vectors = vector_matrix / norms
        
        # Store normalized vectors as index
        self.vector_index = normalized_vectors
    
    def search(self, query: str, top_k: int = 5, 
              filter_metadata: Dict[str, Any] = None) -> List[Tuple[str, float]]:
        """
        Search for documents similar to the query.
        
        Args:
            query: Query text
            top_k: Number of top results to return
            filter_metadata: Optional metadata filter
            
        Returns:
            List of tuples containing (document_id, similarity_score)
        """
        # Build index if needed
        if self.vector_index is None:
            self._build_index()
        
        if not self.vector_index.size:
            return []
        
        # Generate query embedding
        query_vector = np.array(self.embedding_model.embed_query(query))
        
        # Normalize query vector
        query_vector = query_vector / np.linalg.norm(query_vector)
        
        # Calculate similarities
        similarities = cosine_similarity(
            query_vector.reshape(1, -1), 
            self.vector_index
        )[0]
        
        # Get top-k results
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # Apply metadata filter if provided
        results = []
        for idx in top_indices:
            doc_id = self.index_to_id[idx]
            
            # Check if document matches metadata filter
            if filter_metadata:
                match = True
                for key, value in filter_metadata.items():
                    if self.metadata[doc_id].get(key) != value:
                        match = False
                        break
                
                if not match:
                    continue
            
            results.append((doc_id, similarities[idx]))
        
        return results
    
    def get_document(self, document_id: str) -> Dict[str, Any]:
        """
        Get a document by ID.
        
        Args:
            document_id: ID of the document to retrieve
            
        Returns:
            Dictionary containing document data
        """
        if document_id not in self.vectors:
            return None
        
        return {
            "id": document_id,
            "text": self.document_texts[document_id],
            "metadata": self.metadata[document_id],
            "vector": self.vectors[document_id]
        }
    
    def save(self, file_path: str):
        """
        Save the vector database to a file.
        
        Args:
            file_path: Path to save the database
        """
        data = {
            "vectors": self.vectors,
            "metadata": self.metadata,
            "document_texts": self.document_texts
        }
        
        with open(file_path, 'wb') as f:
            pickle.dump(data, f)
    
    def load(self, file_path: str):
        """
        Load the vector database from a file.
        
        Args:
            file_path: Path to the database file
        """
        if os.path.exists(file_path):
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
            
            self.vectors = data["vectors"]
            self.metadata = data["metadata"]
            self.document_texts = data["document_texts"]
            
            # Invalidate index (will be rebuilt on next search)
            self.vector_index = None

# Example 2: Medical Concept Vector Operations
class MedicalConceptVectorOperations:
    """
    Class for performing operations on medical concept vectors.
    """
    
    def __init__(self, vector_db: HealthcareVectorDatabase):
        """
        Initialize with a vector database.
        
        Args:
            vector_db: HealthcareVectorDatabase instance
        """
        self.vector_db = vector_db
    
    def concept_similarity(self, concept1: str, concept2: str) -> float:
        """
        Calculate similarity between two medical concepts.
        
        Args:
            concept1: First medical concept
            concept2: Second medical concept
            
        Returns:
            Similarity score between 0 and 1
        """
        # Generate embeddings for both concepts
        embedding_model = self.vector_db.embedding_model
        vec1 = np.array(embedding_model.embed_query(concept1))
        vec2 = np.array(embedding_model.embed_query(concept2))
        
        # Calculate cosine similarity
        similarity = cosine_similarity(vec1.reshape(1, -1), vec2.reshape(1, -1))[0, 0]
        
        return float(similarity)
    
    def find_similar_concepts(self, concept: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        Find concepts similar to the given concept.
        
        Args:
            concept: Medical concept to find similar concepts for
            top_k: Number of top results to return
            
        Returns:
            List of tuples containing (concept, similarity_score)
        """
        # Search for documents similar to the concept
        results = self.vector_db.search(concept, top_k=top_k)
        
        # Extract document texts and similarity scores
        similar_concepts = [(self.vector_db.document_texts[doc_id], score) 
                           for doc_id, score in results]
        
        return similar_concepts
    
    def concept_analogy(self, concept1: str, concept2: str, concept3: str) -> str:
        """
        Perform concept analogy: concept1 is to concept2 as concept3 is to ?
        
        Args:
            concept1: First concept in analogy
            concept2: Second concept in analogy
            concept3: Third concept in analogy
            
        Returns:
            Fourth concept that completes the analogy
        """
        # Generate embeddings for all concepts
        embedding_model = self.vector_db.embedding_model
        vec1 = np.array(embedding_model.embed_query(concept1))
        vec2 = np.array(embedding_model.embed_query(concept2))
        vec3 = np.array(embedding_model.embed_query(concept3))
        
        # Calculate analogy vector: vec2 - vec1 + vec3
        analogy_vector = vec2 - vec1 + vec3
        
        # Find the document most similar to the analogy vector
        best_match = None
        best_similarity = -1
        
        for doc_id, doc_vector in self.vector_db.vectors.items():
            similarity = cosine_similarity(
                analogy_vector.reshape(1, -1), 
                doc_vector.reshape(1, -1)
            )[0, 0]
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = doc_id
        
        if best_match:
            return self.vector_db.document_texts[best_match]
        else:
            return None
    
    def concept_combination(self, concepts: List[str], weights: List[float] = None) -> np.ndarray:
        """
        Combine multiple concepts into a single vector representation.
        
        Args:
            concepts: List of concepts to combine
            weights: Optional weights for each concept
            
        Returns:
            Combined vector representation
        """
        if weights is None:
            weights = [1.0] * len(concepts)
        
        # Generate embeddings for all concepts
        embedding_model = self.vector_db.embedding_model
        concept_vectors = [np.array(embedding_model.embed_query(concept)) 
                          for concept in concepts]
        
        # Normalize weights
        total_weight = sum(weights)
        normalized_weights = [w / total_weight for w in weights]
        
        # Calculate weighted combination
        combined_vector = np.zeros_like(concept_vectors[0])
        for vector, weight in zip(concept_vectors, normalized_weights):
            combined_vector += weight * vector
        
        # Normalize the result
        combined_vector = combined_vector / np.linalg.norm(combined_vector)
        
        return combined_vector

# Example 3: Medical Vector Visualization
class MedicalVectorVisualizer:
    """
    Class for visualizing medical text vectors.
    """
    
    def __init__(self, vector_db: HealthcareVectorDatabase):
        """
        Initialize with a vector database.
        
        Args:
            vector_db: HealthcareVectorDatabase instance
        """
        self.vector_db = vector_db
    
    def reduce_dimensions(self, method: str = "tsne", n_components: int = 2) -> Tuple[np.ndarray, List[str]]:
        """
        Reduce dimensionality of vectors for visualization.
        
        Args:
            method: Dimensionality reduction method ("pca" or "tsne")
            n_components: Number of dimensions to reduce to
            
        Returns:
            Tuple of (reduced_vectors, document_ids)
        """
        # Get all vectors
        vectors = list(self.vector_db.vectors.values())
        doc_ids = list(self.vector_db.vectors.keys())
        
        if not vectors:
            return np.array([]), []
        
        # Convert to numpy array
        vectors = np.array(vectors)
        
        # Apply dimensionality reduction
        if method.lower() == "pca":
            reducer = PCA(n_components=n_components, random_state=42)
        elif method.lower() == "tsne":
            reducer = TSNE(n_components=n_components, random_state=42)
        else:
            raise ValueError(f"Unsupported dimensionality reduction method: {method}")
        
        reduced_vectors = reducer.fit_transform(vectors)
        
        return reduced_vectors, doc_ids
    
    def plot_vectors(self, method: str = "tsne", color_by: str = None, 
                    figsize: Tuple[int, int] = (10, 8)):
        """
        Plot vectors in 2D space.
        
        Args:
            method: Dimensionality reduction method ("pca" or "tsne")
            color_by: Metadata field to color points by
            figsize: Figure size
        """
        # Reduce dimensions
        reduced_vectors, doc_ids = self.reduce_dimensions(method=method, n_components=2)
        
        if len(reduced_vectors) == 0:
            print("No vectors to plot")
            return
        
        # Create DataFrame for plotting
        df = pd.DataFrame({
            "x": reduced_vectors[:, 0],
            "y": reduced_vectors[:, 1],
            "doc_id": doc_ids
        })
        
        # Add color information if specified
        if color_by:
            colors = []
            for doc_id in doc_ids:
                metadata = self.vector_db.metadata.get(doc_id, {})
                colors.append(metadata.get(color_by, "Unknown"))
            
            df["color"] = colors
        
        # Create plot
        plt.figure(figsize=figsize)
        
        if color_by and "color" in df.columns:
            # Plot with colors
            sns.scatterplot(data=df, x="x", y="y", hue="color", alpha=0.7)
        else:
            # Plot without colors
            plt.scatter(df["x"], df["y"], alpha=0.7)
        
        plt.title(f"Medical Text Vectors ({method.upper()})")
        plt.xlabel("Dimension 1")
        plt.ylabel("Dimension 2")
        
        # Add document labels for some points
        if len(doc_ids) <= 20:  # Only add labels if there aren't too many points
            for i, doc_id in enumerate(doc_ids):
                plt.annotate(
                    doc_id, 
                    (df.loc[i, "x"], df.loc[i, "y"]),
                    fontsize=8,
                    alpha=0.7
                )
        
        plt.tight_layout()
        plt.show()
    
    def plot_interactive_vectors(self, method: str = "tsne", color_by: str = None):
        """
        Create an interactive plot of vectors using Plotly.
        
        Args:
            method: Dimensionality reduction method ("pca" or "tsne")
            color_by: Metadata field to color points by
        """
        try:
            import plotly.express as px
        except ImportError:
            print("Plotly is required for interactive plotting. Install with: pip install plotly")
            return
        
        # Reduce dimensions
        reduced_vectors, doc_ids = self.reduce_dimensions(method=method, n_components=2)
        
        if len(reduced_vectors) == 0:
            print("No vectors to plot")
            return
        
        # Create DataFrame for plotting
        df = pd.DataFrame({
            "x": reduced_vectors[:, 0],
            "y": reduced_vectors[:, 1],
            "doc_id": doc_ids
        })
        
        # Add color information if specified
        if color_by:
            colors = []
            for doc_id in doc_ids:
                metadata = self.vector_db.metadata.get(doc_id, {})
                colors.append(metadata.get(color_by, "Unknown"))
            
            df["color"] = colors
        
        # Create hover text
        hover_texts = []
        for doc_id in doc_ids:
            metadata = self.vector_db.metadata.get(doc_id, {})
            text = self.vector_db.document_texts.get(doc_id, "")
            
            # Truncate text for hover
            if len(text) > 100:
                text = text[:100] + "..."
            
            hover_text = f"ID: {doc_id}<br>"
            hover_text += f"Text: {text}<br>"
            
            for key, value in metadata.items():
                hover_text += f"{key}: {value}<br>"
            
            hover_texts.append(hover_text)
        
        df["hover_text"] = hover_texts
        
        # Create interactive plot
        if color_by and "color" in df.columns:
            fig = px.scatter(
                df, x="x", y="y", color="color",
                hover_data=["hover_text"],
                title=f"Medical Text Vectors ({method.upper()})",
                width=800, height=600
            )
        else:
            fig = px.scatter(
                df, x="x", y="y",
                hover_data=["hover_text"],
                title=f"Medical Text Vectors ({method.upper()})",
                width=800, height=600
            )
        
        fig.update_traces(marker=dict(size=10, opacity=0.7))
        fig.show()

# Example 4: Medical Vector Clustering
class MedicalVectorClustering:
    """
    Class for clustering medical text vectors.
    """
    
    def __init__(self, vector_db: HealthcareVectorDatabase):
        """
        Initialize with a vector database.
        
        Args:
            vector_db: HealthcareVectorDatabase instance
        """
        self.vector_db = vector_db
    
    def kmeans_clustering(self, n_clusters: int = 5) -> Dict[str, List[str]]:
        """
        Perform K-means clustering on vectors.
        
        Args:
            n_clusters: Number of clusters to create
            
        Returns:
            Dictionary mapping cluster IDs to lists of document IDs
        """
        # Get all vectors
        vectors = list(self.vector_db.vectors.values())
        doc_ids = list(self.vector_db.vectors.keys())
        
        if not vectors:
            return {}
        
        # Convert to numpy array
        vectors = np.array(vectors)
        
        # Perform K-means clustering
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(vectors)
        
        # Organize documents by cluster
        clusters = {}
        for doc_id, label in zip(doc_ids, cluster_labels):
            cluster_id = f"cluster_{label}"
            if cluster_id not in clusters:
                clusters[cluster_id] = []
            clusters[cluster_id].append(doc_id)
        
        return clusters
    
    def dbscan_clustering(self, eps: float = 0.5, min_samples: int = 5) -> Dict[str, List[str]]:
        """
        Perform DBSCAN clustering on vectors.
        
        Args:
            eps: Maximum distance between two samples for one to be considered as in the neighborhood of the other
            min_samples: Number of samples in a neighborhood for a point to be considered as a core point
            
        Returns:
            Dictionary mapping cluster IDs to lists of document IDs
        """
        # Get all vectors
        vectors = list(self.vector_db.vectors.values())
        doc_ids = list(self.vector_db.vectors.keys())
        
        if not vectors:
            return {}
        
        # Convert to numpy array
        vectors = np.array(vectors)
        
        # Perform DBSCAN clustering
        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric="cosine")
        cluster_labels = dbscan.fit_predict(vectors)
        
        # Organize documents by cluster
        clusters = {}
        for doc_id, label in zip(doc_ids, cluster_labels):
            cluster_id = f"cluster_{label}"
            if cluster_id not in clusters:
                clusters[cluster_id] = []
            clusters[cluster_id].append(doc_id)
        
        return clusters
    
    def analyze_clusters(self, clusters: Dict[str, List[str]], 
                         top_terms: int = 5) -> Dict[str, Dict[str, Any]]:
        """
        Analyze clusters to identify key terms and themes.
        
        Args:
            clusters: Dictionary mapping cluster IDs to lists of document IDs
            top_terms: Number of top terms to identify for each cluster
            
        Returns:
            Dictionary mapping cluster IDs to analysis results
        """
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        cluster_analysis = {}
        
        for cluster_id, doc_ids in clusters.items():
            if not doc_ids:
                continue
            
            # Get texts for documents in this cluster
            cluster_texts = [self.vector_db.document_texts[doc_id] for doc_id in doc_ids]
            
            # Create TF-IDF vectorizer
            vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
            tfidf_matrix = vectorizer.fit_transform(cluster_texts)
            
            # Get feature names (terms)
            feature_names = vectorizer.get_feature_names_out()
            
            # Calculate mean TF-IDF scores for each term
            mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)
            
            # Get top terms
            top_indices = np.argsort(mean_scores)[::-1][:top_terms]
            top_terms = [feature_names[i] for i in top_indices]
            
            # Analyze metadata distribution
            metadata_analysis = {}
            for doc_id in doc_ids:
                metadata = self.vector_db.metadata.get(doc_id, {})
                for key, value in metadata.items():
                    if key not in metadata_analysis:
                        metadata_analysis[key] = {}
                    if value not in metadata_analysis[key]:
                        metadata_analysis[key][value] = 0
                    metadata_analysis[key][value] += 1
            
            # Store analysis results
            cluster_analysis[cluster_id] = {
                "document_count": len(doc_ids),
                "top_terms": top_terms,
                "metadata_distribution": metadata_analysis,
                "sample_documents": doc_ids[:3]  # Sample a few document IDs
            }
        
        return cluster_analysis

# Example 5: Medical Vector Classification
class MedicalVectorClassifier:
    """
    Class for classifying medical text using vector representations.
    """
    
    def __init__(self, vector_db: HealthcareVectorDatabase):
        """
        Initialize with a vector database.
        
        Args:
            vector_db: HealthcareVectorDatabase instance
        """
        self.vector_db = vector_db
        self.classifier = None
        self.label_encoder = {}
        self.reverse_label_encoder = {}
    
    def train_classifier(self, labeled_documents: List[Tuple[str, str]]):
        """
        Train a classifier on labeled documents.
        
        Args:
            labeled_documents: List of tuples containing (document_id, label)
        """
        from sklearn.svm import SVC
        from sklearn.preprocessing import LabelEncoder
        
        # Get vectors and labels for training
        vectors = []
        labels = []
        
        for doc_id, label in labeled_documents:
            if doc_id in self.vector_db.vectors:
                vectors.append(self.vector_db.vectors[doc_id])
                labels.append(label)
        
        if not vectors:
            print("No valid documents for training")
            return
        
        # Convert to numpy arrays
        X = np.array(vectors)
        y = np.array(labels)
        
        # Encode labels
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y)
        
        # Store label encoders
        self.label_encoder = {label: i for i, label in enumerate(label_encoder.classes_)}
        self.reverse_label_encoder = {i: label for i, label in enumerate(label_encoder.classes_)}
        
        # Train classifier
        self.classifier = SVC(kernel='linear', probability=True, random_state=42)
        self.classifier.fit(X, y_encoded)
        
        print(f"Trained classifier on {len(vectors)} documents with {len(self.label_encoder)} classes")
    
    def classify_document(self, document_id: str) -> Tuple[str, float]:
        """
        Classify a document.
        
        Args:
            document_id: ID of the document to classify
            
        Returns:
            Tuple containing (predicted_label, confidence_score)
        """
        if self.classifier is None:
            print("Classifier not trained")
            return None, 0.0
        
        if document_id not in self.vector_db.vectors:
            print(f"Document {document_id} not found in vector database")
            return None, 0.0
        
        # Get document vector
        vector = self.vector_db.vectors[document_id].reshape(1, -1)
        
        # Predict class and probabilities
        predicted_class = self.classifier.predict(vector)[0]
        probabilities = self.classifier.predict_proba(vector)[0]
        confidence = probabilities[predicted_class]
        
        # Convert class index to label
        predicted_label = self.reverse_label_encoder.get(predicted_class, "Unknown")
        
        return predicted_label, float(confidence)
    
    def classify_text(self, text: str) -> Tuple[str, float]:
        """
        Classify a text.
        
        Args:
            text: Text to classify
            
        Returns:
            Tuple containing (predicted_label, confidence_score)
        """
        if self.classifier is None:
            print("Classifier not trained")
            return None, 0.0
        
        # Generate embedding for text
        vector = np.array(self.vector_db.embedding_model.embed_query(text)).reshape(1, -1)
        
        # Predict class and probabilities
        predicted_class = self.classifier.predict(vector)[0]
        probabilities = self.classifier.predict_proba(vector)[0]
        confidence = probabilities[predicted_class]
        
        # Convert class index to label
        predicted_label = self.reverse_label_encoder.get(predicted_class, "Unknown")
        
        return predicted_label, float(confidence)

# Example usage
if __name__ == "__main__":
    # Example 1: Healthcare Vector Database
    print("Example 1: Healthcare Vector Database")
    
    # Initialize vector database
    vector_db = HealthcareVectorDatabase()
    
    # Add some medical documents
    medical_docs = [
        ("doc1", "Patient presents with chest pain and shortness of breath. ECG shows ST elevation.", 
         {"document_type": "clinical_note", "patient_id": "PT001", "date": "2023-01-15"}),
        ("doc2", "Patient has a history of hypertension and diabetes. Currently on metformin and lisinopril.", 
         {"document_type": "patient_record", "patient_id": "PT002", "date": "2023-01-16"}),
        ("doc3", "Patient complains of headache and nausea. No fever or neck stiffness.", 
         {"document_type": "clinical_note", "patient_id": "PT003", "date": "2023-01-17"}),
        ("doc4", "Patient underwent successful cardiac catheterization. No complications reported.", 
         {"document_type": "procedure_note", "patient_id": "PT001", "date": "2023-01-18"}),
        ("doc5", "Patient with chronic obstructive pulmonary disease experiencing exacerbation.", 
         {"document_type": "clinical_note", "patient_id": "PT004", "date": "2023-01-19"}),
        ("doc6", "Patient with type 2 diabetes and hypertension. HbA1c is 8.2%. Blood pressure is 150/90.", 
         {"document_type": "lab_result", "patient_id": "PT005", "date": "2023-01-20"}),
        ("doc7", "Patient with asthma using albuterol inhaler as needed. Symptoms well controlled.", 
         {"document_type": "clinical_note", "patient_id": "PT006", "date": "2023-01-21"}),
        ("doc8", "Patient with history of myocardial infarction. Currently on aspirin, atorvastatin, and metoprolol.", 
         {"document_type": "patient_record", "patient_id": "PT007", "date": "2023-01-22"})
    ]
    
    vector_db.add_documents(medical_docs)
    
    # Search for similar documents
    query = "heart problems breathing difficulties"
    results = vector_db.search(query, top_k=3)
    print(f"Documents similar to '{query}':")
    for doc_id, similarity in results:
        doc = vector_db.get_document(doc_id)
        print(f"  Document ID: {doc_id}, Similarity: {similarity:.4f}")
        print(f"  Text: {doc['text'][:100]}...")
        print(f"  Metadata: {doc['metadata']}")
        print()
    
    # Example 2: Medical Concept Vector Operations
    print("\n\nExample 2: Medical Concept Vector Operations")
    
    # Initialize concept operations
    concept_ops = MedicalConceptVectorOperations(vector_db)
    
    # Calculate concept similarity
    concept1 = "chest pain"
    concept2 = "heart attack"
    similarity = concept_ops.concept_similarity(concept1, concept2)
    print(f"Similarity between '{concept1}' and '{concept2}': {similarity:.4f}")
    
    # Find similar concepts
    concept = "diabetes"
    similar_concepts = concept_ops.find_similar_concepts(concept, top_k=3)
    print(f"\nConcepts similar to '{concept}':")
    for similar_concept, score in similar_concepts:
        print(f"  {similar_concept} (similarity: {score:.4f})")
    
    # Combine concepts
    concepts = ["diabetes", "hypertension"]
    combined_vector = concept_ops.concept_combination(concepts)
    print(f"\nCombined vector for {concepts}: dimension {len(combined_vector)}")
    
    # Example 3: Medical Vector Visualization
    print("\n\nExample 3: Medical Vector Visualization")
    
    # Initialize visualizer
    visualizer = MedicalVectorVisualizer(vector_db)
    
    # Plot vectors using t-SNE
    print("Creating t-SNE visualization...")
    visualizer.plot_vectors(method="tsne", color_by="document_type")
    
    # Example 4: Medical Vector Clustering
    print("\n\nExample 4: Medical Vector Clustering")
    
    # Initialize clustering
    clusterer = MedicalVectorClustering(vector_db)
    
    # Perform K-means clustering
    clusters = clusterer.kmeans_clustering(n_clusters=3)
    print(f"K-means clustering results:")
    for cluster_id, doc_ids in clusters.items():
        print(f"  {cluster_id}: {doc_ids}")
    
    # Analyze clusters
    cluster_analysis = clusterer.analyze_clusters(clusters, top_terms=3)
    print(f"\nCluster analysis:")
    for cluster_id, analysis in cluster_analysis.items():
        print(f"  {cluster_id}:")
        print(f"    Document count: {analysis['document_count']}")
        print(f"    Top terms: {analysis['top_terms']}")
        print(f"    Sample documents: {analysis['sample_documents']}")
        print()
    
    # Example 5: Medical Vector Classification
    print("\n\nExample 5: Medical Vector Classification")
    
    # Initialize classifier
    classifier = MedicalVectorClassifier(vector_db)
    
    # Prepare labeled documents for training
    labeled_docs = [
        ("doc1", "cardiovascular"),
        ("doc2", "metabolic"),
        ("doc3", "neurological"),
        ("doc4", "cardiovascular"),
        ("doc5", "respiratory"),
        ("doc6", "metabolic"),
        ("doc7", "respiratory"),
        ("doc8", "cardiovascular")
    ]
    
    # Train classifier
    classifier.train_classifier(labeled_docs)
    
    # Classify documents
    print("Classification results:")
    for doc_id in ["doc1", "doc2", "doc3", "doc4", "doc5"]:
        predicted_label, confidence = classifier.classify_document(doc_id)
        print(f"  {doc_id}: {predicted_label} (confidence: {confidence:.4f})")
    
    # Classify new text
    new_text = "Patient with high blood sugar and obesity"
    predicted_label, confidence = classifier.classify_text(new_text)
    print(f"\nNew text: '{new_text}'")
    print(f"Predicted class: {predicted_label} (confidence: {confidence:.4f})")
</code></pre>
                            
                            <div class="code-explanation">
                                <h5><i class="fas fa-code me-2"></i>Code Explanation</h5>
                                <p>This code demonstrates various techniques for working with vector representations in healthcare applications:</p>
                                <ol>
                                    <li><strong>HealthcareVectorDatabase:</strong> This class implements a vector database specifically designed for healthcare applications. It can store document vectors along with metadata and text content, perform similarity searches, and apply metadata filters. The database uses cosine similarity for comparing vectors and includes functionality for saving and loading the database.</li>
                                    <li><strong>MedicalConceptVectorOperations:</strong> This class provides operations for working with medical concept vectors. It can calculate similarity between concepts, find similar concepts, perform concept analogies (e.g., "aspirin is to pain relief as insulin is to ?"), and combine multiple concepts into a single vector representation. These operations enable sophisticated analysis of medical concepts and their relationships.</li>
                                    <li><strong>MedicalVectorVisualizer:</strong> This class handles visualization of medical text vectors. It uses dimensionality reduction techniques like PCA and t-SNE to project high-dimensional vectors into 2D or 3D space for visualization. It can create both static plots with matplotlib and interactive plots with Plotly, enabling exploration of the structure of medical text data.</li>
                                    <li><strong>MedicalVectorClustering:</strong> This class performs clustering on medical text vectors using algorithms like K-means and DBSCAN. It can group similar documents together and analyze the clusters to identify key terms and themes. This is useful for discovering patterns in medical data and organizing documents based on their semantic content.</li>
                                    <li><strong>MedicalVectorClassifier:</strong> This class implements a classifier for medical text using vector representations. It can train a classifier on labeled documents and then classify new documents or text. The classifier uses SVM with linear kernel, which works well with high-dimensional vector data. This enables automated categorization of medical text based on its semantic content.</li>
                                </ol>
                                <p>The code includes comprehensive examples showing how to use each component, from basic vector storage and search to more advanced operations like concept analogies, clustering, and classification. The examples demonstrate how vector representations can be used for various healthcare applications, including clinical decision support, medical research, and patient care management.</p>
                            </div>
                            
                            <div class="industry-example">
                                <h5><i class="fas fa-hospital-alt me-2"></i>Real-world Healthcare Scenario</h5>
                                <p>Consider a large healthcare system implementing an AI-powered platform to improve patient care and medical research. The system needs to process and analyze various types of medical text:</p>
                                <ul>
                                    <li>Clinical notes from electronic health records</li>
                                    <li>Medical literature and research papers</li>
                                    <li>Clinical guidelines and best practices</li>
                                    <li>Patient-generated health data from wearables and patient portals</li>
                                </ul>
                                <p>The healthcare system wants to use this data for multiple purposes:</p>
                                <ol>
                                    <li><strong>Clinical Decision Support:</strong> Provide clinicians with relevant medical literature and similar patient cases as they make treatment decisions.</li>
                                    <li><strong>Medical Research:</strong> Enable researchers to find relevant literature and identify patterns in patient data.</li>
                                    <li><strong>Patient Population Management:</strong> Identify groups of similar patients for targeted interventions and population health management.</li>
                                    <li><strong>Medical Education:</strong> Organize medical literature and guidelines for educational purposes.</li>
                                </ol>
                                <p>Using the vector representation techniques demonstrated in the code, the healthcare system can:</p>
                                <ul>
                                    <li>Store all medical text in a vector database, enabling efficient similarity search and retrieval. Clinicians can quickly find relevant literature and similar patient cases based on semantic similarity, not just keywords.</li>
                                    <li>Perform concept operations to explore relationships between medical concepts. For example, researchers can investigate how different treatments relate to various conditions or identify potential drug repurposing opportunities.</li>
                                    <li>Visualize the structure of medical text data to identify patterns and clusters. This can reveal relationships between different conditions, treatments, and patient populations that might not be apparent through traditional analysis.</li>
                                    <li>Cluster patient records to identify groups with similar characteristics, enabling targeted interventions and personalized care plans. For example, the system might identify a cluster of patients with diabetes who respond well to a particular treatment approach.</li>
                                    <li>Classify medical text to automate categorization and routing. For example, the system could automatically categorize incoming patient messages based on their content, routing urgent issues to appropriate care teams.</li>
                                </ul>
                                <p>By implementing these vector representation techniques, the healthcare system can provide more accurate clinical decision support, accelerate medical research, improve patient population management, and enhance medical education. This ultimately leads to better patient outcomes, more efficient use of healthcare resources, and advancements in medical knowledge.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Embedding Caching -->
                    <div id="embedding-caching" class="component-card">
                        <div class="card-header">
                            <h3 class="mb-0">Embedding Caching</h3>
                        </div>
                        <div class="card-body">
                            <p>Embedding caching stores previously computed embeddings to avoid redundant calculations. In healthcare, where processing large volumes of medical text is common, caching significantly improves efficiency and reduces computational costs. This is particularly important for real-time clinical applications and when working with large medical datasets.</p>
                            
                            <div class="business-case">
                                <h5><i class="fas fa-briefcase-medical me-2"></i>Detailed Business Use Cases</h5>
                                <p><strong>Clinical Decision Support Systems:</strong> A hospital implements an AI-powered clinical decision support system that needs to quickly retrieve relevant medical literature and similar patient cases. Embedding caching allows the system to respond in real-time by storing embeddings of frequently accessed medical literature and patient records.</p>
                                
                                <p><strong>Medical Research Platforms:</strong> A research institution develops a platform for analyzing large volumes of medical literature and patient data. Embedding caching enables researchers to iterate quickly on their analyses without waiting for embeddings to be recomputed for each query.</p>
                                
                                <p><strong>Patient Monitoring Systems:</strong> A healthcare provider uses AI to monitor patient-generated health data from wearables and patient portals. Embedding caching allows the system to efficiently compare new patient data with historical patterns without recomputing embeddings for the entire patient history each time.</p>
                                
                                <p><strong>Clinical Trial Matching Systems:</strong> A pharmaceutical company needs to continuously screen patient records for eligibility in multiple clinical trials. Embedding caching enables efficient screening by storing embeddings of patient records and trial criteria, allowing for rapid similarity comparisons.</p>
                            </div>
                            
                            <div class="problem-solved">
                                <h5><i class="fas fa-lightbulb me-2"></i>Problems Solved</h5>
                                <p>Embedding caching addresses several critical challenges in healthcare data processing:</p>
                                <ul>
                                    <li><strong>Computational Efficiency:</strong> Generating embeddings is computationally expensive, especially for large medical texts and datasets. Caching eliminates redundant computations, significantly improving performance.</li>
                                    <li><strong>Real-time Requirements:</strong> Many healthcare applications, such as clinical decision support, require real-time responses. Caching ensures that embeddings are available immediately when needed.</li>
                                    <li><strong>Cost Optimization:</strong> Cloud-based embedding services often charge per API call. Caching reduces the number of API calls, lowering operational costs.</li>
                                    <li><strong>Scalability:</strong> As healthcare datasets grow, the computational burden of generating embeddings increases. Caching enables systems to scale more efficiently by reusing previously computed embeddings.</li>
                                </ul>
                            </div>
                            
                            <div class="impact-absence">
                                <h5><i class="fas fa-exclamation-triangle me-2"></i>Impact of Absence</h5>
                                <p>If embedding caching didn't exist in healthcare systems:</p>
                                <ul>
                                    <li>Clinical decision support systems would have slower response times, potentially affecting the timeliness of clinical decisions and patient care.</li>
                                    <li>Medical research platforms would be less efficient, as researchers would need to wait for embeddings to be recomputed for each analysis iteration.</li>
                                    <li>Patient monitoring systems would struggle to process data in real-time, potentially missing critical changes in patient conditions.</li>
                                    <li>Clinical trial matching systems would be less effective, as they couldn't efficiently screen large patient populations for multiple trials.</li>
                                    <li>Healthcare organizations would face higher computational costs, limiting the scalability of AI applications and potentially restricting access to advanced analytics capabilities.</li>
                                </ul>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Element Types</h5>
                                <span class="element-type predefined">Predefined</span> <span class="element-type custom">Custom</span>
                            </div>
                            
                            <div class="mb-3">
                                <h5>Relationship Keywords</h5>
                                <span class="relationship">Control</span> <span class="relationship">Optimization</span>
                            </div>
                            
                            <div class="tech-detail">
                                <h5><i class="fas fa-cogs me-2"></i>In-depth Knowledge</h5>
                                <p>Embedding caching in healthcare involves several technical considerations:</p>
                                <ul>
                                    <li><strong>Cache Storage:</strong> Embeddings can be cached in memory for fast access, in local files for persistence, or in distributed systems like Redis for scalability. The choice depends on the size of the dataset and performance requirements.</li>
                                    <li><strong>Cache Keys:</strong> Effective cache keys uniquely identify the text and embedding model used. In healthcare, this might include a hash of the text content, model name, and version to ensure consistency.</li>
                                    <li><strong>Cache Eviction Policies:</strong> When the cache reaches its size limit, policies like LRU (Least Recently Used) or LFU (Least Frequently Used) determine which embeddings to remove. Healthcare applications might prioritize clinically relevant content.</li>
                                    <li><strong>Cache Invalidation:</strong> When text content or embedding models change, the cache must be updated. In healthcare, this might involve tracking document versions or model updates to ensure cached embeddings remain valid.</li>
                                    <li><strong>Distributed Caching:</strong> For large healthcare systems, distributed caching across multiple servers enables scalability and fault tolerance. This is important for systems that need to handle high volumes of requests.</li>
                                </ul>
                                <p>In healthcare applications, embedding caching must be implemented with consideration for data privacy and security. Cached embeddings might contain sensitive patient information, so appropriate safeguards must be in place to protect this data.</p>
                            </div>
                            
                            <div class="business-implication">
                                <h5><i class="fas fa-chart-line me-2"></i>Business Implications</h5>
                                <p>Effective embedding caching enables healthcare organizations to:</p>
                                <ul>
                                    <li><strong>Improve Clinical Decision Support:</strong> Real-time access to embeddings enables faster and more responsive clinical decision support systems, improving the timeliness of care.</li>
                                    <li><strong>Accelerate Medical Research:</strong> Efficient reuse of embeddings allows researchers to iterate more quickly on their analyses, speeding up the discovery process.</li>
                                    <li><strong>Enhance Patient Monitoring:</strong> Real-time processing of patient data enables more timely detection of changes in patient conditions.</li>
                                    <li><strong>Optimize Clinical Trials:</strong> Efficient screening of patient records enables faster identification of eligible participants, accelerating trial enrollment.</li>
                                    <li><strong>Reduce Operational Costs:</strong> Reduced computational requirements lower the cost of running AI applications in healthcare settings.</li>
                                </ul>
                            </div>
                            
                            <h5>Code Example</h5>
                            <pre><code class="language-python"># Embedding Caching in Healthcare Context
import os
import json
import hashlib
import pickle
import time
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import numpy as np
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.schema import Document
import redis
import sqlite3
from collections import OrderedDict

# Example 1: In-memory Embedding Cache
class InMemoryEmbeddingCache:
    """
    In-memory cache for storing text embeddings.
    """
    
    def __init__(self, max_size: int = 1000):
        """
        Initialize the in-memory embedding cache.
        
        Args:
            max_size: Maximum number of embeddings to store in cache
        """
        self.cache = OrderedDict()  # Ordered dictionary for LRU eviction
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
    
    def _generate_key(self, text: str, model_name: str) -> str:
        """
        Generate a unique key for the text and model.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            
        Returns:
            Unique key string
        """
        # Create a hash of the text and model name
        key_data = f"{text}:{model_name}".encode('utf-8')
        return hashlib.md5(key_data).hexdigest()
    
    def get(self, text: str, model_name: str) -> Optional[np.ndarray]:
        """
        Get embedding from cache.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            
        Returns:
            Embedding vector if found, None otherwise
        """
        key = self._generate_key(text, model_name)
        
        if key in self.cache:
            # Move to end to mark as recently used (for LRU)
            self.cache.move_to_end(key)
            self.hits += 1
            return self.cache[key]
        else:
            self.misses += 1
            return None
    
    def set(self, text: str, model_name: str, embedding: np.ndarray):
        """
        Set embedding in cache.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            embedding: Embedding vector to cache
        """
        key = self._generate_key(text, model_name)
        
        # Add to cache
        self.cache[key] = embedding
        
        # Move to end to mark as recently used
        self.cache.move_to_end(key)
        
        # Evict least recently used item if cache is full
        if len(self.cache) > self.max_size:
            self.cache.popitem(last=False)
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics.
        
        Returns:
            Dictionary containing cache statistics
        """
        total_requests = self.hits + self.misses
        hit_rate = self.hits / total_requests if total_requests > 0 else 0
        
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": hit_rate
        }
    
    def clear(self):
        """Clear the cache."""
        self.cache.clear()
        self.hits = 0
        self.misses = 0

# Example 2: File-based Embedding Cache
class FileEmbeddingCache:
    """
    File-based cache for storing text embeddings with persistence.
    """
    
    def __init__(self, cache_dir: str = "./embedding_cache", max_size: int = 10000):
        """
        Initialize the file-based embedding cache.
        
        Args:
            cache_dir: Directory to store cache files
            max_size: Maximum number of embeddings to store in cache
        """
        self.cache_dir = cache_dir
        self.max_size = max_size
        self.index_file = os.path.join(cache_dir, "cache_index.json")
        self.index = {}  # Maps keys to file paths and metadata
        
        # Create cache directory if it doesn't exist
        os.makedirs(cache_dir, exist_ok=True)
        
        # Load index if it exists
        self._load_index()
        
        self.hits = 0
        self.misses = 0
    
    def _generate_key(self, text: str, model_name: str) -> str:
        """
        Generate a unique key for the text and model.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            
        Returns:
            Unique key string
        """
        # Create a hash of the text and model name
        key_data = f"{text}:{model_name}".encode('utf-8')
        return hashlib.md5(key_data).hexdigest()
    
    def _get_file_path(self, key: str) -> str:
        """
        Get file path for a cache key.
        
        Args:
            key: Cache key
            
        Returns:
            File path
        """
        return os.path.join(self.cache_dir, f"{key}.pkl")
    
    def _load_index(self):
        """Load cache index from file."""
        if os.path.exists(self.index_file):
            try:
                with open(self.index_file, 'r') as f:
                    self.index = json.load(f)
            except:
                self.index = {}
    
    def _save_index(self):
        """Save cache index to file."""
        with open(self.index_file, 'w') as f:
            json.dump(self.index, f)
    
    def get(self, text: str, model_name: str) -> Optional[np.ndarray]:
        """
        Get embedding from cache.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            
        Returns:
            Embedding vector if found, None otherwise
        """
        key = self._generate_key(text, model_name)
        
        if key in self.index:
            # Update access time
            self.index[key]["last_accessed"] = datetime.now().isoformat()
            self._save_index()
            
            # Load embedding from file
            file_path = self._get_file_path(key)
            try:
                with open(file_path, 'rb') as f:
                    embedding = pickle.load(f)
                
                self.hits += 1
                return embedding
            except:
                # Remove from index if file is corrupted
                del self.index[key]
                self._save_index()
        
        self.misses += 1
        return None
    
    def set(self, text: str, model_name: str, embedding: np.ndarray):
        """
        Set embedding in cache.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            embedding: Embedding vector to cache
        """
        key = self._generate_key(text, model_name)
        file_path = self._get_file_path(key)
        
        # Save embedding to file
        with open(file_path, 'wb') as f:
            pickle.dump(embedding, f)
        
        # Update index
        self.index[key] = {
            "file_path": file_path,
            "created": datetime.now().isoformat(),
            "last_accessed": datetime.now().isoformat(),
            "model_name": model_name,
            "text_length": len(text)
        }
        
        self._save_index()
        
        # Evict least recently used items if cache is full
        if len(self.index) > self.max_size:
            self._evict_lru()
    
    def _evict_lru(self, count: int = 1):
        """
        Evict least recently used items from cache.
        
        Args:
            count: Number of items to evict
        """
        # Sort by last accessed time
        sorted_items = sorted(
            self.index.items(), 
            key=lambda x: x[1]["last_accessed"]
        )
        
        # Evict the least recently used items
        for key, _ in sorted_items[:count]:
            # Remove file
            file_path = self._get_file_path(key)
            try:
                os.remove(file_path)
            except:
                pass
            
            # Remove from index
            del self.index[key]
        
        self._save_index()
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics.
        
        Returns:
            Dictionary containing cache statistics
        """
        total_requests = self.hits + self.misses
        hit_rate = self.hits / total_requests if total_requests > 0 else 0
        
        return {
            "size": len(self.index),
            "max_size": self.max_size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": hit_rate,
            "cache_dir": self.cache_dir
        }
    
    def clear(self):
        """Clear the cache."""
        # Remove all cache files
        for key in list(self.index.keys()):
            file_path = self._get_file_path(key)
            try:
                os.remove(file_path)
            except:
                pass
        
        # Clear index
        self.index = {}
        self._save_index()
        
        # Reset statistics
        self.hits = 0
        self.misses = 0

# Example 3: Redis-based Embedding Cache
class RedisEmbeddingCache:
    """
    Redis-based cache for storing text embeddings with distributed access.
    """
    
    def __init__(self, host: str = "localhost", port: int = 6379, 
                 db: int = 0, password: str = None, 
                 max_size: int = 10000, ttl: int = 86400):
        """
        Initialize the Redis-based embedding cache.
        
        Args:
            host: Redis host
            port: Redis port
            db: Redis database number
            password: Redis password
            max_size: Maximum number of embeddings to store in cache
            ttl: Time to live for cache entries in seconds
        """
        self.redis_client = redis.StrictRedis(
            host=host, port=port, db=db, password=password
        )
        self.max_size = max_size
        self.ttl = ttl
        self.hits = 0
        self.misses = 0
    
    def _generate_key(self, text: str, model_name: str) -> str:
        """
        Generate a unique key for the text and model.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            
        Returns:
            Unique key string
        """
        # Create a hash of the text and model name
        key_data = f"{text}:{model_name}".encode('utf-8')
        return f"embedding:{hashlib.md5(key_data).hexdigest()}"
    
    def get(self, text: str, model_name: str) -> Optional[np.ndarray]:
        """
        Get embedding from cache.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            
        Returns:
            Embedding vector if found, None otherwise
        """
        key = self._generate_key(text, model_name)
        
        try:
            # Get from Redis
            cached_data = self.redis_client.get(key)
            
            if cached_data:
                # Deserialize
                embedding = pickle.loads(cached_data)
                
                # Update TTL (extend expiration)
                self.redis_client.expire(key, self.ttl)
                
                self.hits += 1
                return embedding
        except:
            pass
        
        self.misses += 1
        return None
    
    def set(self, text: str, model_name: str, embedding: np.ndarray):
        """
        Set embedding in cache.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            embedding: Embedding vector to cache
        """
        key = self._generate_key(text, model_name)
        
        try:
            # Serialize and store in Redis with TTL
            serialized = pickle.dumps(embedding)
            self.redis_client.setex(key, self.ttl, serialized)
            
            # Check if cache is full and evict if necessary
            current_size = self.redis_client.dbsize()
            if current_size > self.max_size:
                self._evict_random(int(current_size * 0.1))  # Evict 10% of entries
        except:
            pass
    
    def _evict_random(self, count: int):
        """
        Randomly evict items from cache.
        
        Args:
            count: Number of items to evict
        """
        try:
            # Get random keys
            keys = [key.decode('utf-8') for key in self.redis_client.random_keys() * count]
            
            # Delete keys
            if keys:
                self.redis_client.delete(*keys)
        except:
            pass
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics.
        
        Returns:
            Dictionary containing cache statistics
        """
        total_requests = self.hits + self.misses
        hit_rate = self.hits / total_requests if total_requests > 0 else 0
        
        return {
            "size": self.redis_client.dbsize(),
            "max_size": self.max_size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": hit_rate,
            "ttl": self.ttl
        }
    
    def clear(self):
        """Clear the cache."""
        try:
            self.redis_client.flushdb()
        except:
            pass
        
        # Reset statistics
        self.hits = 0
        self.misses = 0

# Example 4: SQLite-based Embedding Cache
class SQLiteEmbeddingCache:
    """
    SQLite-based cache for storing text embeddings with metadata and persistence.
    """
    
    def __init__(self, db_path: str = "./embedding_cache.db"):
        """
        Initialize the SQLite-based embedding cache.
        
        Args:
            db_path: Path to the SQLite database file
        """
        self.db_path = db_path
        self.hits = 0
        self.misses = 0
        
        # Initialize database
        self._init_db()
    
    def _init_db(self):
        """Initialize the database schema."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Create cache table
            cursor.execute('''
            CREATE TABLE IF NOT EXISTS embedding_cache (
                key TEXT PRIMARY KEY,
                text_hash TEXT NOT NULL,
                model_name TEXT NOT NULL,
                embedding BLOB NOT NULL,
                created_at TIMESTAMP NOT NULL,
                last_accessed TIMESTAMP NOT NULL,
                access_count INTEGER DEFAULT 0
            )
            ''')
            
            # Create index on text_hash and model_name for faster lookups
            cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_text_model ON embedding_cache(text_hash, model_name)
            ''')
            
            conn.commit()
    
    def _generate_key(self, text: str, model_name: str) -> Tuple[str, str]:
        """
        Generate a unique key and text hash for the text and model.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            
        Returns:
            Tuple of (key, text_hash)
        """
        # Create hashes
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        key_data = f"{text_hash}:{model_name}".encode('utf-8')
        key = hashlib.md5(key_data).hexdigest()
        
        return key, text_hash
    
    def get(self, text: str, model_name: str) -> Optional[np.ndarray]:
        """
        Get embedding from cache.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            
        Returns:
            Embedding vector if found, None otherwise
        """
        key, text_hash = self._generate_key(text, model_name)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Get embedding from cache
            cursor.execute('''
            SELECT embedding FROM embedding_cache 
            WHERE key = ?
            ''', (key,))
            
            result = cursor.fetchone()
            
            if result:
                # Update access statistics
                cursor.execute('''
                UPDATE embedding_cache 
                SET last_accessed = CURRENT_TIMESTAMP, access_count = access_count + 1 
                WHERE key = ?
                ''', (key,))
                
                conn.commit()
                
                # Deserialize embedding
                embedding = pickle.loads(result[0])
                
                self.hits += 1
                return embedding
        
        self.misses += 1
        return None
    
    def set(self, text: str, model_name: str, embedding: np.ndarray):
        """
        Set embedding in cache.
        
        Args:
            text: Input text
            model_name: Name of the embedding model
            embedding: Embedding vector to cache
        """
        key, text_hash = self._generate_key(text, model_name)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Serialize embedding
            serialized = pickle.dumps(embedding)
            
            # Insert or replace in cache
            cursor.execute('''
            INSERT OR REPLACE INTO embedding_cache 
            (key, text_hash, model_name, embedding, created_at, last_accessed, access_count)
            VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, 1)
            ''', (key, text_hash, model_name, serialized))
            
            conn.commit()
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics.
        
        Returns:
            Dictionary containing cache statistics
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Get cache size
            cursor.execute('SELECT COUNT(*) FROM embedding_cache')
            size = cursor.fetchone()[0]
            
            # Get total access count
            cursor.execute('SELECT SUM(access_count) FROM embedding_cache')
            total_access = cursor.fetchone()[0] or 0
        
        total_requests = self.hits + self.misses
        hit_rate = self.hits / total_requests if total_requests > 0 else 0
        
        return {
            "size": size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": hit_rate,
            "total_access": total_access,
            "db_path": self.db_path
        }
    
    def clear(self):
        """Clear the cache."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Clear cache table
            cursor.execute('DELETE FROM embedding_cache')
            
            conn.commit()
        
        # Reset statistics
        self.hits = 0
        self.misses = 0
    
    def cleanup(self, max_age_days: int = 30):
        """
        Clean up old entries from the cache.
        
        Args:
            max_age_days: Maximum age of entries in days
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Delete old entries
            cursor.execute('''
            DELETE FROM embedding_cache 
            WHERE last_accessed < datetime('now', '-{} days')
            '''.format(max_age_days))
            
            conn.commit()

# Example 5: Healthcare Embedding Manager with Caching
class HealthcareEmbeddingManager:
    """
    Manager for generating and caching healthcare text embeddings.
    """
    
    def __init__(self, 
                 model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 cache_type: str = "memory",
                 cache_config: Dict[str, Any] = None):
        """
        Initialize the healthcare embedding manager.
        
        Args:
            model_name: Name of the embedding model
            cache_type: Type of cache ("memory", "file", "redis", "sqlite")
            cache_config: Configuration for the cache
        """
        self.model_name = model_name
        self.embedding_model = HuggingFaceEmbeddings(model_name=model_name)
        
        # Initialize cache based on type
        if cache_type == "memory":
            self.cache = InMemoryEmbeddingCache(**(cache_config or {}))
        elif cache_type == "file":
            self.cache = FileEmbeddingCache(**(cache_config or {}))
        elif cache_type == "redis":
            self.cache = RedisEmbeddingCache(**(cache_config or {}))
        elif cache_type == "sqlite":
            self.cache = SQLiteEmbeddingCache(**(cache_config or {}))
        else:
            raise ValueError(f"Unsupported cache type: {cache_type}")
    
    def embed_text(self, text: str) -> np.ndarray:
        """
        Generate embedding for text with caching.
        
        Args:
            text: Input text
            
        Returns:
            Embedding vector
        """
        # Try to get from cache first
        cached_embedding = self.cache.get(text, self.model_name)
        
        if cached_embedding is not None:
            return cached_embedding
        
        # Generate embedding
        embedding = np.array(self.embedding_model.embed_query(text))
        
        # Store in cache
        self.cache.set(text, self.model_name, embedding)
        
        return embedding
    
    def embed_documents(self, texts: List[str]) -> List[np.ndarray]:
        """
        Generate embeddings for multiple texts with caching.
        
        Args:
            texts: List of input texts
            
        Returns:
            List of embedding vectors
        """
        embeddings = []
        
        for text in texts:
            embedding = self.embed_text(text)
            embeddings.append(embedding)
        
        return embeddings
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics.
        
        Returns:
            Dictionary containing cache statistics
        """
        return self.cache.get_stats()
    
    def clear_cache(self):
        """Clear the cache."""
        self.cache.clear()

# Example 6: Healthcare Application with Embedding Caching
class HealthcareApplicationWithCaching:
    """
    Example healthcare application that uses embedding caching.
    """
    
    def __init__(self, embedding_manager: HealthcareEmbeddingManager):
        """
        Initialize the healthcare application.
        
        Args:
            embedding_manager: Healthcare embedding manager
        """
        self.embedding_manager = embedding_manager
        
        # Sample medical documents
        self.medical_documents = [
            "Patient presents with chest pain and shortness of breath. ECG shows ST elevation.",
            "Patient has a history of hypertension and diabetes. Currently on metformin and lisinopril.",
            "Patient complains of headache and nausea. No fever or neck stiffness.",
            "Patient underwent successful cardiac catheterization. No complications reported.",
            "Patient with chronic obstructive pulmonary disease experiencing exacerbation.",
            "Patient with type 2 diabetes and hypertension. HbA1c is 8.2%. Blood pressure is 150/90.",
            "Patient with asthma using albuterol inhaler as needed. Symptoms well controlled.",
            "Patient with history of myocardial infarction. Currently on aspirin, atorvastatin, and metoprolol."
        ]
        
        # Sample queries
        self.queries = [
            "heart problems breathing difficulties",
            "diabetes high blood sugar",
            "headache nausea symptoms",
            "asthma inhaler treatment",
            "heart attack medications"
        ]
    
    def process_documents(self):
        """Process all medical documents and generate embeddings."""
        print("Processing medical documents...")
        
        start_time = time.time()
        embeddings = self.embedding_manager.embed_documents(self.medical_documents)
        end_time = time.time()
        
        print(f"Processed {len(self.medical_documents)} documents in {end_time - start_time:.2f} seconds")
        print(f"Generated embeddings with dimension {len(embeddings[0])}")
        
        # Get cache stats
        cache_stats = self.embedding_manager.get_cache_stats()
        print(f"Cache stats: {cache_stats}")
    
    def process_queries(self):
        """Process all queries and generate embeddings."""
        print("\nProcessing queries...")
        
        start_time = time.time()
        embeddings = self.embedding_manager.embed_documents(self.queries)
        end_time = time.time()
        
        print(f"Processed {len(self.queries)} queries in {end_time - start_time:.2f} seconds")
        
        # Get cache stats
        cache_stats = self.embedding_manager.get_cache_stats()
        print(f"Cache stats: {cache_stats}")
    
    def demonstrate_caching(self):
        """Demonstrate the effect of caching by processing documents multiple times."""
        print("\nDemonstrating caching effect...")
        
        # Process documents for the first time
        print("\nFirst pass (cache miss expected):")
        start_time = time.time()
        self.embedding_manager.embed_documents(self.medical_documents)
        first_pass_time = time.time() - start_time
        
        cache_stats = self.embedding_manager.get_cache_stats()
        print(f"Time: {first_pass_time:.2f} seconds")
        print(f"Cache stats: {cache_stats}")
        
        # Process documents for the second time (should hit cache)
        print("\nSecond pass (cache hit expected):")
        start_time = time.time()
        self.embedding_manager.embed_documents(self.medical_documents)
        second_pass_time = time.time() - start_time
        
        cache_stats = self.embedding_manager.get_cache_stats()
        print(f"Time: {second_pass_time:.2f} seconds")
        print(f"Cache stats: {cache_stats}")
        
        # Calculate speedup
        if first_pass_time > 0:
            speedup = first_pass_time / second_pass_time
            print(f"\nSpeedup from caching: {speedup:.2f}x")

# Example usage
if __name__ == "__main__":
    # Example 1: In-memory cache
    print("Example 1: In-memory Embedding Cache")
    
    memory_cache = InMemoryEmbeddingCache(max_size=5)
    
    # Add some embeddings
    for i in range(6):
        text = f"Medical text {i}"
        embedding = np.random.rand(384)  # Random embedding for demonstration
        memory_cache.set(text, "test_model", embedding)
    
    # Check cache stats
    stats = memory_cache.get_stats()
    print(f"Cache stats after adding 6 items (max size 5): {stats}")
    
    # Try to get an item
    text = "Medical text 3"
    embedding = memory_cache.get(text, "test_model")
    print(f"Retrieved embedding for '{text}': {'Found' if embedding is not None else 'Not found'}")
    
    # Check cache stats again
    stats = memory_cache.get_stats()
    print(f"Cache stats after retrieval: {stats}")
    
    # Example 2: File-based cache
    print("\n\nExample 2: File-based Embedding Cache")
    
    file_cache = FileEmbeddingCache(cache_dir="./file_cache", max_size=5)
    
    # Add some embeddings
    for i in range(6):
        text = f"Medical text {i}"
        embedding = np.random.rand(384)  # Random embedding for demonstration
        file_cache.set(text, "test_model", embedding)
    
    # Check cache stats
    stats = file_cache.get_stats()
    print(f"Cache stats after adding 6 items (max size 5): {stats}")
    
    # Try to get an item
    text = "Medical text 3"
    embedding = file_cache.get(text, "test_model")
    print(f"Retrieved embedding for '{text}': {'Found' if embedding is not None else 'Not found'}")
    
    # Check cache stats again
    stats = file_cache.get_stats()
    print(f"Cache stats after retrieval: {stats}")
    
    # Clean up
    file_cache.clear()
    
    # Example 3: Redis cache (will be skipped if Redis is not available)
    print("\n\nExample 3: Redis-based Embedding Cache")
    
    try:
        redis_cache = RedisEmbeddingCache(max_size=5, ttl=60)
        
        # Add some embeddings
        for i in range(6):
            text = f"Medical text {i}"
            embedding = np.random.rand(384)  # Random embedding for demonstration
            redis_cache.set(text, "test_model", embedding)
        
        # Check cache stats
        stats = redis_cache.get_stats()
        print(f"Cache stats after adding 6 items (max size 5): {stats}")
        
        # Try to get an item
        text = "Medical text 3"
        embedding = redis_cache.get(text, "test_model")
        print(f"Retrieved embedding for '{text}': {'Found' if embedding is not None else 'Not found'}")
        
        # Check cache stats again
        stats = redis_cache.get_stats()
        print(f"Cache stats after retrieval: {stats}")
        
        # Clean up
        redis_cache.clear()
    except:
        print("Redis not available, skipping Redis cache example")
    
    # Example 4: SQLite cache
    print("\n\nExample 4: SQLite-based Embedding Cache")
    
    sqlite_cache = SQLiteEmbeddingCache(db_path="./sqlite_cache.db")
    
    # Add some embeddings
    for i in range(6):
        text = f"Medical text {i}"
        embedding = np.random.rand(384)  # Random embedding for demonstration
        sqlite_cache.set(text, "test_model", embedding)
    
    # Check cache stats
    stats = sqlite_cache.get_stats()
    print(f"Cache stats after adding 6 items: {stats}")
    
    # Try to get an item
    text = "Medical text 3"
    embedding = sqlite_cache.get(text, "test_model")
    print(f"Retrieved embedding for '{text}': {'Found' if embedding is not None else 'Not found'}")
    
    # Check cache stats again
    stats = sqlite_cache.get_stats()
    print(f"Cache stats after retrieval: {stats}")
    
    # Clean up
    sqlite_cache.clear()
    
    # Example 5: Healthcare embedding manager with different cache types
    print("\n\nExample 5: Healthcare Embedding Manager with Caching")
    
    # Test with in-memory cache
    print("\nTesting with in-memory cache:")
    memory_manager = HealthcareEmbeddingManager(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        cache_type="memory",
        cache_config={"max_size": 10}
    )
    
    app = HealthcareApplicationWithCaching(memory_manager)
    app.process_documents()
    app.process_queries()
    app.demonstrate_caching()
    
    # Test with file-based cache
    print("\n\nTesting with file-based cache:")
    file_manager = HealthcareEmbeddingManager(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        cache_type="file",
        cache_config={"cache_dir": "./healthcare_cache", "max_size": 10}
    )
    
    app = HealthcareApplicationWithCaching(file_manager)
    app.process_documents()
    app.process_queries()
    app.demonstrate_caching()
    
    # Test with SQLite cache
    print("\n\nTesting with SQLite cache:")
    sqlite_manager = HealthcareEmbeddingManager(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        cache_type="sqlite",
        cache_config={"db_path": "./healthcare_cache.db"}
    )
    
    app = HealthcareApplicationWithCaching(sqlite_manager)
    app.process_documents()
    app.process_queries()
    app.demonstrate_caching()
    
    # Clean up cache files
    import shutil
    for cache_dir in ["./file_cache", "./healthcare_cache"]:
        if os.path.exists(cache_dir):
            shutil.rmtree(cache_dir)
    
    if os.path.exists("./sqlite_cache.db"):
        os.remove("./sqlite_cache.db")
    
    if os.path.exists("./healthcare_cache.db"):
        os.remove("./healthcare_cache.db")
</code></pre>
                            
                            <div class="code-explanation">
                                <h5><i class="fas fa-code me-2"></i>Code Explanation</h5>
                                <p>This code demonstrates various approaches to implementing embedding caching for healthcare applications:</p>
                                <ol>
                                    <li><strong>InMemoryEmbeddingCache:</strong> This class implements an in-memory cache using an OrderedDict to support LRU (Least Recently Used) eviction. It's simple and fast but limited by available memory and not persistent across application restarts. This is suitable for small-scale applications or as a component within larger systems.</li>
                                    <li><strong>FileEmbeddingCache:</strong> This class implements a file-based cache that persists embeddings to disk. It uses a JSON index file to track metadata about cached embeddings and stores each embedding in a separate pickle file. This approach provides persistence and can handle larger datasets than in-memory caching.</li>
                                    <li><strong>RedisEmbeddingCache:</strong> This class implements a distributed cache using Redis, a popular in-memory data store. It supports distributed access across multiple servers and includes TTL (Time To Live) functionality to automatically expire old entries. This is suitable for large-scale healthcare applications that need to handle high volumes of requests.</li>
                                    <li><strong>SQLiteEmbeddingCache:</strong> This class implements a cache using SQLite, a lightweight relational database. It stores embeddings as BLOB data along with metadata like creation time, last access time, and access count. This approach provides persistence, efficient querying, and the ability to perform more complex operations on the cache data.</li>
                                    <li><strong>HealthcareEmbeddingManager:</strong> This class provides a high-level interface for generating and caching healthcare text embeddings. It can use any of the cache implementations and handles the logic of checking the cache before generating embeddings and storing them after generation.</li>
                                    <li><strong>HealthcareApplicationWithCaching:</strong> This example class demonstrates how embedding caching can be used in a healthcare application. It shows the performance benefits of caching by processing documents multiple times and measuring the speedup achieved.</li>
                                </ol>
                                <p>The code includes comprehensive examples showing how to use each cache type, from basic operations to demonstrating the performance benefits of caching. The examples show how caching can significantly reduce processing time for repeated operations, which is crucial for healthcare applications that need to process large volumes of medical text efficiently.</p>
                            </div>
                            
                            <div class="industry-example">
                                <h5><i class="fas fa-hospital-alt me-2"></i>Real-world Healthcare Scenario</h5>
                                <p>Consider a large hospital system implementing an AI-powered clinical decision support system that needs to process and analyze various types of medical text:</p>
                                <ul>
                                    <li>Clinical notes from electronic health records</li>
                                    <li>Medical literature and research papers</li>
                                    <li>Clinical guidelines and best practices</li>
                                    <li>Patient-generated health data from wearables and patient portals</li>
                                </ul>
                                <p>The clinical decision support system needs to provide real-time recommendations to clinicians as they make treatment decisions. This requires quickly finding relevant medical literature and similar patient cases based on the current patient's condition and history.</p>
                                <p>Without embedding caching, the system would need to generate embeddings for all relevant medical literature and patient cases each time a clinician requests information. This would result in unacceptable response times, especially during busy periods when multiple clinicians are using the system simultaneously.</p>
                                <p>By implementing embedding caching as demonstrated in the code, the hospital system can:</p>
                                <ol>
                                    <li><strong>Pre-compute and cache embeddings</strong> for frequently accessed medical literature and patient cases, ensuring they are immediately available when needed.</li>
                                    <li><strong>Cache embeddings on-the-fly</strong> as clinicians access new information, building a knowledge base that improves over time.</li>
                                    <li><strong>Use distributed caching</strong> with Redis to ensure high availability and scalability, allowing the system to handle peak loads without performance degradation.</li>
                                    <li><strong>Implement persistence</strong> with SQLite or file-based caching to ensure that embeddings are not lost when the system restarts, preserving the accumulated knowledge.</li>
                                    <li><strong>Monitor cache performance</strong> using statistics to optimize cache size and eviction policies, ensuring the most relevant content remains readily accessible.</li>
                                </ol>
                                <p>The result is a clinical decision support system that provides real-time recommendations to clinicians, improving the timeliness and quality of care. The system can handle high volumes of requests without performance issues, and it becomes more effective over time as it builds a comprehensive cache of medical knowledge and patient cases.</p>
                                <p>This approach also extends to other healthcare applications, such as medical research platforms, patient monitoring systems, and clinical trial matching systems, all of which benefit from the efficiency and scalability provided by embedding caching.</p>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Conclusion -->
                <section id="conclusion" class="mb-5">
                    <h2 class="section-header">Conclusion</h2>
                    <p>Data Management & Processing forms the critical foundation for any advanced AI application in healthcare. As we've explored throughout this level, effective handling of healthcare datafrom ingestion to transformationenables organizations to unlock the full potential of their data while addressing the unique challenges of the healthcare industry.</p>
                    
                    <div class="industry-example">
                        <h5><i class="fas fa-hospital-alt me-2"></i>Healthcare Industry Impact</h5>
                        <p>The implementation of robust data management and processing techniques in healthcare has transformative effects:</p>
                        <ul>
                            <li><strong>Improved Patient Outcomes:</strong> By enabling more accurate clinical decision support and personalized treatment plans, these techniques directly contribute to better patient care and outcomes.</li>
                            <li><strong>Enhanced Operational Efficiency:</strong> Automation of data ingestion, processing, and transformation reduces manual effort, allowing healthcare professionals to focus on patient care rather than administrative tasks.</li>
                            <li><strong>Accelerated Medical Research:</strong> Efficient processing and analysis of medical literature and patient data accelerate the discovery process, leading to new treatments and understanding of diseases.</li>
                            <li><strong>Better Resource Utilization:</strong> By optimizing data workflows and reducing redundant computations, healthcare organizations can make better use of their computational and human resources.</li>
                            <li><strong>Regulatory Compliance:</strong> Proper data management ensures that healthcare organizations can meet regulatory requirements for data privacy, security, and quality.</li>
                        </ul>
                    </div>
                    
                    <div class="business-case">
                        <h5><i class="fas fa-lightbulb me-2"></i>Key Takeaways</h5>
                        <ol>
                            <li><strong>Data Ingestion is the Foundation:</strong> Effective document loadersboth predefined and customenable healthcare organizations to integrate data from diverse sources, breaking down data silos and creating a unified view of patient information.</li>
                            <li><strong>Data Processing Enables Analysis:</strong> Text splitters and message processing techniques prepare healthcare data for analysis by breaking it into manageable chunks, filtering irrelevant content, and maintaining context. This is essential for working with the complex and often unstructured nature of healthcare data.</li>
                            <li><strong>Data Transformation Unlocks Value:</strong> Embedding models, vector representations, and caching strategies transform raw healthcare data into formats that enable sophisticated AI applications, from semantic search to clinical decision support.</li>
                            <li><strong>Healthcare-Specific Considerations are Crucial:</strong> Healthcare data has unique characteristicssensitivity, complexity, and regulatory requirementsthat must be addressed through specialized techniques and careful implementation.</li>
                            <li><strong>Performance and Scalability Matter:</strong> As healthcare datasets grow and real-time requirements increase, techniques like embedding caching become essential for maintaining performance and controlling costs.</li>
                        </ol>
                    </div>
                    
                    <div class="problem-solved">
                        <h5><i class="fas fa-exclamation-triangle me-2"></i>Looking Ahead</h5>
                        <p>As healthcare continues to evolve, data management and processing techniques will need to adapt to new challenges and opportunities:</p>
                        <ul>
                            <li><strong>Integration of New Data Sources:</strong> Wearables, genomics, and social determinants of health will require new approaches to data ingestion and processing.</li>
                            <li><strong>Real-time Processing:</strong> As healthcare moves toward real-time monitoring and intervention, data processing techniques will need to become even more efficient and responsive.</li>
                            <li><strong>Federated Learning:</strong> Privacy concerns will drive the adoption of federated learning approaches, where models are trained across multiple healthcare institutions without sharing raw data.</li>
                            <li><strong>Explainable AI:</strong> As AI becomes more prevalent in healthcare, there will be increasing demand for techniques that make AI decisions transparent and interpretable.</li>
                            <li><strong>Interoperability Standards:</strong> The development and adoption of universal healthcare data standards will simplify data integration and processing across the industry.</li>
                        </ul>
                    </div>
                    
                    <p>By mastering the techniques covered in this levelData Ingestion, Data Processing, and Data Transformationhealthcare organizations can build a solid foundation for AI applications that improve patient care, advance medical research, and optimize healthcare operations. The journey from raw data to actionable insights is complex, but with the right tools and approaches, it's a journey that can transform healthcare delivery and outcomes.</p>
                </section>
            </div>
        </div>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="row">
                <div class="col-md-6">
                    <h5>Data Management & Processing in Healthcare</h5>
                    <p>Level 4 of the AI Implementation Framework</p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p>&copy; 2023 AI in Healthcare. All rights reserved.</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    window.scrollTo({
                        top: target.offsetTop - 70,
                        behavior: 'smooth'
                    });
                }
            });
        });
        
        // Active navigation highlighting
        window.addEventListener('scroll', () => {
            let current = '';
            const sections = document.querySelectorAll('section[id]');
            
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.clientHeight;
                if (scrollY >= (sectionTop - 100)) {
                    current = section.getAttribute('id');
                }
            });
            
            document.querySelectorAll('.navbar-nav a').forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href').slice(1) === current) {
                    link.classList.add('active');
                }
            });
        });
        
        // Code copy functionality
        document.addEventListener('DOMContentLoaded', function() {
            const codeBlocks = document.querySelectorAll('pre code');
            
            codeBlocks.forEach(function(codeBlock) {
                const copyButton = document.createElement('button');
                copyButton.className = 'btn btn-sm btn-outline-secondary position-absolute top-0 end-0 m-2';
                copyButton.innerHTML = '<i class="fas fa-copy"></i>';
                copyButton.style.opacity = '0';
                copyButton.style.transition = 'opacity 0.2s';
                
                const pre = codeBlock.parentNode;
                pre.style.position = 'relative';
                pre.appendChild(copyButton);
                
                pre.addEventListener('mouseenter', function() {
                    copyButton.style.opacity = '1';
                });
                
                pre.addEventListener('mouseleave', function() {
                    copyButton.style.opacity = '0';
                });
                
                copyButton.addEventListener('click', function() {
                    const text = codeBlock.textContent;
                    navigator.clipboard.writeText(text).then(function() {
                        copyButton.innerHTML = '<i class="fas fa-check"></i>';
                        setTimeout(function() {
                            copyButton.innerHTML = '<i class="fas fa-copy"></i>';
                        }, 2000);
                    });
                });
            });
        });
    </script>
</body>
</html>
