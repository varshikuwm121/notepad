<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Level 10: Production & Optimization</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        :root {
            --primary-color: #2563eb;
            --secondary-color: #1e40af;
            --accent-color: #3b82f6;
            --dark-bg: #0f172a;
            --card-bg: #1e293b;
            --text-light: #e2e8f0;
            --text-muted: #94a3b8;
            --border-color: #334155;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: var(--dark-bg);
            color: var(--text-light);
            line-height: 1.6;
        }
        
        .main-header {
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        
        .section-card {
            background-color: var(--card-bg);
            border-radius: 12px;
            padding: 2rem;
            margin-bottom: 2rem;
            border: 1px solid var(--border-color);
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        
        .section-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 15px rgba(0, 0, 0, 0.1);
        }
        
        .component-card {
            background-color: rgba(30, 41, 59, 0.5);
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            border-left: 4px solid var(--accent-color);
        }
        
        h1 {
            font-weight: 700;
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        h2 {
            font-weight: 600;
            font-size: 2rem;
            color: var(--accent-color);
            margin-top: 2rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--border-color);
        }
        
        h3 {
            font-weight: 600;
            font-size: 1.5rem;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            color: var(--text-light);
        }
        
        .component-title {
            font-weight: 600;
            font-size: 1.3rem;
            color: var(--accent-color);
            margin-bottom: 1rem;
        }
        
        .info-section {
            margin-bottom: 1.5rem;
        }
        
        .info-title {
            font-weight: 600;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
        }
        
        .info-title i {
            margin-right: 0.5rem;
        }
        
        .business-case {
            color: #10b981;
        }
        
        .problems-solved {
            color: #f59e0b;
        }
        
        .impact-absent {
            color: #ef4444;
        }
        
        .predefined-custom {
            color: #8b5cf6;
        }
        
        .relationship {
            color: #06b6d4;
        }
        
        .technical-insight {
            color: #ec4899;
        }
        
        pre {
            background-color: #0d1117 !important;
            border-radius: 8px !important;
            padding: 1.5rem !important;
            overflow-x: auto !important;
            margin: 1.5rem 0 !important;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1) !important;
            max-height: 500px !important;
        }
        
        code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            color: #e6edf3 !important;
        }
        
        .keyword-badge {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 9999px;
            font-size: 0.875rem;
            font-weight: 600;
            margin-left: 0.5rem;
        }
        
        .foundation {
            background-color: rgba(59, 130, 246, 0.2);
            color: #93c5fd;
        }
        
        .execution {
            background-color: rgba(16, 185, 129, 0.2);
            color: #6ee7b7;
        }
        
        .control {
            background-color: rgba(245, 158, 11, 0.2);
            color: #fcd34d;
        }
        
        .interface {
            background-color: rgba(139, 92, 246, 0.2);
            color: #c4b5fd;
        }
        
        .toc {
            background-color: rgba(30, 41, 59, 0.5);
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 2rem;
            position: sticky;
            top: 1rem;
        }
        
        .toc ul {
            list-style-type: none;
            padding-left: 0;
        }
        
        .toc li {
            margin-bottom: 0.5rem;
        }
        
        .toc a {
            color: var(--text-light);
            text-decoration: none;
            transition: color 0.3s ease;
        }
        
        .toc a:hover {
            color: var(--accent-color);
        }
        
        .domain-example {
            background-color: rgba(59, 130, 246, 0.1);
            border-left: 3px solid var(--accent-color);
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 0 8px 8px 0;
        }
        
        .footer {
            background-color: var(--card-bg);
            padding: 2rem 0;
            margin-top: 3rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            color: var(--text-muted);
        }
    </style>
</head>
<body>
    <header class="main-header">
        <div class="container">
            <h1>Level 10: Production & Optimization</h1>
            <p class="lead">Build on all previous - deploy and maintain systems</p>
        </div>
    </header>

    <div class="container">
        <div class="row">
            <div class="col-lg-3">
                <div class="toc">
                    <h3>Table of Contents</h3>
                    <ul>
                        <li><a href="#section-101">10.1 Performance Optimization</a></li>
                        <li><a href="#section-102">10.2 Monitoring & Observability</a></li>
                        <li><a href="#section-103">10.3 Quality Assurance</a></li>
                        <li><a href="#section-104">10.4 Deployment & Integration</a></li>
                    </ul>
                </div>
            </div>
            
            <div class="col-lg-9">
                <!-- Section 10.1 -->
                <section id="section-101" class="section-card">
                    <h2>10.1 Performance Optimization</h2>
                    
                    <!-- Component 10.1.1 -->
                    <div class="component-card">
                        <h3 class="component-title">10.1.1 Caching Strategies</h3>
                        
                        <div class="info-section">
                            <div class="info-title business-case">
                                <i class="fas fa-check-circle"></i> Business Use Case
                            </div>
                            <p>In the banking industry, caching strategies significantly improve customer experience by reducing response times for frequent queries. For example, a major retail bank implemented response caching for their AI-powered virtual assistant that handles customer inquiries about account balances, transaction history, and loan eligibility. This resulted in a 60% reduction in response time for repeated questions and a 40% decrease in computational costs, while maintaining 99.9% accuracy for cached responses.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title problems-solved">
                                <i class="fas fa-lightbulb"></i> Problems Solved
                            </div>
                            <p>Caching strategies solve the critical business challenge of computational resource optimization and response latency. In production environments, repeated queries to language models consume significant computational resources and create bottlenecks that affect user experience. By implementing intelligent caching, businesses can reduce API calls, lower operational costs, and deliver faster responses to end-users without sacrificing the quality of interactions.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title impact-absent">
                                <i class="fas fa-exclamation-triangle"></i> Impact if Absent
                            </div>
                            <p>Without caching strategies, businesses would face exponentially higher operational costs as user scales increase. Each user query would require fresh computation, leading to slower response times and potential system overload during peak usage. In a healthcare context, this could mean delayed responses to critical patient inquiries, increased infrastructure costs by 3-5x, and inability to scale services efficiently during health crises or peak demand periods.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title predefined-custom">
                                <i class="fas fa-cogs"></i> Predefined or Custom
                            </div>
                            <p>Custom</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title relationship">
                                <i class="fas fa-link"></i> Relationship Keyword
                            </div>
                            <p>Execution</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title technical-insight">
                                <i class="fas fa-microchip"></i> In-depth Technical Insight
                            </div>
                            <p>Caching in LangChain applications involves storing model responses or intermediate computations to avoid redundant processing. There are several types of caching strategies:</p>
                            
                            <pre><code>from langchain.cache import InMemoryCache, SQLiteCache
from langchain.globals import set_llm_cache
import langchain
from datetime import timedelta
from typing import Optional, Dict, Any
import hashlib
import json

# Configure different types of caches
# 1. In-memory cache (suitable for single-process applications)
set_llm_cache(InMemoryCache())

# 2. SQLite cache (persists across sessions)
# set_llm_cache(SQLiteCache(database_path=".langchain.db"))

# 3. Custom Redis cache for distributed systems
import redis
from langchain.cache import RedisCache

# Connect to Redis
redis_client = redis.Redis(host='localhost', port=6379, db=0)
set_llm_cache(RedisCache(redis_client))

# Custom caching strategy with semantic similarity
class SemanticCache:
    def __init__(self, similarity_threshold=0.95):
        self.cache = {}
        self.similarity_threshold = similarity_threshold
        # In a real implementation, you would use embeddings and similarity search
        # This is a simplified version for demonstration
        
    def _generate_key(self, prompt: str, **kwargs) -> str:
        """Generate a consistent key for caching"""
        # Create a deterministic hash of the prompt and parameters
        cache_data = {"prompt": prompt, **kwargs}
        serialized = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(serialized.encode()).hexdigest()
    
    def lookup(self, prompt: str, **kwargs) -> Optional[str]:
        """Look up a response in the cache"""
        key = self._generate_key(prompt, **kwargs)
        return self.cache.get(key)
    
    def update(self, prompt: str, response: str, **kwargs) -> None:
        """Update the cache with a new response"""
        key = self._generate_key(prompt, **kwargs)
        self.cache[key] = response
    
    def clear(self) -> None:
        """Clear the cache"""
        self.cache.clear()

# Time-based cache eviction
class TimedCache:
    def __init__(self, ttl: timedelta = timedelta(hours=1)):
        self.cache = {}
        self.ttl = ttl
        
    def _is_expired(self, timestamp) -> bool:
        """Check if a cache entry has expired"""
        return datetime.now() - timestamp > self.ttl
    
    def lookup(self, prompt: str, **kwargs) -> Optional[str]:
        """Look up a response in the cache if not expired"""
        key = self._generate_key(prompt, **kwargs)
        if key in self.cache:
            response, timestamp = self.cache[key]
            if not self._is_expired(timestamp):
                return response
            else:
                # Remove expired entry
                del self.cache[key]
        return None
    
    def update(self, prompt: str, response: str, **kwargs) -> None:
        """Update the cache with a new response and timestamp"""
        key = self._generate_key(prompt, **kwargs)
        self.cache[key] = (response, datetime.now())
    
    def _generate_key(self, prompt: str, **kwargs) -> str:
        """Generate a consistent key for caching"""
        cache_data = {"prompt": prompt, **kwargs}
        serialized = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(serialized.encode()).hexdigest()

# Example usage with a language model
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

# Set up caching
semantic_cache = SemanticCache()
timed_cache = TimedCache(ttl=timedelta(minutes=30))

llm = ChatOpenAI(model="gpt-4", temperature=0)

def get_cached_response(prompt: str, system_message: str = None, use_cache=True):
    """Get a response from the LLM with caching"""
    # Prepare messages
    messages = []
    if system_message:
        messages.append(SystemMessage(content=system_message))
    messages.append(HumanMessage(content=prompt))
    
    # Check cache first
    if use_cache:
        cached_response = semantic_cache.lookup(prompt)
        if cached_response:
            print("Retrieved from cache")
            return cached_response
    
    # Get fresh response
    response = llm.invoke(messages).content
    
    # Update cache
    if use_cache:
        semantic_cache.update(prompt, response)
        timed_cache.update(prompt, response)
    
    return response

# Example in a banking context
banking_prompt = "What are the current interest rates for home loans?"
system_message = "You are a helpful banking assistant. Provide accurate, up-to-date information about banking products."

# First call - not cached
response1 = get_cached_response(banking_prompt, system_message)
print(f"First response: {response1}")

# Second call - should be cached
response2 = get_cached_response(banking_prompt, system_message)
print(f"Second response: {response2}")

# Cache statistics and monitoring
def get_cache_stats():
    """Get statistics about cache performance"""
    return {
        "semantic_cache_size": len(semantic_cache.cache),
        "timed_cache_size": len(timed_cache.cache),
        "hit_rate": calculate_hit_rate()  # This would be implemented in a real system
    }

print(f"Cache stats: {get_cache_stats()}")

# Cache invalidation strategy for banking data
def invalidate_cache_for_data_update(data_type: str):
    """Invalidate cache entries when underlying data changes"""
    # In a banking system, this would be called when interest rates change
    # or when new banking products are introduced
    
    if data_type == "interest_rates":
        # Find and remove all cache entries related to interest rates
        keys_to_remove = [k for k in semantic_cache.cache.keys() 
                          if "interest rate" in k.lower() or "loan" in k.lower()]
        for key in keys_to_remove:
            if key in semantic_cache.cache:
                del semantic_cache.cache[key]
            if key in timed_cache.cache:
                del timed_cache.cache[key]
        
        print(f"Invalidated {len(keys_to_remove)} cache entries related to interest rates")
    
    # Similar logic for other data types</code></pre>
                            
                            <p>In a production banking environment, this caching implementation would be extended with:</p>
                            <ul>
                                <li>Distributed caching using Redis or Memcached for horizontal scalability</li>
                                <li>Cache invalidation hooks connected to the bank's core systems to ensure data freshness</li>
                                <li>Performance monitoring to track cache hit rates and optimize TTL values</li>
                                <li>Security measures to ensure sensitive financial data is properly handled in the cache</li>
                            </ul>
                        </div>
                    </div>
                    
                    <!-- Component 10.1.2 -->
                    <div class="component-card">
                        <h3 class="component-title">10.1.2 Batch Processing</h3>
                        
                        <div class="info-section">
                            <div class="info-title business-case">
                                <i class="fas fa-check-circle"></i> Business Use Case
                            </div>
                            <p>In the healthcare industry, batch processing enables efficient analysis of large volumes of patient data. A regional hospital implemented batch processing to analyze thousands of medical records overnight, identifying potential drug interactions and flagging patients at risk for specific conditions. This system processes 50,000+ patient records in a single batch run, reducing processing time from 72 hours to just 8 hours, and enabling physicians to receive critical insights before morning rounds, ultimately improving patient outcomes by 23%.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title problems-solved">
                                <i class="fas fa-lightbulb"></i> Problems Solved
                            </div>
                            <p>Batch processing addresses the challenge of efficiently handling large volumes of data processing tasks that would be impractical to process individually. In production environments, processing each request individually creates significant overhead and limits throughput. Batch processing optimizes resource utilization by grouping similar operations together, reducing API calls, and minimizing computational overhead, which is essential for cost-effective scaling of AI applications.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title impact-absent">
                                <i class="fas fa-exclamation-triangle"></i> Impact if Absent
                            </div>
                            <p>Without batch processing, organizations would face prohibitive costs and impractical timeframes when processing large datasets. In a logistics context, this would mean analyzing delivery routes and optimizing shipments one at a time, potentially taking days instead of hours for daily operations. This would result in missed delivery windows, increased fuel consumption by 15-20%, higher operational costs, and inability to respond to changing conditions in real-time, ultimately degrading service quality and customer satisfaction.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title predefined-custom">
                                <i class="fas fa-cogs"></i> Predefined or Custom
                            </div>
                            <p>Custom</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title relationship">
                                <i class="fas fa-link"></i> Relationship Keyword
                            </div>
                            <p>Execution</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title technical-insight">
                                <i class="fas fa-microchip"></i> In-depth Technical Insight
                            </div>
                            <p>Batch processing in LangChain involves grouping multiple operations together to process them efficiently, reducing overhead and improving throughput. This is particularly important when dealing with large datasets or when processing multiple similar requests.</p>
                            
                            <pre><code>from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain.callbacks import get_openai_callback
import asyncio
from typing import List, Dict, Any, Tuple
import time
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
import pandas as pd

@dataclass
class BatchResult:
    """Container for batch processing results"""
    input_data: Any
    output: Any
    processing_time: float
    success: bool
    error_message: str = None

class BatchProcessor:
    def __init__(self, llm, batch_size=10, max_workers=4):
        self.llm = llm
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.results = []
        
    def process_batch(self, inputs: List[str], prompt_template: str) -> List[BatchResult]:
        """Process a batch of inputs using the provided prompt template"""
        # Create the prompt template
        prompt = PromptTemplate(
            input_variables=["input_text"],
            template=prompt_template
        )
        
        # Create the chain
        chain = LLMChain(llm=self.llm, prompt=prompt)
        
        # Process in batches
        batch_results = []
        for i in range(0, len(inputs), self.batch_size):
            batch = inputs[i:i + self.batch_size]
            batch_start_time = time.time()
            
            try:
                with get_openai_callback() as cb:
                    # Process the batch
                    batch_outputs = chain.apply([{"input_text": text} for text in batch])
                
                batch_processing_time = time.time() - batch_start_time
                
                # Create result objects for each item in the batch
                for j, output in enumerate(batch_outputs):
                    batch_results.append(BatchResult(
                        input_data=batch[j],
                        output=output['text'],
                        processing_time=batch_processing_time / len(batch),
                        success=True
                    ))
                
                print(f"Processed batch {i//self.batch_size + 1}/{(len(inputs)-1)//self.batch_size + 1} in {batch_processing_time:.2f}s")
                print(f"Tokens used: {cb.total_tokens}, Cost: ${cb.total_cost:.4f}")
                
            except Exception as e:
                batch_processing_time = time.time() - batch_start_time
                # If the entire batch fails, create failure results for each item
                for text in batch:
                    batch_results.append(BatchResult(
                        input_data=text,
                        output=None,
                        processing_time=batch_processing_time / len(batch),
                        success=False,
                        error_message=str(e)
                    ))
        
        return batch_results
    
    def process_batch_parallel(self, inputs: List[str], prompt_template: str) -> List[BatchResult]:
        """Process inputs in parallel using ThreadPoolExecutor"""
        # Split inputs into batches
        batches = [inputs[i:i + self.batch_size] for i in range(0, len(inputs), self.batch_size)]
        
        # Create the prompt template and chain
        prompt = PromptTemplate(
            input_variables=["input_text"],
            template=prompt_template
        )
        chain = LLMChain(llm=self.llm, prompt=prompt)
        
        batch_results = []
        
        # Process batches in parallel
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all batches
            future_to_batch = {
                executor.submit(self._process_single_batch, batch, chain): i 
                for i, batch in enumerate(batches)
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_batch):
                batch_idx = future_to_batch[future]
                try:
                    results = future.result()
                    batch_results.extend(results)
                    print(f"Completed batch {batch_idx + 1}/{len(batches)}")
                except Exception as e:
                    print(f"Batch {batch_idx + 1}/{len(batches)} generated an exception: {e}")
                    # Create failure results for this batch
                    batch = batches[batch_idx]
                    for text in batch:
                        batch_results.append(BatchResult(
                            input_data=text,
                            output=None,
                            processing_time=0,
                            success=False,
                            error_message=str(e)
                        ))
        
        return batch_results
    
    def _process_single_batch(self, batch: List[str], chain) -> List[BatchResult]:
        """Process a single batch and return results"""
        batch_start_time = time.time()
        
        try:
            with get_openai_callback() as cb:
                # Process the batch
                batch_outputs = chain.apply([{"input_text": text} for text in batch])
            
            batch_processing_time = time.time() - batch_start_time
            
            # Create result objects for each item in the batch
            results = []
            for j, output in enumerate(batch_outputs):
                results.append(BatchResult(
                    input_data=batch[j],
                    output=output['text'],
                    processing_time=batch_processing_time / len(batch),
                    success=True
                ))
            
            return results
            
        except Exception as e:
            batch_processing_time = time.time() - batch_start_time
            # If the entire batch fails, create failure results for each item
            results = []
            for text in batch:
                results.append(BatchResult(
                    input_data=text,
                    output=None,
                    processing_time=batch_processing_time / len(batch),
                    success=False,
                    error_message=str(e)
                ))
            return results
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get processing statistics"""
        if not self.results:
            return {"error": "No results available"}
        
        total_items = len(self.results)
        successful_items = sum(1 for r in self.results if r.success)
        failed_items = total_items - successful_items
        avg_processing_time = sum(r.processing_time for r in self.results) / total_items
        
        return {
            "total_items": total_items,
            "successful_items": successful_items,
            "failed_items": failed_items,
            "success_rate": successful_items / total_items * 100,
            "average_processing_time": avg_processing_time,
            "total_processing_time": sum(r.processing_time for r in self.results)
        }
    
    def reset_metrics(self) -> None:
        """Reset performance metrics"""
        self.results = []
    
    def export_results(self, filepath: str) -> None:
        """Export results to a file"""
        # Convert results to a format suitable for export
        export_data = []
        for result in self.results:
            export_data.append({
                "input": result.input_data,
                "output": result.output,
                "processing_time": result.processing_time,
                "success": result.success,
                "error_message": result.error_message
            })
        
        # Write to file
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        print(f"Results exported to {filepath}")

# Example usage in a healthcare context
def batch_processing_example():
    """Example of batch processing with resource management"""
    # Create batch resource manager
    batch_manager = BatchResourceManager(
        max_cpu_percent=75.0,
        max_memory_percent=75.0,
        max_api_calls_per_minute=30,
        batch_size=3
    )
    
    # Start monitoring
    batch_manager.start_monitoring()
    
    # Create a resource-aware LLM
    batch_llm = ResourceAwareLLM(base_llm, batch_manager)
    
    # Define a function to process a logistics query
    def process_query(query):
        messages = [
            SystemMessage(content=logistics_system_message),
            HumanMessage(content=query)
        ]
        return batch_llm.invoke(messages)
    
    # Example patient data for batch processing
    patient_notes = [
        "Patient is a 65-year-old male with history of hypertension and diabetes. Presenting with chest pain and shortness of breath. ECG shows ST elevation.",
        "45-year-old female with previous allergic reaction to penicillin. Complaining of fever and cough. Chest X-ray shows right lower lobe infiltrate.",
        "72-year-old male with COPD and congestive heart failure. Admitted with increased dyspnea and weight gain. Oxygen saturation 88% on room air.",
        "58-year-old female with history of breast cancer, currently on chemotherapy. Presenting with neutropenic fever. WBC count 1.2.",
        "63-year-old male with history of stroke and atrial fibrillation. Admitted with left-sided weakness and slurred speech. Symptoms started 3 hours ago.",
        "39-year-old female with no significant past medical history. Presenting with severe abdominal pain and vomiting. CT abdomen shows appendicitis.",
        "81-year-old female with dementia and hypertension. Fell at home, found on floor by family. X-ray shows right hip fracture.",
        "50-year-old male with history of alcohol abuse. Presenting with jaundice and abdominal distension. Liver function tests elevated.",
        "29-year-old pregnant female at 32 weeks gestation. Presenting with headache and blurred vision. Blood pressure 160/100.",
        "67-year-old male with history of prostate cancer and radiation therapy. Presenting with urinary retention and hematuria."
    ]
    
    # Process the batch
    print("Processing patient notes in batches...")
    results = []

    for i, note in enumerate(patient_notes):
        print(f"\nProcessing note {i+1}: {note[:50]}...")
        
        try:
            # Add to batch
            batch_manager.add_to_batch(process_query, (note,))
            
        except Exception as e:
            print(f"Error: {str(e)}")
    
    # Wait for batch processing to complete
    while batch_manager.batch_processing:
        time.sleep(0.5)
    
    # Stop monitoring
    batch_manager.stop_monitoring()
    
    # Get statistics
    stats = batch_manager.get_statistics()
    print(f"\nProcessing Statistics:")
    print(f"Total items processed: {stats['total_items']}")
    print(f"Success rate: {stats['success_rate']:.2f}%")
    print(f"Average processing time per item: {stats['average_processing_time']:.2f} seconds")
    print(f"Total processing time: {stats['total_processing_time']:.2f} seconds")

# Run the batch processing example
batch_processing_example()</code></pre>
                            
                            <p>In a production healthcare environment, this batch processing implementation would be extended with:</p>
                            <ul>
                                <li>Integration with Electronic Health Record (EHR) systems for automated data extraction</li>
                                <li>Advanced error handling and retry mechanisms for critical medical analyses</li>
                                <li>Compliance with HIPAA and other healthcare data privacy regulations</li>
                                <li>Priority processing for urgent cases based on clinical assessment</li>
                                <li>Integration with clinical decision support systems</li>
                            </ul>
                        </div>
                    </div>
                    
                    <!-- Component 10.1.3 -->
                    <div class="component-card">
                        <h3 class="component-title">10.1.3 Streaming Optimization</h3>
                        
                        <div class="info-section">
                            <div class="info-title business-case">
                                <i class="fas fa-check-circle"></i> Business Use Case
                            </div>
                            <p>In the retail industry, streaming optimization enables real-time customer service interactions that significantly enhance the shopping experience. A major e-commerce platform implemented streaming optimization for their AI-powered shopping assistant, allowing customers to receive instant responses as they type queries about products, availability, and recommendations. This implementation reduced perceived response time by 80%, increased customer engagement by 35%, and improved conversion rates by 18% as customers received immediate, helpful information during their decision-making process.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title problems-solved">
                                <i class="fas fa-lightbulb"></i> Problems Solved
                            </div>
                            <p>Streaming optimization addresses the critical challenge of latency in user-facing applications. Traditional request-response models create noticeable delays that degrade user experience, especially for complex queries. By implementing streaming responses, businesses can provide immediate feedback to users, displaying partial results as they become available, which dramatically improves perceived performance and user satisfaction, even when the complete response takes time to generate.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title impact-absent">
                                <i class="fas fa-exclamation-triangle"></i> Impact if Absent
                            </div>
                            <p>Without streaming optimization, businesses would face significant user experience issues, particularly in customer-facing applications. In a banking context, customers using a virtual assistant for loan inquiries would experience frustrating delays between asking questions and receiving answers, leading to higher abandonment rates (estimated at 40-60%), decreased customer satisfaction, and increased load on human customer service representatives as users seek faster alternatives. This would ultimately result in higher operational costs and lost business opportunities.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title predefined-custom">
                                <i class="fas fa-cogs"></i> Predefined or Custom
                            </div>
                            <p>Custom</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title relationship">
                                <i class="fas fa-link"></i> Relationship Keyword
                            </div>
                            <p>Execution</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title technical-insight">
                                <i class="fas fa-microchip"></i> In-depth Technical Insight
                            </div>
                            <p>Streaming optimization in LangChain involves processing and delivering responses in chunks as they're generated, rather than waiting for the complete response. This significantly improves perceived performance and user experience, especially for complex queries that take time to process.</p>
                            
                            <pre><code>from langchain_core.messages import AIMessageChunk
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.manager import CallbackManager
from typing import AsyncIterator, Iterator, List, Dict, Any, Optional
import asyncio
import json
from time import time
from dataclasses import dataclass
from fastapi import FastAPI, WebSocket
from fastapi.responses import HTMLResponse
import uvicorn

@dataclass
class StreamingMetrics:
    """Metrics for streaming performance"""
    chunks_received: int = 0
    total_tokens: int = 0
    time_to_first_chunk: float = 0
    time_to_last_chunk: float = 0
    average_chunk_time: float = 0

class StreamingOptimizer:
    def __init__(self, llm):
        self.llm = llm
        self.metrics = StreamingMetrics()
        
    def stream_response(self, messages: List[Dict[str, str]], callback_handler=None) -> Iterator[str]:
        """Stream a response from the language model"""
        start_time = time()
        first_chunk_received = False
        
        # Prepare callback manager
        if callback_handler:
            callback_manager = CallbackManager([callback_handler])
        else:
            callback_manager = None
            
        # Generate streaming response
        for chunk in self.llm.stream(messages, callbacks=callback_manager):
            if not first_chunk_received:
                self.metrics.time_to_first_chunk = time() - start_time
                first_chunk_received = True
                
            self.metrics.chunks_received += 1
            if hasattr(chunk, 'content'):
                self.metrics.total_tokens += len(chunk.content.split())  # Approximate token count
                yield chunk.content
                
        self.metrics.time_to_last_chunk = time() - start_time
        if self.metrics.chunks_received > 0:
            self.metrics.average_chunk_time = (self.metrics.time_to_last_chunk - self.metrics.time_to_first_chunk) / (self.metrics.chunks_received - 1)
    
    async def astream_response(self, messages: List[Dict[str, str]], callback_handler=None) -> AsyncIterator[str]:
        """Asynchronously stream a response from the language model"""
        start_time = time()
        first_chunk_received = False
        
        # Prepare callback manager
        if callback_handler:
            callback_manager = CallbackManager([callback_handler])
        else:
            callback_manager = None
            
        # Generate streaming response
        async for chunk in self.llm.astream(messages, callbacks=callback_manager):
            if not first_chunk_received:
                self.metrics.time_to_first_chunk = time() - start_time
                first_chunk_received = True
                
            self.metrics.chunks_received += 1
            if hasattr(chunk, 'content'):
                self.metrics.total_tokens += len(chunk.content.split())  # Approximate token count
                yield chunk.content
                
        self.metrics.time_to_last_chunk = time() - start_time
        if self.metrics.chunks_received > 0:
            self.metrics.average_chunk_time = (self.metrics.time_to_last_chunk - self.metrics.time_to_first_chunk) / (self.metrics.chunks_received - 1)
    
    def reset_metrics(self):
        """Reset the streaming metrics"""
        self.metrics = StreamingMetrics()
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get the current streaming metrics"""
        return {
            "chunks_received": self.metrics.chunks_received,
            "total_tokens": self.metrics.total_tokens,
            "time_to_first_chunk_ms": round(self.metrics.time_to_first_chunk * 1000, 2),
            "time_to_last_chunk_ms": round(self.metrics.time_to_last_chunk * 1000, 2),
            "average_chunk_time_ms": round(self.metrics.average_chunk_time * 1000, 2),
            "tokens_per_second": round(self.metrics.total_tokens / self.metrics.time_to_last_chunk, 2) if self.metrics.time_to_last_chunk > 0 else 0
        }

class CustomStreamingCallbackHandler(StreamingStdOutCallbackHandler):
    """Custom callback handler for streaming responses"""
    def __init__(self, on_chunk=None, on_llm_start=None, on_llm_end=None):
        super().__init__()
        self.on_chunk = on_chunk
        self.on_llm_start = on_llm_start
        self.on_llm_end = on_llm_end
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Called when a new token is generated"""
        super().on_llm_new_token(token, **kwargs)
        if self.on_chunk:
            self.on_chunk(token)
    
    def on_llm_start(self, serialized, prompts, **kwargs) -> None:
        """Called when LLM starts processing"""
        super().on_llm_start(serialized, prompts, **kwargs)
        if self.on_llm_start:
            self.on_llm_start(prompts)
    
    def on_llm_end(self, response, **kwargs) -> None:
        """Called when LLM finishes processing"""
        super().on_llm_end(response, **kwargs)
        if self.on_llm_end:
            self.on_llm_end(response)

# Example usage in a retail context
def streaming_example():
    """Example of using streaming in a retail application"""
    # Create LLM
    llm = ChatOpenAI(model="gpt-4", temperature=0.7, streaming=True)
    streaming_optimizer = StreamingOptimizer(llm)

    # Define a system message for a retail shopping assistant
    system_message = """
    You are an expert retail shopping assistant. You help customers find products, 
    provide recommendations, answer questions about features and specifications, 
    and assist with purchase decisions. Always be helpful, informative, and polite.
    """

    # Example customer query
    customer_query = "I'm looking for a birthday gift for my wife who loves cooking. She already has a good set of knives and a stand mixer. What would you recommend around $100?"

    # Create messages
    messages = [
        SystemMessage(content=system_message),
        HumanMessage(content=customer_query)
    ]

    # Define a custom callback to process chunks
    def process_chunk(chunk):
        """Process each chunk of the streaming response"""
        print(chunk, end='', flush=True)  # Print without newline

    # Create callback handler
    callback_handler = CustomStreamingCallbackHandler(on_chunk=process_chunk)

    print("Streaming response:")
    streaming_optimizer.reset_metrics()
    response_chunks = list(streaming_optimizer.stream_response(messages, callback_handler))
    print("\n")

    # Get metrics
    metrics = streaming_optimizer.get_metrics()
    print(f"Streaming metrics: {json.dumps(metrics, indent=2)}")

    # Example of async streaming
    async def async_streaming_example():
        """Example of asynchronous streaming"""
        print("\nAsync streaming example:")
        
        # Define an async chunk processor
        async def process_chunk_async(chunk):
            print(chunk, end='', flush=True)
            # Simulate some async processing
            await asyncio.sleep(0.01)
        
        streaming_optimizer.reset_metrics()
        async for chunk in streaming_optimizer.astream_response(messages):
            await process_chunk_async(chunk)
        
        print("\n")
        metrics = streaming_optimizer.get_metrics()
        print(f"Async streaming metrics: {json.dumps(metrics, indent=2)}")

    # Run the async example
    asyncio.run(async_streaming_example())

# Run the streaming example
streaming_example()</code></pre>
                            
                            <p>In a production retail environment, this streaming optimization implementation would be extended with:</p>
                            <ul>
                                <li>Integration with real-time inventory systems to ensure product availability</li>
                                <li>Personalization based on customer history and preferences</li>
                                <li>A/B testing of different streaming strategies to optimize conversion rates</li>
                                <li>Advanced buffering strategies for different network conditions</li>
                                <li>Integration with analytics platforms to measure the impact on user engagement and sales</li>
                            </ul>
                        </div>
                    </div>
                    
                    <!-- Component 10.1.4 -->
                    <div class="component-card">
                        <h3 class="component-title">10.1.4 Resource Management</h3>
                        
                        <div class="info-section">
                            <div class="info-title business-case">
                                <i class="fas fa-check-circle"></i> Business Use Case
                            </div>
                            <p>In the logistics industry, effective resource management enables companies to optimize their AI-powered supply chain operations during peak demand periods. A global shipping company implemented intelligent resource management for their route optimization and demand forecasting systems, allowing them to dynamically allocate computational resources based on shipment volume and priority. This implementation reduced infrastructure costs by 35% while maintaining 99.5% service level agreements, even during peak seasons like holidays, by automatically scaling resources up or down based on real-time demand.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title problems-solved">
                                <i class="fas fa-lightbulb"></i> Problems Solved
                            </div>
                            <p>Resource management addresses the critical challenge of balancing computational costs with performance requirements in production AI systems. Without proper resource management, organizations either over-provision resources (leading to unnecessary costs) or under-provision (leading to poor performance and service disruptions). Effective resource management ensures optimal utilization of computational resources, maintains service quality during demand fluctuations, and minimizes operational costs while meeting performance targets.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title impact-absent">
                                <i class="fas fa-exclamation-triangle"></i> Impact if Absent
                            </div>
                            <p>Without resource management, businesses would face significant operational inefficiencies and cost overruns. In a healthcare context, this would mean hospitals either paying for idle computational resources during quiet periods or experiencing critical delays in patient data analysis during peak times. This could lead to 40-60% higher infrastructure costs, potential system failures during critical situations, inability to scale services effectively during health crises, and compromised patient care due to delayed insights and decision support.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title predefined-custom">
                                <i class="fas fa-cogs"></i> Predefined or Custom
                            </div>
                            <p>Custom</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title relationship">
                                <i class="fas fa-link"></i> Relationship Keyword
                            </div>
                            <p>Control</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title technical-insight">
                                <i class="fas fa-microchip"></i> In-depth Technical Insight
                            </div>
                            <p>Resource management in LangChain applications involves monitoring and controlling computational resources to optimize performance and cost. This includes managing API rate limits, memory usage, CPU/GPU utilization, and scaling infrastructure based on demand.</p>
                            
                            <pre><code>import psutil
import time
import threading
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass, field
from enum import Enum
import asyncio
from queue import Queue, Empty
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import numpy as np
from datetime import datetime, timedelta

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ResourceType(Enum):
    """Types of resources that can be managed"""
    CPU = "cpu"
    MEMORY = "memory"
    GPU = "gpu"
    API_TOKENS = "api_tokens"
    CUSTOM = "custom"

@dataclass
class ResourceLimit:
    """Defines limits for a specific resource"""
    resource_type: ResourceType
    max_value: float
    current_usage: float = 0.0
    warning_threshold: float = 0.8  # 80% of max_value
    critical_threshold: float = 0.95  # 95% of max_value
    
    def usage_percentage(self) -> float:
        """Calculate current usage as a percentage of the maximum"""
        return min(100.0, (self.current_usage / self.max_value) * 100.0)
    
    def is_warning(self) -> bool:
        """Check if usage is at warning level"""
        return self.usage_percentage() >= (self.warning_threshold * 100.0)
    
    def is_critical(self) -> bool:
        """Check if usage is at critical level"""
        return self.usage_percentage() >= (self.critical_threshold * 100.0)

@dataclass
class ResourceUsageSnapshot:
    """Snapshot of resource usage at a point in time"""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_used: float
    memory_total: float
    api_calls: int = 0
    custom_metrics: Dict[str, float] = field(default_factory=dict)

class ResourceManager:
    """Manages computational resources for LangChain applications"""
    
    def __init__(self, 
                 max_cpu_percent: float = 80.0,
                 max_memory_percent: float = 80.0,
                 max_api_calls_per_minute: int = 60,
                 max_concurrent_requests: int = 10,
                 monitoring_interval: float = 1.0):
        """
        Initialize the resource manager
        
        Args:
            max_cpu_percent: Maximum allowed CPU usage percentage
            max_memory_percent: Maximum allowed memory usage percentage
            max_api_calls_per_minute: Maximum API calls per minute
            max_concurrent_requests: Maximum concurrent requests to process
            monitoring_interval: Interval in seconds for resource monitoring
        """
        # Define resource limits
        self.resource_limits = {
            ResourceType.CPU: ResourceLimit(ResourceType.CPU, max_cpu_percent),
            ResourceType.MEMORY: ResourceLimit(ResourceType.MEMORY, max_memory_percent),
            ResourceType.API_TOKENS: ResourceLimit(ResourceType.API_TOKENS, max_api_calls_per_minute)
        }
        
        # Request management
        self.max_concurrent_requests = max_concurrent_requests
        self.active_requests = 0
        self.request_queue = Queue()
        self.request_lock = threading.Lock()
        
        # API call tracking
        self.api_calls = []
        self.api_call_lock = threading.Lock()
        
        # Monitoring
        self.monitoring_interval = monitoring_interval
        self.monitoring_active = False
        self.usage_history = []
        self.max_history_size = 1000
        
        # Callbacks
        self.warning_callbacks: Dict[ResourceType, List[Callable]] = {
            resource_type: [] for resource_type in ResourceType
        }
        self.critical_callbacks: Dict[ResourceType, List[Callable]] = {
            resource_type: [] for resource_type in ResourceType
        }
        
        # Custom metrics
        self.custom_metrics: Dict[str, float] = {}
        self.custom_metrics_lock = threading.Lock()
        
    def start_monitoring(self):
        """Start the resource monitoring thread"""
        if not self.monitoring_active:
            self.monitoring_active = True
            self.monitor_thread = threading.Thread(target=self._monitor_resources)
            self.monitor_thread.daemon = True
            self.monitor_thread.start()
            print("Resource monitoring started")
    
    def stop_monitoring(self):
        """Stop the resource monitoring thread"""
        self.monitoring_active = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join()
        print("Resource monitoring stopped")
    
    def _monitor_resources(self):
        """Monitor resource usage in a separate thread"""
        while self.monitoring_active:
            # Get current resource usage
            cpu_percent = psutil.cpu_percent(interval=None)
            memory_info = psutil.virtual_memory()
            
            # Update resource limits
            self.resource_limits[ResourceType.CPU].current_usage = cpu_percent
            self.resource_limits[ResourceType.MEMORY].current_usage = memory_info.percent
            
            # Check API calls in the last minute
            now = datetime.now()
            one_minute_ago = now - timedelta(minutes=1)
            
            with self.api_call_lock:
                recent_api_calls = [call_time for call_time in self.api_calls if call_time > one_minute_ago]
                self.api_calls = recent_api_calls
                self.resource_limits[ResourceType.API_TOKENS].current_usage = len(recent_api_calls)
            
            # Take a snapshot
            snapshot = ResourceUsageSnapshot(
                timestamp=now,
                cpu_percent=cpu_percent,
                memory_percent=memory_info.percent,
                memory_used=memory_info.used,
                memory_total=memory_info.total,
                api_calls=len(recent_api_calls),
                custom_metrics=self.custom_metrics.copy()
            )
            
            # Add to history
            self.usage_history.append(snapshot)
            if len(self.usage_history) > self.max_history_size:
                self.usage_history.pop(0)
            
            # Check thresholds and trigger callbacks
            self._check_thresholds()
            
            # Sleep until next monitoring interval
            time.sleep(self.monitoring_interval)
    
    def _check_thresholds(self):
        """Check if any resource thresholds are exceeded and trigger callbacks"""
        for resource_type, limit in self.resource_limits.items():
            if limit.is_critical():
                for callback in self.critical_callbacks[resource_type]:
                    try:
                        callback(resource_type, limit)
                    except Exception as e:
                        print(f"Error in critical callback for {resource_type}: {e}")
            elif limit.is_warning():
                for callback in self.warning_callbacks[resource_type]:
                    try:
                        callback(resource_type, limit)
                    except Exception as e:
                        print(f"Error in warning callback for {resource_type}: {e}")
    
    def register_warning_callback(self, resource_type: ResourceType, callback: Callable):
        """Register a callback to be called when a resource reaches warning level"""
        self.warning_callbacks[resource_type].append(callback)
    
    def register_critical_callback(self, resource_type: ResourceType, callback: Callable):
        """Register a callback to be called when a resource reaches critical level"""
        self.critical_callbacks[resource_type].append(callback)
    
    def track_api_call(self):
        """Track an API call for rate limiting"""
        with self.api_call_lock:
            self.api_calls.append(datetime.now())
    
    def can_make_api_call(self) -> bool:
        """Check if an API call can be made based on rate limits"""
        with self.api_call_lock:
            now = datetime.now()
            one_minute_ago = now - timedelta(minutes=1)
            recent_api_calls = [call_time for call_time in self.api_calls if call_time > one_minute_ago]
            return len(recent_api_calls) < self.resource_limits[ResourceType.API_TOKENS].max_value
    
    def wait_for_api_slot(self, timeout: float = 60.0) -> bool:
        """Wait until an API slot is available"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            if self.can_make_api_call():
                return True
            time.sleep(0.1)
        return False
    
    def execute_with_resource_management(self, func: Callable, *args, **kwargs) -> Any:
        """Execute a function with resource management"""
        # Add to request queue
        self.request_queue.put((func, args, kwargs))
        
        # Wait for execution
        return self._process_next_request()
    
    def _process_next_request(self) -> Any:
        """Process the next request in the queue"""
        with self.request_lock:
            if self.active_requests >= self.max_concurrent_requests:
                # Wait for a request to complete
                while self.active_requests >= self.max_concurrent_requests:
                    time.sleep(0.1)
            
            # Get the next request
            try:
                func, args, kwargs = self.request_queue.get_nowait()
            except Empty:
                return None
            
            # Increment active requests
            self.active_requests += 1
        
        try:
            # Execute the function
            result = func(*args, **kwargs)
            return result
        finally:
            # Decrement active requests
            with self.request_lock:
                self.active_requests -= 1
    
    def get_current_usage(self) -> Dict[str, float]:
        """Get current resource usage"""
        return {
            "cpu_percent": self.resource_limits[ResourceType.CPU].current_usage,
            "memory_percent": self.resource_limits[ResourceType.MEMORY].current_usage,
            "api_calls_per_minute": self.resource_limits[ResourceType.API_TOKENS].current_usage,
            "active_requests": self.active_requests,
            "queued_requests": self.request_queue.qsize()
        }
    
    def get_usage_history(self, minutes: int = 10) -> List[Dict[str, Any]]:
        """Get resource usage history for the specified number of minutes"""
        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        history = []
        
        for snapshot in self.usage_history:
            if snapshot.timestamp >= cutoff_time:
                history.append({
                    "timestamp": snapshot.timestamp.isoformat(),
                    "cpu_percent": snapshot.cpu_percent,
                    "memory_percent": snapshot.memory_percent,
                    "memory_used": snapshot.memory_used,
                    "memory_total": snapshot.memory_total,
                    "api_calls": snapshot.api_calls,
                    "custom_metrics": snapshot.custom_metrics
                })
        
        return history
    
    def update_custom_metric(self, name: str, value: float):
        """Update a custom metric"""
        with self.custom_metrics_lock:
            self.custom_metrics[name] = value
    
    def get_custom_metrics(self) -> Dict[str, float]:
        """Get all custom metrics"""
        with self.custom_metrics_lock:
            return self.custom_metrics.copy()

class ResourceAwareLLM:
    """Wrapper for LLM that respects resource limits"""
    
    def __init__(self, llm, resource_manager: ResourceManager):
        self.llm = llm
        self.resource_manager = resource_manager
    
    def invoke(self, messages, **kwargs):
        """Invoke the LLM with resource management"""
        # Wait for API slot if needed
        if not self.resource_manager.can_make_api_call():
            print("Waiting for API slot...")
            if not self.resource_manager.wait_for_api_slot():
                raise Exception("Timeout waiting for API slot")
        
        # Track the API call
        self.resource_manager.track_api_call()
        
        # Execute with resource management
        return self.resource_manager.execute_with_resource_management(
            self.llm.invoke, messages, **kwargs
        )
    
    def stream(self, messages, **kwargs):
        """Stream from the LLM with resource management"""
        # Wait for API slot if needed
        if not self.resource_manager.can_make_api_call():
            print("Waiting for API slot...")
            if not self.resource_manager.wait_for_api_slot():
                raise Exception("Timeout waiting for API slot")
        
        # Track the API call
        self.resource_manager.track_api_call()
        
        # Execute with resource management
        return self.resource_manager.execute_with_resource_management(
            self.llm.stream, messages, **kwargs
        )

# Example usage in a logistics context
def resource_management_example():
    """Example of using resource management in a logistics application"""
    # Create resource manager
    resource_manager = ResourceManager(
        max_cpu_percent=75.0,
        max_memory_percent=75.0,
        max_api_calls_per_minute=30,
        max_concurrent_requests=5,
        monitoring_interval=0.5
    )

    # Start monitoring
    resource_manager.start_monitoring()

    # Create a resource-aware LLM
    base_llm = ChatOpenAI(model="gpt-4", temperature=0.1)
    llm = ResourceAwareLLM(base_llm, resource_manager)

    # Define a system message for logistics optimization
    logistics_system_message = """
    You are a logistics optimization expert. Analyze the provided shipping information and 
    provide recommendations for route optimization, cost reduction, and delivery efficiency.
    Consider factors like distance, traffic patterns, delivery windows, and vehicle capacity.
    """

    # Example logistics queries
    logistics_queries = [
        "I need to optimize delivery routes for 50 packages across downtown Chicago during rush hour. What's the most efficient approach?",
        "We have 10 trucks with different capacity constraints. How should we allocate 200 shipments to minimize total distance?",
        "Our delivery costs have increased by 15% this quarter. What strategies can we implement to reduce fuel consumption?",
        "We need to deliver perishable goods with strict time windows. How can we optimize routes while meeting these constraints?",
        "What's the most cost-effective way to handle returns processing in our reverse logistics chain?"
    ]

    # Process queries with resource management
    print("Processing logistics queries with resource management...")
    results = []

    for query in logistics_queries:
        print(f"\nProcessing: {query[:50]}...")

        try:
            # Create messages
            messages = [
                SystemMessage(content=logistics_system_message),
                HumanMessage(content=query)
            ]

            # Get response with resource management
            response = llm.invoke(messages)
            results.append(response.content)
            print(f"Response received ({len(response.content)} characters)")
        except Exception as e:
            print(f"Error: {str(e)}")

    # Display current resource usage
    print("\nCurrent resource usage:")
    usage = resource_manager.get_current_usage()
    for key, value in usage.items():
        print(f"  {key}: {value}")

    # Get usage history
    print("\nResource usage history (last 2 minutes):")
    history = resource_manager.get_usage_history(minutes=2)
    if history:
        # Calculate averages
        avg_cpu = sum(h["cpu_percent"] for h in history) / len(history)
        avg_memory = sum(h["memory_percent"] for h in history) / len(history)
        avg_api_calls = sum(h["api_calls"] for h in history) / len(history)

        print(f"  Average CPU: {avg_cpu:.1f}%")
        print(f"  Average Memory: {avg_memory:.1f}%")
        print(f"  Average API calls per minute: {avg_api_calls:.1f}")

        # Find peaks
        max_cpu = max(h["cpu_percent"] for h in history)
        max_memory = max(h["memory_percent"] for h in history)
        max_api_calls = max(h["api_calls"] for h in history)

        print(f"  Peak CPU: {max_cpu:.1f}%")
        print(f"  Peak Memory: {max_memory:.1f}%")
        print(f"  Peak API calls per minute: {max_api_calls}")

    # Example of custom metrics
    resource_manager.update_custom_metric("active_shipments", 1250)
    resource_manager.update_custom_metric("delivery_success_rate", 98.7)
    resource_manager.update_custom_metric("avg_delivery_time", 2.4)

    print("\nCustom metrics:")
    custom_metrics = resource_manager.get_custom_metrics()
    for key, value in custom_metrics.items():
        print(f"  {key}: {value}")

    # Stop monitoring
    resource_manager.stop_monitoring()

# Run the resource management example
resource_management_example()</code></pre>
                            
                            <p>In a production logistics environment, this resource management implementation would be extended with:</p>
                            <ul>
                                <li>Integration with cloud auto-scaling services to dynamically adjust infrastructure based on demand</li>
                                <li>Advanced predictive scaling based on historical patterns and upcoming events</li>
                                <li>Priority-based resource allocation for urgent shipments and high-value customers</li>
                                <li>Integration with logistics management systems to correlate resource usage with business metrics</li>
                                <li>Advanced cost allocation and chargeback mechanisms for different business units</li>
                            </ul>
                        </div>
                    </div>
                </section>
                
                <!-- Section 10.2 -->
                <section id="section-102" class="section-card">
                    <h2>10.2 Monitoring & Observability</h2>
                    
                    <!-- Component 10.2.1 -->
                    <div class="component-card">
                        <h3 class="component-title">10.2.1 Callbacks</h3>
                        
                        <div class="info-section">
                            <div class="info-title business-case">
                                <i class="fas fa-check-circle"></i> Business Use Case
                            </div>
                            <p>In the banking industry, callbacks enable real-time monitoring and intervention in AI-powered customer service systems. A leading financial institution implemented callback handlers to track customer interactions with their virtual banking assistant, allowing them to identify when customers were becoming frustrated or when sensitive financial topics were being discussed. This system automatically escalated complex conversations to human agents when confidence scores dropped below 85%, resulting in a 40% reduction in customer resolution time and a 25% increase in customer satisfaction scores for complex financial inquiries.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title problems-solved">
                                <i class="fas fa-lightbulb"></i> Problems Solved
                            </div>
                            <p>Callbacks solve the critical challenge of observability and intervention in AI systems. Without callbacks, businesses operate their AI applications as "black boxes" with limited visibility into internal processes, making it difficult to monitor performance, debug issues, or implement business logic based on intermediate states. Callbacks provide hooks into the execution pipeline, enabling real-time monitoring, custom logging, performance measurement, and dynamic intervention based on specific conditions.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title impact-absent">
                                <i class="fas fa-exclamation-triangle"></i> Impact if Absent
                            </div>
                            <p>Without callbacks, businesses would lack essential visibility into their AI systems' operations, leading to significant operational challenges. In a healthcare context, this would mean inability to monitor AI-powered diagnostic systems in real-time, potentially allowing errors to go undetected until they impact patient care. This could result in undetected model drift, delayed responses to critical issues, inability to audit AI decisions for compliance, and missed opportunities to improve system performance based on real-world usage patterns.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title predefined-custom">
                                <i class="fas fa-cogs"></i> Predefined or Custom
                            </div>
                            <p>Custom</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title relationship">
                                <i class="fas fa-link"></i> Relationship Keyword
                            </div>
                            <p>Control</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title technical-insight">
                                <i class="fas fa-microchip"></i> In-depth Technical Insight
                            </div>
                            <p>Callbacks in LangChain are functions that are called at specific points during the execution of a chain or agent. They provide hooks into the execution pipeline, enabling monitoring, logging, and custom behavior. There are several types of callbacks, including runtime callbacks, module-level callbacks, constructor callbacks, custom handlers, and async callbacks.</p>
                            
                            <pre><code>from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.tracers import LangChainTracer
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from typing import Any, Dict, List, Optional, Union
import json
import time
import asyncio
from datetime import datetime
from dataclasses import dataclass, field
import uuid
import logging
from enum import Enum

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CallbackEvent(Enum):
    """Types of callback events"""
    ON_LLM_START = "on_llm_start"
    ON_LLM_END = "on_llm_end"
    ON_LLM_ERROR = "on_llm_error"
    ON_LLM_NEW_TOKEN = "on_llm_new_token"
    ON_CHAIN_START = "on_chain_start"
    ON_CHAIN_END = "on_chain_end"
    ON_CHAIN_ERROR = "on_chain_error"
    ON_TOOL_START = "on_tool_start"
    ON_TOOL_END = "on_tool_end"
    ON_TOOL_ERROR = "on_tool_error"
    ON_AGENT_ACTION = "on_agent_action"
    ON_AGENT_FINISH = "on_agent_finish"
    ON_TEXT = "on_text"
    ON_RETRIEVER_START = "on_retriever_start"
    ON_RETRIEVER_END = "on_retriever_end"
    ON_RETRIEVER_ERROR = "on_retriever_error"

@dataclass
class CallbackData:
    """Data structure for callback information"""
    event_type: CallbackEvent
    timestamp: datetime
    run_id: str
    parent_run_id: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    data: Dict[str, Any] = field(default_factory=dict)

class BankingCallbackHandler(BaseCallbackHandler):
    """Custom callback handler for banking applications"""
    
    def __init__(self, log_to_file: bool = False, escalate_threshold: float = 0.7):
        self.log_to_file = log_to_file
        self.escalate_threshold = escalate_threshold
        self.conversation_history = []
        self.current_run_id = None
        self.current_parent_run_id = None
        self.sensitive_keywords = [
            "fraud", "suspicious", "unauthorized", "theft", "stolen", 
            "complaint", "dispute", "escalate", "manager", "supervisor"
        ]
        
        if log_to_file:
            self.log_file = f"banking_callbacks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
            self.file_handler = logging.FileHandler(self.log_file)
            self.file_handler.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            self.file_handler.setFormatter(formatter)
            logger.addHandler(self.file_handler)
    
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        """Called when LLM starts processing"""
        self.current_run_id = str(uuid.uuid4())
        self.current_parent_run_id = kwargs.get("parent_run_id")
        
        logger.info(f"LLM started - Run ID: {self.current_run_id}")
        
        # Log the prompts
        for i, prompt in enumerate(prompts):
            logger.info(f"Prompt {i+1}: {prompt[:100]}...")
            
            # Check for sensitive content
            self._check_sensitive_content(prompt)
    
    def on_llm_end(self, response: Any, **kwargs: Any) -> Any:
        """Called when LLM finishes processing"""
        logger.info(f"LLM finished - Run ID: {self.current_run_id}")
        
        # Extract response content
        if hasattr(response, 'generations'):
            for generation_list in response.generations:
                for generation in generation_list:
                    if hasattr(generation, 'text'):
                        logger.info(f"Response: {generation.text[:100]}...")
                        
                        # Check for sensitive content in response
                        self._check_sensitive_content(generation.text)
    
    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Called when LLM encounters an error"""
        logger.error(f"LLM error - Run ID: {self.current_run_id}, Error: {str(error)}")
    
    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        """Called when a new token is generated"""
        # This is called for each token in streaming responses
        # We can use this for real-time monitoring
        pass
    
    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> Any:
        """Called when a chain starts processing"""
        chain_id = str(uuid.uuid4())
        logger.info(f"Chain started - Chain ID: {chain_id}, Type: {serialized.get('id', ['Unknown'])[-1]}")
        
        # Log inputs
        for key, value in inputs.items():
            if isinstance(value, str):
                logger.info(f"Input {key}: {value[:100]}...")
                self._check_sensitive_content(value)
    
    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:
        """Called when a chain finishes processing"""
        logger.info("Chain finished")
        
        # Log outputs
        for key, value in outputs.items():
            if isinstance(value, str):
                logger.info(f"Output {key}: {value[:100]}...")
                self._check_sensitive_content(value)
    
    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Called when a chain encounters an error"""
        logger.error(f"Chain error: {str(error)}")
    
    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> Any:
        """Called when a tool starts processing"""
        tool_name = serialized.get('name', 'Unknown')
        logger.info(f"Tool started - Name: {tool_name}, Input: {input_str[:100]}...")
        
        # Check for sensitive content
        self._check_sensitive_content(input_str)
    
    def on_tool_end(self, output: str, **kwargs: Any) -> Any:
        """Called when a tool finishes processing"""
        logger.info(f"Tool finished - Output: {output[:100]}...")
        
        # Check for sensitive content
        self._check_sensitive_content(output)
    
    def on_tool_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Called when a tool encounters an error"""
        logger.error(f"Tool error: {str(error)}")
    
    def on_agent_action(self, action: Any, **kwargs: Any) -> Any:
        """Called when an agent takes an action"""
        logger.info(f"Agent action - Tool: {action.tool}, Input: {action.tool_input[:100] if hasattr(action, 'tool_input') else 'N/A'}")
        
        # Check for sensitive content
        if hasattr(action, 'tool_input') and isinstance(action.tool_input, str):
            self._check_sensitive_content(action.tool_input)
    
    def on_agent_finish(self, finish: Any, **kwargs: Any) -> Any:
        """Called when an agent finishes"""
        logger.info(f"Agent finished - Output: {finish.return_values['output'][:100] if hasattr(finish, 'return_values') else 'N/A'}...")
        
        # Check for sensitive content
        if hasattr(finish, 'return_values') and 'output' in finish.return_values:
            self._check_sensitive_content(finish.return_values['output'])
    
    def on_text(self, text: str, **kwargs: Any) -> Any:
        """Called when text is generated"""
        logger.info(f"Text generated: {text[:100]}...")
        
        # Check for sensitive content
        self._check_sensitive_content(text)
    
    def _check_sensitive_content(self, text: str) -> bool:
        """Check if text contains sensitive content that requires escalation"""
        text_lower = text.lower()
        sensitive_found = False
        
        for keyword in self.sensitive_keywords:
            if keyword in text_lower:
                logger.warning(f"Sensitive keyword detected: {keyword}")
                sensitive_found = True
                
                # In a real banking system, this would trigger escalation
                self._escalate_conversation(keyword, text)
                break
        
        return sensitive_found
    
    def _escalate_conversation(self, keyword: str, context: str) -> None:
        """Escalate conversation to human agent"""
        escalation_data = {
            "timestamp": datetime.now().isoformat(),
            "keyword": keyword,
            "context": context,
            "run_id": self.current_run_id,
            "parent_run_id": self.current_parent_run_id
        }
        
        logger.warning(f"ESCALATION TRIGGERED: {json.dumps(escalation_data)}")
        
        # In a real banking system, this would integrate with the escalation system
        # For example, sending a notification to a human agent or creating a ticket
        
        # Store in conversation history for audit purposes
        self.conversation_history.append({
            "type": "escalation",
            "data": escalation_data
        })
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """Get the conversation history"""
        return self.conversation_history
    
    def clear_conversation_history(self) -> None:
        """Clear the conversation history"""
        self.conversation_history = []

class PerformanceCallbackHandler(BaseCallbackHandler):
    """Callback handler for performance monitoring"""
    
    def __init__(self):
        self.metrics = {
            "llm_calls": 0,
            "total_llm_time": 0,
            "chain_calls": 0,
            "total_chain_time": 0,
            "tool_calls": 0,
            "total_tool_time": 0,
            "token_count": 0
        }
        self.timings = {}
        self.current_run_id = None
    
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        """Called when LLM starts processing"""
        self.current_run_id = str(uuid.uuid4())
        self.timings[self.current_run_id] = time.time()
        self.metrics["llm_calls"] += 1
    
    def on_llm_end(self, response: Any, **kwargs: Any) -> Any:
        """Called when LLM finishes processing"""
        if self.current_run_id in self.timings:
            elapsed = time.time() - self.timings[self.current_run_id]
            self.metrics["total_llm_time"] += elapsed
            
            # Count tokens if available
            if hasattr(response, 'llm_output') and response.llm_output:
                if 'token_usage' in response.llm_output:
                    token_usage = response.llm_output['token_usage']
                    self.metrics["token_count"] += token_usage.get('total_tokens', 0)
    
    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> Any:
        """Called when a chain starts processing"""
        chain_id = str(uuid.uuid4())
        self.timings[chain_id] = time.time()
        self.metrics["chain_calls"] += 1
    
    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:
        """Called when a chain finishes processing"""
        # Find the matching chain start timing
        for run_id, start_time in self.timings.items():
            if run_id in str(kwargs.get('run_id', '')):
                elapsed = time.time() - start_time
                self.metrics["total_chain_time"] += elapsed
                del self.timings[run_id]
                break
    
    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> Any:
        """Called when a tool starts processing"""
        tool_id = str(uuid.uuid4())
        self.timings[tool_id] = time.time()
        self.metrics["tool_calls"] += 1
    
    def on_tool_end(self, output: str, **kwargs: Any) -> Any:
        """Called when a tool finishes processing"""
        # Find the matching tool start timing
        for run_id, start_time in self.timings.items():
            if run_id in str(kwargs.get('run_id', '')):
                elapsed = time.time() - start_time
                self.metrics["total_tool_time"] += elapsed
                del self.timings[run_id]
                break
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get performance metrics"""
        metrics = self.metrics.copy()
        
        # Calculate averages
        if metrics["llm_calls"] > 0:
            metrics["avg_llm_time"] = metrics["total_llm_time"] / metrics["llm_calls"]
        else:
            metrics["avg_llm_time"] = 0
            
        if metrics["chain_calls"] > 0:
            metrics["avg_chain_time"] = metrics["total_chain_time"] / metrics["chain_calls"]
        else:
            metrics["avg_chain_time"] = 0
            
        if metrics["tool_calls"] > 0:
            metrics["avg_tool_time"] = metrics["total_tool_time"] / metrics["tool_calls"]
        else:
            metrics["avg_tool_time"] = 0
        
        return metrics
    
    def reset_metrics(self) -> None:
        """Reset performance metrics"""
        self.metrics = {
            "llm_calls": 0,
            "total_llm_time": 0,
            "chain_calls": 0,
            "total_chain_time": 0,
            "tool_calls": 0,
            "total_tool_time": 0,
            "token_count": 0
        }
        self.timings = {}

class AsyncCallbackHandler(BaseCallbackHandler):
    """Asynchronous callback handler for real-time monitoring"""
    
    def __init__(self):
        self.event_queue = asyncio.Queue()
        self.processing = False
    
    async def on_llm_start_async(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        """Async version of on_llm_start"""
        await self.event_queue.put({
            "event": "llm_start",
            "data": {
                "serialized": serialized,
                "prompts": prompts,
                "kwargs": kwargs
            }
        })
        
        # Start processing if not already running
        if not self.processing:
            self.processing = True
            asyncio.create_task(self._process_events())
    
    async def on_llm_end_async(self, response: Any, **kwargs: Any) -> Any:
        """Async version of on_llm_end"""
        await self.event_queue.put({
            "event": "llm_end",
            "data": {
                "response": response,
                "kwargs": kwargs
            }
        })
    
    async def _process_events(self):
        """Process events from the queue"""
        while True:
            try:
                # Get event with timeout
                event = await asyncio.wait_for(self.event_queue.get(), timeout=1.0)
                
                # Process the event
                event_type = event["event"]
                data = event["data"]
                
                if event_type == "llm_start":
                    print(f"[ASYNC] LLM started with {len(data['prompts'])} prompts")
                elif event_type == "llm_end":
                    print(f"[ASYNC] LLM finished")
                
                # In a real system, this would send events to a monitoring system
                # or trigger other async actions
                
            except asyncio.TimeoutError:
                # No events in queue, check if we should stop processing
                if self.event_queue.empty():
                    self.processing = False
                    break

# Example usage in a banking context
def callback_example():
    """Example of using callbacks in a banking application"""
    # Create LLM
    llm = ChatOpenAI(model="gpt-4", temperature=0.1)
    
    # Create callback handlers
    banking_handler = BankingCallbackHandler(log_to_file=True)
    performance_handler = PerformanceCallbackHandler()
    
    # Create callback manager with multiple handlers
    callback_manager = CallbackManager([banking_handler, performance_handler])
    
    # Create a chain with callbacks
    prompt_template = """
    You are a helpful banking assistant. Answer the customer's question accurately and professionally.
    If you detect any sensitive topics like fraud or disputes, be cautious and consider escalating.
    
    Customer Question: {question}
    
    Your Response:
    """
    
    prompt = PromptTemplate(
        input_variables=["question"],
        template=prompt_template
    )
    
    chain = LLMChain(
        llm=llm,
        prompt=prompt,
        callbacks=callback_manager
    )
    
    # Example questions - some with sensitive content
    questions = [
        "What's the current interest rate for savings accounts?",
        "I think there's a fraudulent charge on my credit card, what should I do?",
        "How do I set up automatic bill payments?",
        "I need to dispute a transaction on my account",
        "What are the fees for international wire transfers?"
    ]
    
    # Process each question
    for question in questions:
        print(f"\nProcessing: {question}")
        try:
            response = chain.run(question=question)
            print(f"Response: {response[:100]}...")
        except Exception as e:
            print(f"Error: {str(e)}")
    
    # Get performance metrics
    metrics = performance_handler.get_metrics()
    print("\nPerformance Metrics:")
    for key, value in metrics.items():
        print(f"  {key}: {value}")
    
    # Get conversation history
    history = banking_handler.get_conversation_history()
    print(f"\nConversation History ({len(history)} events):")
    for event in history:
        if event["type"] == "escalation":
            print(f"  Escalation: {event['data']['keyword']} - {event['data']['timestamp']}")

# Run the callback example
callback_example()</code></pre>
                            
                            <p>In a production banking environment, this callback implementation would be extended with:</p>
                            <ul>
                                <li>Integration with the bank's security monitoring systems to detect potential fraud in real-time</li>
                                <li>Compliance logging to meet financial regulatory requirements</li>
                                <li>Integration with customer relationship management (CRM) systems to track customer interactions</li>
                                <li>Advanced sentiment analysis to detect customer frustration and trigger appropriate responses</li>
                                <li>Real-time dashboarding for operations teams to monitor system performance and customer interactions</li>
                            </ul>
                        </div>
                    </div>
                    
                    <!-- Component 10.2.2 -->
                    <div class="component-card">
                        <h3 class="component-title">10.2.2 Custom Callback Events</h3>
                        
                        <div class="info-section">
                            <div class="info-title business-case">
                                <i class="fas fa-check-circle"></i> Business Use Case
                            </div>
                            <p>In the healthcare industry, custom callback events enable specialized monitoring of AI-powered diagnostic systems. A hospital network implemented custom callback events to track when their AI system identified potential critical conditions in patient data, such as early signs of sepsis or cardiac events. These custom events triggered immediate notifications to medical staff, integrated with the hospital's alert system, and logged detailed information for compliance and quality improvement. This implementation reduced response time to critical conditions by 70% and improved early detection rates by 35%, directly impacting patient outcomes.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title problems-solved">
                                <i class="fas fa-lightbulb"></i> Problems Solved
                            </div>
                            <p>Custom callback events solve the challenge of domain-specific monitoring and intervention in AI systems. While standard callbacks provide general-purpose hooks into the execution pipeline, they often lack the specificity needed for industry-specific requirements. Custom callback events allow businesses to define and respond to domain-specific conditions that are critical to their operations, such as compliance triggers, security events, or business logic conditions that require special handling.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title impact-absent">
                                <i class="fas fa-exclamation-triangle"></i> Impact if Absent
                            </div>
                            <p>Without custom callback events, businesses would be limited to generic monitoring capabilities that may not capture critical domain-specific events. In a logistics context, this would mean inability to automatically detect and respond to specialized conditions like delivery exceptions, customs clearance issues, or supply chain disruptions. This would result in manual monitoring requirements, delayed responses to critical events, increased operational costs, and diminished ability to proactively address issues before they impact customers.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title predefined-custom">
                                <i class="fas fa-cogs"></i> Predefined or Custom
                            </div>
                            <p>Custom</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title relationship">
                                <i class="fas fa-link"></i> Relationship Keyword
                            </div>
                            <p>Control</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title technical-insight">
                                <i class="fas fa-microchip"></i> In-depth Technical Insight
                            </div>
                            <p>Custom callback events extend LangChain's callback system by allowing developers to define and respond to domain-specific events that occur during chain execution. This enables specialized monitoring, intervention, and business logic that goes beyond the standard callback events provided by the framework.</p>
                            
                            <pre><code>from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from typing import Any, Dict, List, Optional, Union, Callable
import json
import time
import asyncio
from datetime import datetime
from dataclasses import dataclass, field
import uuid
import logging
from enum import Enum
import re
from abc import ABC, abstractmethod

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CustomEventType(Enum):
    """Types of custom callback events"""
    MEDICAL_CONDITION_DETECTED = "medical_condition_detected"
    TREATMENT_RECOMMENDED = "treatment_recommended"
    PATIENT_RISK_ASSESSMENT = "patient_risk_assessment"
    COMPLIANCE_CHECK = "compliance_check"
    DATA_PRIVACY_ALERT = "data_privacy_alert"
    CRITICAL_VALUE_DETECTED = "critical_value_detected"
    PRESCRIPTION_GENERATED = "prescription_generated"
    REFERRAL_NEEDED = "referral_needed"
    FOLLOW_UP_REQUIRED = "follow_up_required"
    EMERGENCY_PROTOCOL_TRIGGERED = "emergency_protocol_triggered"

@dataclass
class CustomEvent:
    """Data structure for custom callback events"""
    event_type: CustomEventType
    timestamp: datetime
    severity: str  # "info", "warning", "error", "critical"
    patient_id: Optional[str] = None
    provider_id: Optional[str] = None
    data: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    event_id: str = field(default_factory=lambda: str(uuid.uuid4()))

class CustomEventHandler(ABC):
    """Abstract base class for custom event handlers"""
    
    @abstractmethod
    def handle_event(self, event: CustomEvent) -> None:
        """Handle a custom event"""
        pass

class MedicalConditionHandler(CustomEventHandler):
    """Handler for medical condition detection events"""
    
    def __init__(self, notification_system=None):
        self.notification_system = notification_system
        self.condition_database = {
            "sepsis": {"severity": "critical", "protocol": "sepsis_protocol"},
            "myocardial_infarction": {"severity": "critical", "protocol": "cardiac_protocol"},
            "stroke": {"severity": "critical", "protocol": "stroke_protocol"},
            "pneumonia": {"severity": "warning", "protocol": "respiratory_protocol"},
            "uti": {"severity": "info", "protocol": "infection_protocol"}
        }
    
    def handle_event(self, event: CustomEvent) -> None:
        """Handle medical condition detection events"""
        condition = event.data.get("condition", "").lower()
        
        if condition in self.condition_database:
            condition_info = self.condition_database[condition]
            
            # Log the event
            logger.warning(f"Medical condition detected: {condition} (Severity: {condition_info['severity']})")
            
            # Trigger notification if available
            if self.notification_system:
                self.notification_system.send_alert(
                    recipient=event.provider_id,
                    message=f"Potential {condition} detected for patient {event.patient_id}",
                    severity=condition_info["severity"],
                    protocol=condition_info["protocol"]
                )
            
            # Update event with protocol information
            event.metadata["protocol"] = condition_info["protocol"]
            event.metadata["recommended_actions"] = self._get_recommended_actions(condition)
    
    def _get_recommended_actions(self, condition: str) -> List[str]:
        """Get recommended actions for a condition"""
        actions = {
            "sepsis": [
                "Initiate sepsis protocol",
                "Order blood cultures",
                "Administer broad-spectrum antibiotics",
                "Monitor vital signs closely"
            ],
            "myocardial_infarction": [
                "Activate cardiac catheterization team",
                "Administer aspirin and nitroglycerin",
                "Order ECG and cardiac enzymes",
                "Prepare for possible PCI"
            ],
            "stroke": [
                "Activate stroke team",
                "Assess for tPA eligibility",
                "Order emergent CT scan",
                "Monitor neurological status"
            ],
            "pneumonia": [
                "Order chest X-ray",
                "Start appropriate antibiotics",
                "Monitor oxygen saturation",
                "Consider respiratory support"
            ],
            "uti": [
                "Order urinalysis and culture",
                "Start empiric antibiotics",
                "Monitor for signs of sepsis",
                "Follow up culture results"
            ]
        }
        
        return actions.get(condition, ["Consult specialist", "Order appropriate tests"])

class ComplianceHandler(CustomEventHandler):
    """Handler for compliance-related events"""
    
    def __init__(self, compliance_officer_email=None):
        self.compliance_officer_email = compliance_officer_email
        self.compliance_log = []
    
    def handle_event(self, event: CustomEvent) -> None:
        """Handle compliance events"""
        compliance_type = event.data.get("compliance_type", "")
        
        logger.info(f"Compliance event: {compliance_type}")
        
        # Log the compliance event
        self.compliance_log.append({
            "timestamp": event.timestamp,
            "event_id": event.event_id,
            "patient_id": event.patient_id,
            "provider_id": event.provider_id,
            "compliance_type": compliance_type,
            "details": event.data
        })
        
        # For high-severity compliance issues, notify compliance officer
        if event.severity in ["error", "critical"] and self.compliance_officer_email:
            # In a real system, this would send an email
            logger.warning(f"Compliance notification sent to {self.compliance_officer_email}")
    
    def get_compliance_report(self, start_date: datetime, end_date: datetime) -> List[Dict]:
        """Generate a compliance report for a date range"""
        return [
            event for event in self.compliance_log
            if start_date <= event["timestamp"] <= end_date
        ]

class HealthcareCallbackHandler(BaseCallbackHandler):
    """Custom callback handler for healthcare applications"""
    
    def __init__(self):
        self.custom_handlers = {
            CustomEventType.MEDICAL_CONDITION_DETECTED: MedicalConditionHandler(),
            CustomEventType.TREATMENT_RECOMMENDED: None,  # Would implement specific handler
            CustomEventType.PATIENT_RISK_ASSESSMENT: None,  # Would implement specific handler
            CustomEventType.COMPLIANCE_CHECK: ComplianceHandler(),
            CustomEventType.DATA_PRIVACY_ALERT: None,  # Would implement specific handler
            CustomEventType.CRITICAL_VALUE_DETECTED: None,  # Would implement specific handler
            CustomEventType.PRESCRIPTION_GENERATED: None,  # Would implement specific handler
            CustomEventType.REFERRAL_NEEDED: None,  # Would implement specific handler
            CustomEventType.FOLLOW_UP_REQUIRED: None,  # Would implement specific handler
            CustomEventType.EMERGENCY_PROTOCOL_TRIGGERED: None  # Would implement specific handler
        }
        
        self.event_history = []
        self.current_patient_id = None
        self.current_provider_id = None
    
    def register_handler(self, event_type: CustomEventType, handler: CustomEventHandler) -> None:
        """Register a custom event handler"""
        self.custom_handlers[event_type] = handler
    
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        """Called when LLM starts processing"""
        # Extract patient and provider IDs from prompts or metadata
        self._extract_context_info(prompts, kwargs)
    
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:
        """Called when LLM finishes processing"""
        # Analyze the response for potential custom events
        self._analyze_response(response)
    
    def _extract_context_info(self, prompts: List[str], kwargs: Dict[str, Any]) -> None:
        """Extract patient and provider context from prompts"""
        # In a real system, this would parse structured data or use metadata
        # For this example, we'll use simple regex to find IDs
        
        # Combine all prompts
        all_text = " ".join(prompts)
        
        # Look for patient ID pattern (e.g., "Patient ID: 12345")
        patient_match = re.search(r'Patient ID[:\s]+([A-Za-z0-9-]+)', all_text)
        if patient_match:
            self.current_patient_id = patient_match.group(1)
        
        # Look for provider ID pattern (e.g., "Provider: Dr. Smith")
        provider_match = re.search(r'Provider[:\s]+([A-Za-z0-9\s.]+)', all_text)
        if provider_match:
            self.current_provider_id = provider_match.group(1)
        
        # Also check metadata
        if "metadata" in kwargs:
            metadata = kwargs["metadata"]
            if "patient_id" in metadata:
                self.current_patient_id = metadata["patient_id"]
            if "provider_id" in metadata:
                self.current_provider_id = metadata["provider_id"]
    
    def _analyze_response(self, response: LLMResult) -> None:
        """Analyze LLM response for potential custom events"""
        # Get the response text
        if response.generations and len(response.generations) > 0:
            generation = response.generations[0]
            if len(generation) > 0:
                response_text = generation[0].text
                
                # Check for medical conditions
                self._check_for_medical_conditions(response_text)
                
                # Check for compliance issues
                self._check_for_compliance_issues(response_text)
                
                # Check for other custom events
                self._check_for_critical_values(response_text)
                self._check_for_prescriptions(response_text)
                self._check_for_referrals(response_text)
    
    def _check_for_medical_conditions(self, text: str) -> None:
        """Check for medical conditions in the response text"""
        # Define patterns for medical conditions
        conditions = [
            r'\b(sepsis|septic)\b',
            r'\b(myocardial infarction|heart attack|MI)\b',
            r'\b(stroke|CVA)\b',
            r'\b(pneumonia)\b',
            r'\b(UTI|urinary tract infection)\b'
        ]
        
        for pattern in conditions:
            if re.search(pattern, text, re.IGNORECASE):
                # Extract the condition name
                match = re.search(pattern, text, re.IGNORECASE)
                condition = match.group(1)
                
                # Create and trigger custom event
                event = CustomEvent(
                    event_type=CustomEventType.MEDICAL_CONDITION_DETECTED,
                    timestamp=datetime.now(),
                    severity="warning",  # Default severity
                    patient_id=self.current_patient_id,
                    provider_id=self.current_provider_id,
                    data={
                        "condition": condition,
                        "context": text,
                        "confidence": 0.8  # In a real system, this would be calculated
                    }
                )
                
                # Adjust severity based on condition
                if condition.lower() in ["sepsis", "myocardial infarction", "stroke"]:
                    event.severity = "critical"
                
                self._trigger_event(event)
    
    def _check_for_compliance_issues(self, text: str) -> None:
        """Check for compliance issues in the response text"""
        # Define patterns for compliance issues
        compliance_patterns = [
            (r'\b(without consent|consent not obtained)\b', "informed_consent"),
            (r'\b(HIPAA violation|privacy breach)\b', "privacy_violation"),
            (r'\b(off-label use|unapproved indication)\b', "off_label_use"),
            (r'\b(documentation missing|not documented)\b', "documentation")
        ]
        
        for pattern, compliance_type in compliance_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                # Create and trigger custom event
                event = CustomEvent(
                    event_type=CustomEventType.COMPLIANCE_CHECK,
                    timestamp=datetime.now(),
                    severity="warning",
                    patient_id=self.current_patient_id,
                    provider_id=self.current_provider_id,
                    data={
                        "compliance_type": compliance_type,
                        "context": text,
                        "detected_pattern": pattern
                    }
                )
                
                self._trigger_event(event)
    
    def _check_for_critical_values(self, text: str) -> None:
        """Check for critical lab values in the response text"""
        # Define patterns for critical values
        critical_patterns = [
            (r'\b(potassium|K)\s*[:=]\s*(\d+\.?\d*)\s*mEq/L', ("potassium", 2.5, 6.0)),
            (r'\b(sodium|Na)\s*[:=]\s*(\d+\.?\d*)\s*mEq/L', ("sodium", 120, 160)),
            (r'\b(glucose|blood sugar)\s*[:=]\s*(\d+\.?\d*)\s*mg/dL', ("glucose", 40, 600)),
            (r'\b(hemoglobin|Hgb)\s*[:=]\s*(\d+\.?\d*)\s*g/dL', ("hemoglobin", 6.5, 18.0))
        ]
        
        for pattern, (analyte, min_val, max_val) in critical_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                value = float(match.group(2))
                
                # Check if value is outside critical range
                if value < min_val or value > max_val:
                    event = CustomEvent(
                        event_type=CustomEventType.CRITICAL_VALUE_DETECTED,
                        timestamp=datetime.now(),
                        severity="critical",
                        patient_id=self.current_patient_id,
                        provider_id=self.current_provider_id,
                        data={
                            "analyte": analyte,
                            "value": value,
                            "min_critical": min_val,
                            "max_critical": max_val,
                            "context": text
                        }
                    )
                    
                    self._trigger_event(event)
    
    def _check_for_prescriptions(self, text: str) -> None:
        """Check for prescriptions in the response text"""
        # Define patterns for prescriptions
        prescription_pattern = r'\b(prescribe|prescription|rx)\s+([A-Za-z]+)'
        
        matches = re.finditer(prescription_pattern, text, re.IGNORECASE)
        for match in matches:
            medication = match.group(2)
            
            event = CustomEvent(
                event_type=CustomEventType.PRESCRIPTION_GENERATED,
                timestamp=datetime.now(),
                severity="info",
                patient_id=self.current_patient_id,
                provider_id=self.current_provider_id,
                data={
                    "medication": medication,
                    "context": text
                }
            )
            
            self._trigger_event(event)
    
    def _check_for_referrals(self, text: str) -> None:
        """Check for referrals in the response text"""
        # Define patterns for referrals
        referral_pattern = r'\b(refer|referral|consult)\s+(to|with)\s+([A-Za-z\s]+)'
        
        matches = re.finditer(referral_pattern, text, re.IGNORECASE)
        for match in matches:
            specialist = match.group(3)
            
            event = CustomEvent(
                event_type=CustomEventType.REFERRAL_NEEDED,
                timestamp=datetime.now(),
                severity="info",
                patient_id=self.current_patient_id,
                provider_id=self.current_provider_id,
                data={
                    "specialist": specialist,
                    "context": text
                }
            )
            
            self._trigger_event(event)
    
    def _trigger_event(self, event: CustomEvent) -> None:
        """Trigger a custom event"""
        # Add to event history
        self.event_history.append(event)
        
        # Log the event
        logger.info(f"Custom event triggered: {event.event_type.value} (Severity: {event.severity})")
        
        # Get the appropriate handler
        handler = self.custom_handlers.get(event.event_type)
        
        # Handle the event if a handler is available
        if handler:
            try:
                handler.handle_event(event)
            except Exception as e:
                logger.error(f"Error handling custom event {event.event_type.value}: {str(e)}")
    
    def get_event_history(self, event_type: Optional[CustomEventType] = None) -> List[CustomEvent]:
        """Get the event history, optionally filtered by event type"""
        if event_type is None:
            return self.event_history.copy()
        else:
            return [event for event in self.event_history if event.event_type == event_type]
    
    def clear_event_history(self) -> None:
        """Clear the event history"""
        self.event_history = []

# Example notification system for demonstration
class NotificationSystem:
    """Simple notification system for demonstration"""
    
    def send_alert(self, recipient: str, message: str, severity: str, protocol: str) -> None:
        """Send an alert"""
        print(f"[NOTIFICATION] To: {recipient}")
        print(f"[NOTIFICATION] Message: {message}")
        print(f"[NOTIFICATION] Severity: {severity}")
        print(f"[NOTIFICATION] Protocol: {protocol}")
        print(f"[NOTIFICATION] Time: {datetime.now()}")

# Example usage in a healthcare context
def custom_callback_example():
    """Example of using custom callbacks in a healthcare application"""
    # Create notification system
    notification_system = NotificationSystem()
    
    # Create callback handler
    callback_handler = HealthcareCallbackHandler()
    
    # Set up custom handlers
    callback_handler.register_handler(
        CustomEventType.MEDICAL_CONDITION_DETECTED,
        MedicalConditionHandler(notification_system)
    )
    
    callback_handler.register_handler(
        CustomEventType.COMPLIANCE_CHECK,
        ComplianceHandler(compliance_officer_email="compliance@hospital.org")
    )
    
    # Create LLM
    llm = ChatOpenAI(model="gpt-4", temperature=0.1)
    
    # Create a chain with the custom callback handler
    prompt_template = """
    You are a medical AI assistant. Analyze the patient information and provide a professional assessment.
    
    Patient ID: {patient_id}
    Provider: {provider_id}
    Patient Information: {patient_info}
    
    Assessment:
    """
    
    prompt = PromptTemplate(
        input_variables=["patient_id", "provider_id", "patient_info"],
        template=prompt_template
    )
    
    chain = LLMChain(
        llm=llm,
        prompt=prompt,
        callbacks=[callback_handler]
    )
    
    # Example patient cases
    patient_cases = [
        {
            "patient_id": "P-12345",
            "provider_id": "Dr. Smith",
            "patient_info": "65-year-old male presenting with fever, elevated heart rate, and confusion. Vital signs: BP 90/60, HR 120, Temp 103°F. Lab results show WBC 15,000, lactate 4.2 mmol/L."
        },
        {
            "patient_id": "P-67890",
            "provider_id": "Dr. Johnson",
            "patient_info": "45-year-old female with chest pain and shortness of breath. ECG shows ST elevation in leads II, III, and aVF. Troponin elevated at 5.2 ng/mL."
        },
        {
            "patient_id": "P-24680",
            "provider_id": "Dr. Williams",
            "patient_info": "72-year-old male with sudden onset of left-sided weakness and slurred speech. Symptoms began 2 hours ago. History of hypertension and atrial fibrillation."
        },
        {
            "patient_id": "P-13579",
            "provider_id": "Dr. Brown",
            "patient_info": "58-year-old female with cough, fever, and productive sputum. Chest X-ray shows right lower lobe infiltrate. Vital signs stable. Patient reports no consent for treatment."
        }
    ]
    
    # Process each patient case
    for case in patient_cases:
        print(f"\nProcessing patient {case['patient_id']}...")
        
        try:
            response = chain.run(
                patient_id=case["patient_id"],
                provider_id=case["provider_id"],
                patient_info=case["patient_info"]
            )
            print(f"Assessment: {response[:200]}...")
        except Exception as e:
            print(f"Error: {str(e)}")
    
    # Get event history
    print("\nEvent History:")
    for event in callback_handler.get_event_history():
        print(f"  {event.timestamp}: {event.event_type.value} (Severity: {event.severity})")
        if event.patient_id:
            print(f"    Patient ID: {event.patient_id}")
        if event.data:
            print(f"    Data: {json.dumps(event.data, indent=2)}")
    
    # Get compliance handler
    compliance_handler = callback_handler.custom_handlers[CustomEventType.COMPLIANCE_CHECK]
    if compliance_handler:
        # Generate compliance report
        start_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = datetime.now()
        
        report = compliance_handler.get_compliance_report(start_date, end_date)
        print(f"\nCompliance Report ({len(report)} events):")
        for event in report:
            print(f"  {event['timestamp']}: {event['compliance_type']}")

# Run the custom callback example
custom_callback_example()</code></pre>
                            
                            <p>In a production healthcare environment, this custom callback event implementation would be extended with:</p>
                            <ul>
                                <li>Integration with Electronic Health Record (EHR) systems to automatically update patient records based on detected conditions</li>
                                <li>Real-time alerts to medical staff through hospital communication systems</li>
                                <li>Compliance with healthcare regulations such as HIPAA for patient data handling</li>
                                <li>Integration with clinical decision support systems to provide evidence-based recommendations</li>
                                <li>Advanced analytics to track the effectiveness of interventions and improve detection algorithms over time</li>
                            </ul>
                        </div>
                    </div>
                    
                    <!-- Component 10.2.3 -->
                    <div class="component-card">
                        <h3 class="component-title">10.2.3 Tracing</h3>
                        
                        <div class="info-section">
                            <div class="info-title business-case">
                                <i class="fas fa-check-circle"></i> Business Use Case
                            </div>
                            <p>In the retail industry, tracing enables businesses to understand and optimize customer journey interactions with AI-powered shopping assistants. A major e-commerce platform implemented LangSmith tracing to track every step of customer interactions, from initial query to product recommendation and purchase decision. This implementation provided detailed insights into conversation flows, identified points where customers abandoned interactions, and revealed which product recommendations led to the highest conversion rates. As a result, the company optimized their conversational flows, increasing conversion rates by 28% and reducing customer acquisition costs by 18%.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title problems-solved">
                                <i class="fas fa-lightbulb"></i> Problems Solved
                            </div>
                            <p>Tracing solves the critical challenge of understanding and debugging complex AI system behaviors in production environments. Without tracing, businesses operate with limited visibility into how their AI systems process requests, make decisions, and generate responses. This makes it difficult to identify performance bottlenecks, debug errors, optimize prompts, or understand user interaction patterns. Tracing provides end-to-end visibility into the execution flow, enabling businesses to analyze, debug, and optimize their AI applications effectively.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title impact-absent">
                                <i class="fas fa-exclamation-triangle"></i> Impact if Absent
                            </div>
                            <p>Without tracing, businesses would face significant challenges in maintaining and improving their AI systems. In a banking context, this would mean inability to debug issues in virtual banking assistants, difficulty understanding why certain customer queries fail, and limited visibility into system performance during peak usage periods. This would result in longer resolution times for customer issues, inconsistent service quality, inability to optimize system performance, and missed opportunities to improve customer experience based on interaction data.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title predefined-custom">
                                <i class="fas fa-cogs"></i> Predefined or Custom
                            </div>
                            <p>Custom</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title relationship">
                                <i class="fas fa-link"></i> Relationship Keyword
                            </div>
                            <p>Control</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title technical-insight">
                                <i class="fas fa-microchip"></i> In-depth Technical Insight
                            </div>
                            <p>Tracing in LangChain involves capturing detailed information about the execution flow of chains and agents, including inputs, outputs, intermediate steps, and performance metrics. LangSmith provides a comprehensive platform for collecting, analyzing, and visualizing this trace data to help developers understand and optimize their applications.</p>
                            
                            <pre><code>import os
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.chains import LLMChain, SequentialChain
from langchain.prompts import PromptTemplate
from langchain.callbacks.tracers import LangChainTracer
from langchain.callbacks.manager import CallbackManager
from langsmith import Client
from typing import Dict, List, Any, Optional, Union
import json
import time
from datetime import datetime, timedelta
from dataclasses import dataclass, field
import uuid
import logging
from enum import Enum
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Set up LangSmith environment variables
# In a real application, these would be set in the environment
os.environ["LANGCHAIN_API_KEY"] = "your_langsmith_api_key"
os.environ["LANGCHAIN_PROJECT"] = "retail-shopping-assistant"

@dataclass
class TraceMetadata:
    """Metadata for trace information"""
    trace_id: str
    session_id: str
    user_id: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    duration: Optional[float] = None  # in seconds
    success: bool = True
    error_message: Optional[str] = None
    input_tokens: int = 0
    output_tokens: int = 0
    total_cost: float = 0.0
    custom_metrics: Dict[str, Any] = field(default_factory=dict)

class RetailTraceAnalyzer:
    """Analyzer for retail shopping assistant traces"""
    
    def __init__(self, langsmith_client: Client):
        self.client = langsmith_client
        self.trace_cache = {}
        self.session_data = defaultdict(list)
        
    def fetch_traces(self, 
                    project_name: str = None,
                    start_time: datetime = None,
                    end_time: datetime = None,
                    limit: int = 100) -> List[Dict]:
        """Fetch traces from LangSmith"""
        # Set default project if not provided
        if project_name is None:
            project_name = os.environ.get("LANGCHAIN_PROJECT", "default")
        
        # Set default time range if not provided
        if start_time is None:
            start_time = datetime.now() - timedelta(days=7)
        if end_time is None:
            end_time = datetime.now()
        
        # Fetch traces
        traces = self.client.list_runs(
            project_name=project_name,
            start_time=start_time,
            end_time=end_time,
            limit=limit
        )
        
        # Convert to list of dictionaries
        trace_list = []
        for trace in traces:
            trace_data = {
                "id": trace.id,
                "name": trace.name,
                "start_time": trace.start_time,
                "end_time": trace.end_time,
                "duration": (trace.end_time - trace.start_time).total_seconds() if trace.end_time else None,
                "error": trace.error,
                "inputs": trace.inputs,
                "outputs": trace.outputs,
                "tags": trace.tags,
                "metadata": trace.metadata or {}
            }
            trace_list.append(trace_data)
            
            # Cache the trace
            self.trace_cache[trace.id] = trace_data
            
            # Organize by session
            session_id = trace.metadata.get("session_id", "unknown")
            self.session_data[session_id].append(trace_data)
        
        return trace_list
    
    def analyze_conversation_flows(self) -> Dict[str, Any]:
        """Analyze conversation flows to identify patterns"""
        # Group traces by session
        session_flows = {}
        
        for session_id, traces in self.session_data.items():
            # Sort traces by start time
            sorted_traces = sorted(traces, key=lambda x: x["start_time"])
            
            # Extract conversation flow
            flow = []
            for trace in sorted_traces:
                # Extract user input and assistant response
                inputs = trace.get("inputs", {})
                outputs = trace.get("outputs", {})
                
                user_input = inputs.get("input", inputs.get("question", ""))
                assistant_response = outputs.get("output", outputs.get("text", ""))
                
                flow.append({
                    "timestamp": trace["start_time"],
                    "user_input": user_input,
                    "assistant_response": assistant_response,
                    "trace_id": trace["id"],
                    "duration": trace["duration"]
                })
            
            session_flows[session_id] = flow
        
        # Analyze patterns
        analysis = {
            "total_sessions": len(session_flows),
            "average_conversation_length": sum(len(flow) for flow in session_flows.values()) / len(session_flows) if session_flows else 0,
            "common_patterns": self._identify_common_patterns(session_flows),
            "abandonment_points": self._identify_abandonment_points(session_flows)
        }
        
        return analysis
    
    def _identify_common_patterns(self, session_flows: Dict[str, List]) -> List[Dict]:
        """Identify common conversation patterns"""
        # Extract sequences of user inputs
        input_sequences = []
        for session_id, flow in session_flows.items():
            sequence = [interaction["user_input"] for interaction in flow]
            input_sequences.append(sequence)
        
        # Find common sequences (simplified approach)
        # In a real system, this would use more sophisticated pattern mining
        pattern_counts = defaultdict(int)
        
        for sequence in input_sequences:
            # Extract pairs of consecutive inputs
            for i in range(len(sequence) - 1):
                pattern = (sequence[i], sequence[i+1])
                pattern_counts[pattern] += 1
        
        # Get top patterns
        top_patterns = sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return [
            {
                "pattern": list(pattern),
                "count": count
            }
            for pattern, count in top_patterns
        ]
    
    def _identify_abandonment_points(self, session_flows: Dict[str, List]) -> List[Dict]:
        """Identify points where users abandon conversations"""
        abandonment_points = []
        
        for session_id, flow in session_flows.items():
            # Check if the last interaction has no response (potential abandonment)
            if flow and not flow[-1]["assistant_response"]:
                abandonment_points.append({
                    "session_id": session_id,
                    "timestamp": flow[-1]["timestamp"],
                    "last_input": flow[-1]["user_input"],
                    "conversation_length": len(flow)
                })
        
        return abandonment_points
    
    def analyze_product_recommendations(self) -> Dict[str, Any]:
        """Analyze product recommendation effectiveness"""
        # Find traces that include product recommendations
        recommendation_traces = []
        
        for session_id, traces in self.session_data.items():
            for trace in traces:
                outputs = trace.get("outputs", {})
                response = outputs.get("output", outputs.get("text", ""))
                
                # Check if response contains product recommendations
                if "recommend" in response.lower() or "product" in response.lower():
                    recommendation_traces.append({
                        "trace_id": trace["id"],
                        "session_id": session_id,
                        "response": response,
                        "timestamp": trace["start_time"]
                    })
        
        # Analyze recommendation patterns
        analysis = {
            "total_recommendations": len(recommendation_traces),
            "recommendation_frequency": len(recommendation_traces) / len(self.trace_cache) if self.trace_cache else 0,
            "top_recommended_products": self._extract_top_products(recommendation_traces),
            "recommendation_outcomes": self._analyze_recommendation_outcomes(recommendation_traces)
        }
        
        return analysis
    
    def _extract_top_products(self, recommendation_traces: List[Dict]) -> List[Dict]:
        """Extract the most frequently recommended products"""
        # Simple product extraction (in a real system, this would use NLP)
        product_counts = defaultdict(int)
        
        for trace in recommendation_traces:
            response = trace["response"]
            
            # Look for product patterns (simplified)
            # In a real system, this would use named entity recognition
            words = response.split()
            for i in range(len(words) - 1):
                if words[i].lower() in ["product", "item", "model"] and i + 1 < len(words):
                    product = words[i + 1]
                    product_counts[product] += 1
        
        # Get top products
        top_products = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return [
            {
                "product": product,
                "count": count
            }
            for product, count in top_products
        ]
    
    def _analyze_recommendation_outcomes(self, recommendation_traces: List[Dict]) -> Dict[str, Any]:
        """Analyze the outcomes of product recommendations"""
        # This would require additional data about user actions after recommendations
        # For this example, we'll simulate some analysis
        
        # Simulate conversion rates
        total_recommendations = len(recommendation_traces)
        simulated_conversions = int(total_recommendations * 0.25)  # 25% conversion rate
        
        return {
            "total_recommendations": total_recommendations,
            "simulated_conversions": simulated_conversions,
            "simulated_conversion_rate": simulated_conversions / total_recommendations if total_recommendations > 0 else 0
        }
    
    def analyze_performance_metrics(self) -> Dict[str, Any]:
        """Analyze performance metrics across traces"""
        if not self.trace_cache:
            return {"error": "No trace data available"}
        
        # Calculate aggregate metrics
        total_traces = len(self.trace_cache)
        successful_traces = sum(1 for trace in self.trace_cache.values() if not trace.get("error"))
        
        durations = [trace["duration"] for trace in self.trace_cache.values() if trace["duration"] is not None]
        avg_duration = sum(durations) / len(durations) if durations else 0
        
        # Calculate token usage (if available)
        total_input_tokens = 0
        total_output_tokens = 0
        
        for trace in self.trace_cache.values():
            metadata = trace.get("metadata", {})
            total_input_tokens += metadata.get("input_tokens", 0)
            total_output_tokens += metadata.get("output_tokens", 0)
        
        # Calculate costs (simplified)
        # In a real system, this would use actual pricing
        input_cost = total_input_tokens * 0.00001  # $0.01 per 1K tokens
        output_cost = total_output_tokens * 0.00003  # $0.03 per 1K tokens
        total_cost = input_cost + output_cost
        
        return {
            "total_traces": total_traces,
            "successful_traces": successful_traces,
            "success_rate": successful_traces / total_traces if total_traces > 0 else 0,
            "average_duration_seconds": avg_duration,
            "total_input_tokens": total_input_tokens,
            "total_output_tokens": total_output_tokens,
            "total_tokens": total_input_tokens + total_output_tokens,
            "estimated_cost": total_cost,
            "cost_per_trace": total_cost / total_traces if total_traces > 0 else 0
        }
    
    def generate_report(self, output_format: str = "dict") -> Union[Dict, str]:
        """Generate a comprehensive analysis report"""
        # Perform all analyses
        conversation_analysis = self.analyze_conversation_flows()
        recommendation_analysis = self.analyze_product_recommendations()
        performance_analysis = self.analyze_performance_metrics()
        
        # Combine into a single report
        report = {
            "timestamp": datetime.now().isoformat(),
            "conversation_analysis": conversation_analysis,
            "recommendation_analysis": recommendation_analysis,
            "performance_analysis": performance_analysis
        }
        
        if output_format == "json":
            return json.dumps(report, indent=2)
        else:
            return report
    
    def visualize_data(self):
        """Create visualizations of the trace data"""
        # Create a figure with multiple subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Retail Shopping Assistant Trace Analysis', fontsize=16)
        
        # Plot 1: Conversation length distribution
        session_lengths = [len(traces) for traces in self.session_data.values()]
        axes[0, 0].hist(session_lengths, bins=20, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('Conversation Length Distribution')
        axes[0, 0].set_xlabel('Number of Interactions')
        axes[0, 0].set_ylabel('Number of Sessions')
        
        # Plot 2: Trace duration distribution
        durations = [trace["duration"] for trace in self.trace_cache.values() if trace["duration"] is not None]
        axes[0, 1].hist(durations, bins=20, color='lightgreen', edgecolor='black')
        axes[0, 1].set_title('Trace Duration Distribution')
        axes[0, 1].set_xlabel('Duration (seconds)')
        axes[0, 1].set_ylabel('Number of Traces')
        
        # Plot 3: Hourly trace distribution
        hourly_counts = defaultdict(int)
        for trace in self.trace_cache.values():
            hour = trace["start_time"].hour
            hourly_counts[hour] += 1
        
        hours = list(range(24))
        counts = [hourly_counts[hour] for hour in hours]
        
        axes[1, 0].bar(hours, counts, color='salmon')
        axes[1, 0].set_title('Hourly Trace Distribution')
        axes[1, 0].set_xlabel('Hour of Day')
        axes[1, 0].set_ylabel('Number of Traces')
        axes[1, 0].set_xticks(hours[::2])
        
        # Plot 4: Success rate by tag
        tag_success = defaultdict(lambda: {"total": 0, "successful": 0})
        
        for trace in self.trace_cache.values():
            for tag in trace.get("tags", []):
                tag_success[tag]["total"] += 1
                if not trace.get("error"):
                    tag_success[tag]["successful"] += 1
        
        tags = list(tag_success.keys())
        success_rates = [tag_success[tag]["successful"] / tag_success[tag]["total"] if tag_success[tag]["total"] > 0 else 0 for tag in tags]
        
        axes[1, 1].bar(tags, success_rates, color='orchid')
        axes[1, 1].set_title('Success Rate by Tag')
        axes[1, 1].set_xlabel('Tag')
        axes[1, 1].set_ylabel('Success Rate')
        axes[1, 1].set_ylim(0, 1)
        
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.show()

# Example usage in a retail context
def tracing_example():
    """Example of using LangSmith tracing in a retail application"""
    # Initialize LangSmith client
    # In a real application, you would use your actual API key
    # client = Client(api_key="your_langsmith_api_key")
    
    # For this example, we'll simulate the client
    class MockClient:
        def list_runs(self, project_name, start_time, end_time, limit):
            # Return empty list for simulation
            return []
    
    client = MockClient()
    
    # Create trace analyzer
    analyzer = RetailTraceAnalyzer(client)
    
    # Create LLM with tracing
    llm = ChatOpenAI(model="gpt-4", temperature=0.7)
    
    # Create a tracer
    tracer = LangChainTracer()
    
    # Create a callback manager with the tracer
    callback_manager = CallbackManager([tracer])
    
    # Create a chain for product recommendations
    product_recommendation_prompt = """
    You are a helpful retail shopping assistant. The customer is asking for product recommendations.
    Provide personalized recommendations based on their needs and preferences.
    
    Customer: {customer_input}
    
    Your Response:
    """
    
    product_recommendation_template = PromptTemplate(
        input_variables=["customer_input"],
        template=product_recommendation_prompt
    )
    
    product_chain = LLMChain(
        llm=llm,
        prompt=product_recommendation_template,
        callbacks=callback_manager
    )
    
    # Create a chain for product information
    product_info_prompt = """
    You are a retail product expert. Provide detailed information about the requested product.
    Include features, specifications, and benefits.
    
    Product: {product_name}
    
    Information:
    """
    
    product_info_template = PromptTemplate(
        input_variables=["product_name"],
        template=product_info_prompt
    )
    
    info_chain = LLMChain(
        llm=llm,
        prompt=product_info_template,
        callbacks=callback_manager
    )
    
    # Create a sequential chain
    sequential_chain = SequentialChain(
        chains=[product_chain, info_chain],
        input_variables=["customer_input", "product_name"],
        output_variables=["output", "text"],
        callbacks=callback_manager
    )
    
    # Example customer interactions
    customer_interactions = [
        {
            "customer_input": "I'm looking for a birthday gift for my wife who loves cooking. She already has good knives and a stand mixer.",
            "product_name": "kitchen appliance"
        },
        {
            "customer_input": "I need a new laptop for work. I mostly use it for email, web browsing, and occasional video calls.",
            "product_name": "laptop computer"
        },
        {
            "customer_input": "My son wants a new gaming console for Christmas. Which one would you recommend?",
            "product_name": "gaming console"
        }
    ]
    
    # Process each interaction
    for interaction in customer_interactions:
        print(f"\nProcessing: {interaction['customer_input'][:50]}...")
        
        try:
            # Add metadata for tracing
            metadata = {
                "session_id": str(uuid.uuid4()),
                "user_id": "customer_123",
                "interaction_type": "product_recommendation"
            }
            
            # Run the chain with metadata
            result = sequential_chain(
                customer_input=interaction["customer_input"],
                product_name=interaction["product_name"],
                metadata=metadata
            )
            
            print(f"Recommendation: {result['output'][:100]}...")
            print(f"Product info: {result['text'][:100]}...")
            
        except Exception as e:
            print(f"Error: {str(e)}")
    
    # In a real application, we would now fetch and analyze the traces
    # For this example, we'll simulate the analysis
    
    # Simulate fetching traces
    print("\nSimulating trace analysis...")
    
    # Create some simulated trace data
    simulated_traces = [
        {
            "id": str(uuid.uuid4()),
            "name": "ProductRecommendationChain",
            "start_time": datetime.now() - timedelta(days=1),
            "end_time": datetime.now() - timedelta(days=1) + timedelta(seconds=5),
            "duration": 5.2,
            "error": None,
            "inputs": {"customer_input": "I'm looking for a birthday gift..."},
            "outputs": {"output": "I recommend a high-quality food processor..."},
            "tags": ["recommendation", "kitchen"],
            "metadata": {
                "session_id": "session_1",
                "user_id": "customer_123",
                "input_tokens": 50,
                "output_tokens": 150
            }
        },
        {
            "id": str(uuid.uuid4()),
            "name": "ProductInfoChain",
            "start_time": datetime.now() - timedelta(days=1) + timedelta(seconds=5.2),
            "end_time": datetime.now() - timedelta(days=1) + timedelta(seconds=8.7),
            "duration": 3.5,
            "error": None,
            "inputs": {"product_name": "food processor"},
            "outputs": {"text": "A food processor is a versatile kitchen appliance..."},
            "tags": ["information", "kitchen"],
            "metadata": {
                "session_id": "session_1",
                "user_id": "customer_123",
                "input_tokens": 10,
                "output_tokens": 200
            }
        }
    ]
    
    # Add simulated traces to analyzer
    for trace in simulated_traces:
        analyzer.trace_cache[trace["id"]] = trace
        session_id = trace["metadata"]["session_id"]
        analyzer.session_data[session_id].append(trace)
    
    # Generate analysis report
    report = analyzer.generate_report()
    
    print("\nAnalysis Report:")
    print(json.dumps(report, indent=2))
    
    # Visualize data (in a real application, this would display plots)
    print("\nGenerating visualizations...")
    # analyzer.visualize_data()

# Run the tracing example
tracing_example()</code></pre>
                            
                            <p>In a production retail environment, this tracing implementation would be extended with:</p>
                            <ul>
                                <li>Integration with customer analytics platforms to correlate AI interactions with purchase behavior</li>
                                <li>Real-time dashboards for monitoring system performance and customer experience metrics</li>
                                <li>Automated alerting for unusual patterns or performance degradation</li>
                                <li>A/B testing capabilities to compare different prompt strategies and conversation flows</li>
                                <li>Integration with inventory management systems to track product recommendation effectiveness against stock levels</li>
                            </ul>
                        </div>
                    </div>
                    
                    <!-- Component 10.2.4 -->
                    <div class="component-card">
                        <h3 class="component-title">10.2.4 Chain Inspection</h3>
                        
                        <div class="info-section">
                            <div class="info-title business-case">
                                <i class="fas fa-check-circle"></i> Business Use Case
                            </div>
                            <p>In the banking industry, chain inspection enables financial institutions to understand and optimize their AI-powered customer service systems. A major retail bank implemented chain inspection tools to analyze how their virtual banking assistant processes customer inquiries, from initial query to final response. This implementation revealed that 40% of loan application queries were being mishandled due to ambiguous prompts, leading to customer frustration. By optimizing these chains based on inspection insights, the bank reduced query resolution time by 35% and increased customer satisfaction scores by 22% for financial service inquiries.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title problems-solved">
                                <i class="fas fa-lightbulb"></i> Problems Solved
                            </div>
                            <p>Chain inspection solves the challenge of understanding the internal workings of complex AI chains and agents. Without inspection capabilities, developers and business stakeholders have limited visibility into how inputs are transformed through various processing steps to produce outputs. This makes it difficult to debug issues, optimize performance, ensure consistent behavior, or validate that business requirements are being met throughout the chain execution.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title impact-absent">
                                <i class="fas fa-exclamation-triangle"></i> Impact if Absent
                            </div>
                            <p>Without chain inspection, businesses would operate their AI systems with limited understanding of internal processes, leading to significant operational challenges. In a healthcare context, this would mean inability to verify how diagnostic AI systems process patient data, potentially allowing errors or biases to go undetected. This could result in incorrect medical recommendations, compliance violations, inability to optimize system performance, and lack of auditability for critical healthcare decisions.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title predefined-custom">
                                <i class="fas fa-cogs"></i> Predefined or Custom
                            </div>
                            <p>Custom</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title relationship">
                                <i class="fas fa-link"></i> Relationship Keyword
                            </div>
                            <p>Interface</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title technical-insight">
                                <i class="fas fa-microchip"></i> In-depth Technical Insight
                            </div>
                            <p>Chain inspection in LangChain involves examining the structure, inputs, outputs, and intermediate states of chains and agents. This capability allows developers to understand how data flows through the system, identify bottlenecks, debug issues, and optimize performance. Inspection can be performed at various levels, from individual components to entire chain executions.</p>
                            
                            <pre><code>import unittest
from unittest.mock import Mock, patch
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.chains import LLMChain, SequentialChain, TransformChain
from langchain.prompts import PromptTemplate
from typing import Dict, List, Any, Optional, Union, Callable
import json
import time
from datetime import datetime
from dataclasses import dataclass, field
import uuid
import logging
from enum import Enum
import pytest
import pandas as pd
import numpy as np
from io import StringIO
import sys
import os
import tempfile
import shutil

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TestCase:
    """Represents a test case with inputs and expected outputs"""
    name: str
    inputs: Dict[str, Any]
    expected_outputs: Dict[str, Any]
    description: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)
    timeout: float = 30.0

@dataclass
class TestResult:
    """Represents the result of a test case execution"""
    test_case: TestCase
    actual_outputs: Dict[str, Any]
    success: bool
    execution_time: float
    error_message: Optional[str] = None
    diff: Optional[str] = None

class TestRunner:
    """Runs test cases and collects results"""
    
    def __init__(self):
        self.results = []
        self.setup_functions = []
        self.teardown_functions = []
    
    def add_setup(self, func: Callable) -> None:
        """Add a setup function to be run before each test"""
        self.setup_functions.append(func)
    
    def add_teardown(self, func: Callable) -> None:
        """Add a teardown function to be run after each test"""
        self.teardown_functions.append(func)
    
    def run_test(self, test_case: TestCase, test_func: Callable) -> TestResult:
        """Run a single test case"""
        start_time = time.time()
        
        # Run setup functions
        for setup_func in self.setup_functions:
            try:
                setup_func()
            except Exception as e:
                return TestResult(
                    test_case=test_case,
                    actual_outputs={},
                    success=False,
                    execution_time=time.time() - start_time,
                    error_message=f"Setup failed: {str(e)}"
                )
        
        # Run the test function with timeout
        try:
            # Run with timeout
            actual_outputs = self._run_with_timeout(test_func, test_case.inputs, test_case.timeout)
            
            # Compare with expected outputs
            success, diff = self._compare_outputs(actual_outputs, test_case.expected_outputs)
            
            result = TestResult(
                test_case=test_case,
                actual_outputs=actual_outputs,
                success=success,
                execution_time=time.time() - start_time,
                diff=diff
            )
            
        except Exception as e:
            result = TestResult(
                test_case=test_case,
                actual_outputs={},
                success=False,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
        
        # Run teardown functions
        for teardown_func in self.teardown_functions:
            try:
                teardown_func()
            except Exception as e:
                logger.error(f"Teardown failed: {str(e)}")
        
        return result
    
    def _run_with_timeout(self, func: Callable, inputs: Dict[str, Any], timeout: float) -> Dict[str, Any]:
        """Run a function with a timeout"""
        # For simplicity, we'll use a basic approach
        # In a real system, you might use more sophisticated timeout handling
        
        # Create a new event loop for async functions
        if asyncio.iscoroutinefunction(func):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(
                    asyncio.wait_for(func(**inputs), timeout=timeout)
                )
            finally:
                loop.close()
        else:
            # For sync functions, just call directly
            # In a real system, you might use threading or signal-based timeout
            return func(**inputs)
    
    def _compare_outputs(self, actual: Dict[str, Any], expected: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """Compare actual outputs with expected outputs"""
        # For simplicity, we'll use a basic comparison
        # In a real system, you might use more sophisticated comparison logic
        
        if set(actual.keys()) != set(expected.keys()):
            diff = f"Key mismatch: actual={list(actual.keys())}, expected={list(expected.keys())}"
            return False, diff
        
        for key in expected:
            if key not in actual:
                diff = f"Missing key in actual output: {key}"
                return False, diff
            
            # Handle different comparison strategies based on type
            if isinstance(expected[key], str) and isinstance(actual[key], str):
                # For strings, use a flexible comparison
                if expected[key].lower() not in actual[key].lower():
                    diff = f"String mismatch for key '{key}': expected='{expected[key]}', actual='{actual[key]}'"
                    return False, diff
            elif isinstance(expected[key], (int, float)) and isinstance(actual[key], (int, float)):
                # For numbers, use a tolerance-based comparison
                tolerance = 0.01  # 1% tolerance
                if abs(actual[key] - expected[key]) > abs(expected[key] * tolerance):
                    diff = f"Number mismatch for key '{key}': expected={expected[key]}, actual={actual[key]}"
                    return False, diff
            elif isinstance(expected[key], dict) and isinstance(actual[key], dict):
                # For dictionaries, recursive comparison
                success, diff = self._compare_outputs(actual[key], expected[key])
                if not success:
                    return False, f"Dictionary mismatch for key '{key}': {diff}"
            elif isinstance(expected[key], list) and isinstance(actual[key], list):
                # For lists, compare elements
                if len(expected[key]) != len(actual[key]):
                    diff = f"List length mismatch for key '{key}': expected={len(expected[key])}, actual={len(actual[key])}"
                    return False, diff
                
                for i, (exp_item, act_item) in enumerate(zip(expected[key], actual[key])):
                    if isinstance(exp_item, dict) and isinstance(act_item, dict):
                        success, diff = self._compare_outputs(act_item, exp_item)
                        if not success:
                            return False, f"List element mismatch at index {i} for key '{key}': {diff}"
                    elif exp_item != act_item:
                        diff = f"List element mismatch at index {i} for key '{key}': expected={exp_item}, actual={act_item}"
                        return False, diff
            elif expected[key] != actual[key]:
                diff = f"Value mismatch for key '{key}': expected={expected[key]}, actual={actual[key]}"
                return False, diff
        
        return True, None
    
    def run_tests(self, test_cases: List[TestCase], test_func: Callable) -> List[TestResult]:
        """Run multiple test cases"""
        results = []
        for test_case in test_cases:
            result = self.run_test(test_case, test_func)
            results.append(result)
            self.results.append(result)
        return results
    
    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of test results"""
        if not self.results:
            return {"total": 0, "passed": 0, "failed": 0, "pass_rate": 0}
        
        total = len(self.results)
        passed = sum(1 for result in self.results if result.success)
        failed = total - passed
        
        return {
            "total": total,
            "passed": passed,
            "failed": failed,
            "pass_rate": passed / total if total > 0 else 0,
            "average_execution_time": sum(result.execution_time for result in self.results) / total
        }

class LangChainTester:
    """Tester for LangChain applications"""
    
    def __init__(self):
        self.test_runner = TestRunner()
        self.test_cases = {}
        self.mocks = {}
    
    def add_test_case(self, category: str, test_case: TestCase) -> None:
        """Add a test case to a category"""
        if category not in self.test_cases:
            self.test_cases[category] = []
        self.test_cases[category].append(test_case)
    
    def add_mock(self, name: str, mock_obj: Any) -> None:
        """Add a mock object for testing"""
        self.mocks[name] = mock_obj
    
    def run_unit_tests(self) -> Dict[str, Any]:
        """Run unit tests for individual components"""
        results = {}
        
        for category, test_cases in self.test_cases.items():
            if category.startswith("unit_"):
                # Determine the test function based on the category
                test_func = self._get_test_function(category)
                
                if test_func:
                    test_results = self.test_runner.run_tests(test_cases, test_func)
                    results[category] = {
                        "test_results": test_results,
                        "summary": self._summarize_results(test_results)
                    }
        
        return results
    
    def run_integration_tests(self) -> Dict[str, Any]:
        """Run integration tests for component interactions"""
        results = {}
        
        for category, test_cases in self.test_cases.items():
            if category.startswith("integration_"):
                # Determine the test function based on the category
                test_func = self._get_test_function(category)
                
                if test_func:
                    test_results = self.test_runner.run_tests(test_cases, test_func)
                    results[category] = {
                        "test_results": test_results,
                        "summary": self._summarize_results(test_results)
                    }
        
        return results
    
    def run_end_to_end_tests(self) -> Dict[str, Any]:
        """Run end-to-end tests for complete workflows"""
        results = {}
        
        for category, test_cases in self.test_cases.items():
            if category.startswith("e2e_"):
                # Determine the test function based on the category
                test_func = self._get_test_function(category)
                
                if test_func:
                    test_results = self.test_runner.run_tests(test_cases, test_func)
                    results[category] = {
                        "test_results": test_results,
                        "summary": self._summarize_results(test_results)
                    }
        
        return results
    
    def run_all_tests(self) -> Dict[str, Any]:
        """Run all tests"""
        unit_results = self.run_unit_tests()
        integration_results = self.run_integration_tests()
        e2e_results = self.run_end_to_end_tests()
        
        # Combine all results
        all_results = {**unit_results, **integration_results, **e2e_results}
        
        # Calculate overall summary
        total_passed = 0
        total_failed = 0
        total_tests = 0
        
        for category_data in all_results.values():
            summary = category_data["summary"]
            total_passed += summary["passed"]
            total_failed += summary["failed"]
            total_tests += summary["total"]
        
        overall_summary = {
            "total": total_tests,
            "passed": total_passed,
            "failed": total_failed,
            "pass_rate": total_passed / total_tests if total_tests > 0 else 0
        }
        
        return {
            "results": all_results,
            "overall_summary": overall_summary
        }
    
    def _get_test_function(self, category: str) -> Optional[Callable]:
        """Get the test function for a category"""
        # This is a simplified approach
        # In a real system, you might have a more sophisticated mapping
        
        if category == "unit_llm_chain":
            return self._test_llm_chain
        elif category == "unit_transform_chain":
            return self._test_transform_chain
        elif category == "unit_memory":
            return self._test_memory
        elif category == "integration_sequential_chain":
            return self._test_sequential_chain
        elif category == "integration_agent":
            return self._test_agent
        elif category == "e2e_banking_assistant":
            return self._test_banking_assistant
        elif category == "e2e_healthcare_diagnostic":
            return self._test_healthcare_diagnostic
        else:
            logger.warning(f"No test function found for category: {category}")
            return None
    
    def _summarize_results(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """Summarize test results"""
        if not test_results:
            return {"total": 0, "passed": 0, "failed": 0, "pass_rate": 0}
        
        total = len(test_results)
        passed = sum(1 for result in test_results if result.success)
        failed = total - passed
        
        return {
            "total": total,
            "passed": passed,
            "failed": failed,
            "pass_rate": passed / total if total > 0 else 0,
            "average_execution_time": sum(result.execution_time for result in test_results) / total
        }
    
    def _test_llm_chain(self, **inputs) -> Dict[str, Any]:
        """Test an LLM chain"""
        # Create a mock LLM
        mock_llm = Mock()
        mock_llm.invoke.return_value = AIMessage(content="Mock response from LLM")
        
        # Create a prompt template
        prompt_template = PromptTemplate(
            input_variables=["question"],
            template="Answer the question: {question}"
        )
        
        # Create the chain
        chain = LLMChain(llm=mock_llm, prompt=prompt_template)
        
        # Run the chain
        result = chain.run(question=inputs.get("question", "What is AI?"))
        
        return {"response": result}
    
    def _test_transform_chain(self, **inputs) -> Dict[str, Any]:
        """Test a transform chain"""
        # Define a transform function
        def uppercase_text(inputs: Dict[str, Any]) -> Dict[str, Any]:
            text = inputs.get("text", "")
            return {"uppercase_text": text.upper()}
        
        # Create the transform chain
        chain = TransformChain(
            transform=uppercase_text,
            input_variables=["text"],
            output_variables=["uppercase_text"]
        )
        
        # Run the chain
        result = chain(text=inputs.get("text", "Hello, World!"))
        
        return {"uppercase_text": result["uppercase_text"]}
    
    def _test_memory(self, **inputs) -> Dict[str, Any]:
        """Test conversation memory"""
        # Create a memory object
        memory = ConversationBufferMemory()
        
        # Add some messages
        memory.chat_memory.add_user_message(inputs.get("user_message", "Hello"))
        memory.chat_memory.add_ai_message(inputs.get("ai_message", "Hi there!"))
        
        # Get the buffer
        buffer = memory.load_memory_variables({})
        
        return {"buffer": buffer}
    
    def _test_sequential_chain(self, **inputs) -> Dict[str, Any]:
        """Test a sequential chain"""
        # Create mock LLM
        mock_llm = Mock()
        mock_llm.invoke.return_value = AIMessage(content="Mock response")
        
        # Create first chain
        first_prompt = PromptTemplate(
            input_variables=["topic"],
            template="Write a sentence about {topic}"
        )
        first_chain = LLMChain(llm=mock_llm, prompt=first_prompt, output_key="sentence")
        
        # Create second chain
        second_prompt = PromptTemplate(
            input_variables=["sentence"],
            template="Translate this sentence to French: {sentence}"
        )
        second_chain = LLMChain(llm=mock_llm, prompt=second_prompt, output_key="translation")
        
        # Create sequential chain
        sequential_chain = SequentialChain(
            chains=[first_chain, second_chain],
            input_variables=["topic"],
            output_variables=["sentence", "translation"]
        )
        
        # Run the chain
        result = sequential_chain(topic=inputs.get("topic", "AI"))
        
        return {
            "sentence": result["sentence"],
            "translation": result["translation"]
        }
    
    def _test_agent(self, **inputs) -> Dict[str, Any]:
        """Test an agent"""
        # Create mock tools
        def search_api(query: str) -> str:
            return f"Search results for: {query}"
        
        def calculator(expression: str) -> str:
            try:
                return str(eval(expression))
            except:
                return "Error: Invalid expression"
        
        tools = [
            Tool(
                name="Search",
                func=search_api,
                description="Useful for searching the internet"
            ),
            Tool(
                name="Calculator",
                func=calculator,
                description="Useful for performing mathematical calculations"
            )
        ]
        
        # Create mock LLM
        mock_llm = Mock()
        mock_llm.invoke.return_value = AIMessage(
            content="I need to use the Search tool to answer this question."
        )
        
        # Create agent
        agent = initialize_agent(
            tools,
            mock_llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True
        )
        
        # Run the agent
        result = agent.run(inputs.get("question", "What is the capital of France?"))
        
        return {"response": result}
    
    def _test_banking_assistant(self, **inputs) -> Dict[str, Any]:
        """Test a banking assistant end-to-end"""
        # Create mock LLM
        mock_llm = Mock()
        mock_llm.invoke.return_value = AIMessage(
            content="Your account balance is $5,000. Would you like to know about our investment options?"
        )
        
        # Create prompt template
        prompt_template = PromptTemplate(
            input_variables=["customer_id", "inquiry"],
            template="""
            You are a banking assistant. Answer the customer's inquiry.
            
            Customer ID: {customer_id}
            Inquiry: {inquiry}
            
            Response:
            """
        )
        
        # Create chain
        chain = LLMChain(llm=mock_llm, prompt=prompt_template)
        
        # Run the chain
        result = chain.run(
            customer_id=inputs.get("customer_id", "CUST12345"),
            inquiry=inputs.get("inquiry", "What is my account balance?")
        )
        
        return {"response": result}
    
    def _test_healthcare_diagnostic(self, **inputs) -> Dict[str, Any]:
        """Test a healthcare diagnostic system end-to-end"""
        # Create mock LLM
        mock_llm = Mock()
        mock_llm.invoke.return_value = AIMessage(
            content="Based on the symptoms provided, the patient may have influenza. Recommend rest, fluids, and over-the-counter fever reducers."
        )
        
        # Create prompt template
        prompt_template = PromptTemplate(
            input_variables=["patient_id", "symptoms"],
            template="""
            You are a healthcare AI assistant. Analyze the patient's symptoms and provide a preliminary diagnosis.
            
            Patient ID: {patient_id}
            Symptoms: {symptoms}
            
            Diagnosis:
            """
        )
        
        # Create chain
        chain = LLMChain(llm=mock_llm, prompt=prompt_template)
        
        # Run the chain
        result = chain.run(
            patient_id=inputs.get("patient_id", "PAT12345"),
            symptoms=inputs.get("symptoms", "Fever, cough, body aches")
        )
        
        return {"diagnosis": result}

# Example usage in a healthcare context
def chain_inspection_example():
    """Example of chain inspection in a healthcare application"""
    # Create inspector
    inspector = ChainInspector()
    
    # Create callback handler
    callback_handler = InspectionCallbackHandler(inspector)
    
    # Create LLM
    llm = ChatOpenAI(model="gpt-4", temperature=0.1)
    
    # Create chains for healthcare operations
    
    # Chain for account balance inquiry
    balance_prompt = PromptTemplate(
        input_variables=["customer_id", "account_type"],
        template="""
        You are a banking assistant. Provide the account balance information for the requested account.
        
        Customer ID: {customer_id}
        Account Type: {account_type}
        
        Balance Information:
        """
    )
    
    balance_chain = LLMChain(
        llm=llm,
        prompt=balance_prompt,
        callbacks=[callback_handler]
    )
    
    # Register the chain
    inspector.register_chain(
        "balance_inquiry",
        balance_chain,
        "Provides account balance information to customers"
    )
    
    # Chain for loan application processing
    loan_prompt = PromptTemplate(
        input_variables=["customer_id", "loan_type", "loan_amount"],
        template="""
        You are a banking loan officer. Process the loan application and provide a preliminary decision.
        
        Customer ID: {customer_id}
        Loan Type: {loan_type}
        Loan Amount: ${loan_amount}
        
        Loan Decision:
        """
    )
    
    loan_chain = LLMChain(
        llm=llm,
        prompt=loan_prompt,
        callbacks=[callback_handler]
    )
    
    # Register the chain
    inspector.register_chain(
        "loan_application",
        loan_chain,
        "Processes loan applications and provides preliminary decisions"
    )
    
    # Chain for transaction history
    transaction_prompt = PromptTemplate(
        input_variables=["customer_id", "account_type", "time_period"],
        template="""
        You are a banking assistant. Provide the transaction history for the requested account and time period.
        
        Customer ID: {customer_id}
        Account Type: {account_type}
        Time Period: {time_period}
        
        Transaction History:
        """
    )
    
    transaction_chain = LLMChain(
        llm=llm,
        prompt=transaction_prompt,
        callbacks=[callback_handler]
    )
    
    # Register the chain
    inspector.register_chain(
        "transaction_history",
        transaction_chain,
        "Provides transaction history for customer accounts"
    )
    
    # Create a sequential chain for customer onboarding
    onboarding_prompt1 = PromptTemplate(
        input_variables=["customer_name", "customer_id"],
        template="""
        You are a banking assistant. Welcome the new customer and provide initial account information.
        
        Customer Name: {customer_name}
        Customer ID: {customer_id}
        
        Welcome Message:
        """
    )
    
    onboarding_chain1 = LLMChain(
        llm=llm,
        prompt=onboarding_prompt1,
        output_key="welcome_message",
        callbacks=[callback_handler]
    )
    
    onboarding_prompt2 = PromptTemplate(
        input_variables=["welcome_message", "account_type"],
        template="""
        You are a banking assistant. Based on the welcome message, provide information about the requested account type.
        
        Welcome Message: {welcome_message}
        Account Type: {account_type}
        
        Account Information:
        """
    )
    
    onboarding_chain2 = LLMChain(
        llm=llm,
        prompt=onboarding_prompt2,
        output_key="account_info",
        callbacks=[callback_handler]
    )
    
    onboarding_chain = SequentialChain(
        chains=[onboarding_chain1, onboarding_chain2],
        input_variables=["customer_name", "customer_id", "account_type"],
        output_variables=["welcome_message", "account_info"],
        callbacks=[callback_handler]
    )
    
    # Register the chain
    inspector.register_chain(
        "customer_onboarding",
        onboarding_chain,
        "Handles new customer onboarding process"
    )
    
    # Run the chains
    print("Running balance inquiry chain...")
    balance_result = balance_chain.run(
        customer_id="CUST12345",
        account_type="Checking"
    )
    print(f"Balance inquiry result: {balance_result[:100]}...")
    
    print("\nRunning loan application chain...")
    loan_result = loan_chain.run(
        customer_id="CUST67890",
        loan_type="Mortgage",
        loan_amount="350000"
    )
    print(f"Loan application result: {loan_result[:100]}...")
    
    print("\nRunning transaction history chain...")
    transaction_result = transaction_chain.run(
        customer_id="CUST12345",
        account_type="Checking",
        time_period="Last 30 days"
    )
    print(f"Transaction history result: {transaction_result[:100]}...")
    
    print("\nRunning customer onboarding chain...")
    onboarding_result = onboarding_chain({
        "customer_name": "John Smith",
        "customer_id": "CUST54321",
        "account_type": "Savings"
    })
    print(f"Welcome message: {onboarding_result['welcome_message'][:100]}...")
    print(f"Account info: {onboarding_result['account_info'][:100]}...")
    
    # Analyze execution history
    print("\nExecution History:")
    for execution in inspector.get_execution_history():
        print(f"  {execution.execution_id}: {execution.chain_name} "
              f"(duration: {execution.duration:.2f}s, success: {execution.success})")
    
    # Analyze specific execution
    if inspector.execution_history:
        execution_id = inspector.execution_history[0].execution_id
        analysis = inspector.analyze_execution(execution_id)
        
        print(f"\nExecution Analysis for {execution_id}:")
        print(f"  Chain: {analysis['chain_name']}")
        print(f"  Duration: {analysis['duration']:.2f}s")
        print(f"  Success: {analysis['success']}")
        print(f"  Step count: {analysis['step_count']}")
        print(f"  Average step duration: {analysis['average_step_duration']:.2f}s")
        
        # Analyze chain performance
        chain_name = "balance_inquiry"
        performance = inspector.analyze_chain_performance(chain_name)
        
        print(f"\nPerformance Analysis for {chain_name}:")
        print(f"  Execution count: {performance['execution_count']}")
        print(f"  Success rate: {performance['success_rate']:.2%}")
        print(f"  Average duration: {performance['average_duration']:.2f}s")
        
        # Visualize execution flow (in a real environment, this would display the graph)
        if inspector.execution_history:
            execution_id = inspector.execution_history[0].execution_id
            flow_graph = inspector.visualize_execution_flow(execution_id)
            if flow_graph:
                # In a real environment, this would render the graph
                print(f"\nExecution flow graph created for {execution_id}")
                
                # Save to file
                flow_graph.render(f"execution_flow_{execution_id}", view=False, format="png")
                print(f"Execution flow graph saved as execution_flow_{execution_id}.png")
        
        # Visualize step durations (in a real environment, this would display the chart)
        if inspector.execution_history:
            execution_id = inspector.execution_history[0].execution_id
            print(f"\nStep duration chart created for {execution_id}")
            # In a real environment, this would display the chart
            # inspector.visualize_step_durations(execution_id)
    
    # Compare executions
    if len(inspector.execution_history) >= 2:
        execution_ids = [exec.execution_id for exec in inspector.execution_history[:2]]
        comparison = inspector.compare_executions(execution_ids)
        
        print(f"\nExecution Comparison:")
        print(f"  Execution count: {comparison['execution_count']}")
        if "average_duration" in comparison:
            print(f"  Average duration: {comparison['average_duration']:.2f}s")
        
        for exec_data in comparison["executions"]:
            print(f"  {exec_data['execution_id']}: {exec_data['chain_name']} "
                  f"(duration: {exec_data['duration']:.2f}s, success: {exec_data['success']})")

# Run the chain inspection example
chain_inspection_example()</code></pre>
                            
                            <p>In a production banking environment, this chain inspection implementation would be extended with:</p>
                            <ul>
                                <li>Integration with the bank's monitoring and alerting systems to notify teams of performance issues or errors</li>
                                <li>Compliance logging to meet financial regulatory requirements for AI system transparency</li>
                                <li>Integration with A/B testing frameworks to compare different chain configurations</li>
                                <li>Advanced analytics to identify patterns in customer interactions and system performance</li>
                                <li>Automated optimization recommendations based on inspection data</li>
                            </ul>
                        </div>
                    </div>
                </section>
                
                <!-- Section 10.3 -->
                <section id="section-103" class="section-card">
                    <h2>10.3 Quality Assurance</h2>
                    
                    <!-- Component 10.3.1 -->
                    <div class="component-card">
                        <h3 class="component-title">10.3.1 Testing</h3>
                        
                        <div class="info-section">
                            <div class="info-title business-case">
                                <i class="fas fa-check-circle"></i> Business Use Case
                            </div>
                            <p>In the healthcare industry, comprehensive testing of AI-powered diagnostic systems is critical to ensure patient safety and regulatory compliance. A hospital network implemented a rigorous testing framework for their AI system that analyzes medical images and patient data to detect potential conditions. This testing framework included unit tests for individual components, integration tests for data pipelines, and scenario-based tests for complex diagnostic cases. The implementation identified and resolved 37 critical issues before deployment, including false positives in cancer detection and incorrect medication interaction warnings, ultimately improving diagnostic accuracy by 28% and reducing false alarms by 45%.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title problems-solved">
                                <i class="fas fa-lightbulb"></i> Problems Solved
                            </div>
                            <p>Testing solves the fundamental challenge of ensuring AI systems behave correctly, reliably, and safely in production environments. Without comprehensive testing, businesses risk deploying systems that produce incorrect outputs, fail under specific conditions, or behave unpredictably. Testing provides confidence that AI applications will meet business requirements, handle edge cases appropriately, maintain performance under load, and comply with regulatory standards.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title impact-absent">
                                <i class="fas fa-exclamation-triangle"></i> Impact if Absent
                            </div>
                            <p>Without testing, businesses would face significant risks when deploying AI systems. In a banking context, this could mean virtual assistants providing incorrect financial advice, loan approval systems making biased decisions, or fraud detection algorithms missing critical patterns. This would result in financial losses, regulatory penalties, reputational damage, and erosion of customer trust. The cost of fixing issues after deployment is estimated to be 5-10x higher than addressing them during development, making testing an essential investment in quality and risk management.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title predefined-custom">
                                <i class="fas fa-cogs"></i> Predefined or Custom
                            </div>
                            <p>Custom</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title relationship">
                                <i class="fas fa-link"></i> Relationship Keyword
                            </div>
                            <p>Control</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title technical-insight">
                                <i class="fas fa-microchip"></i> In-depth Technical Insight
                            </div>
                            <p>Testing in LangChain applications involves verifying the correctness, reliability, and performance of chains, agents, and components. This includes unit tests for individual components, integration tests for interactions between components, and end-to-end tests for complete workflows. Effective testing requires strategies for mocking external dependencies, managing test data, and validating outputs against expected results.</p>
                            
                            <pre><code>import unittest
from unittest.mock import Mock, patch
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.chains import LLMChain, SequentialChain, TransformChain
from langchain.prompts import PromptTemplate
from typing import Dict, List, Any, Optional, Union, Callable
import json
import time
from datetime import datetime
from dataclasses import dataclass, field
import uuid
import logging
from enum import Enum
import pytest
import pandas as pd
import numpy as np
from io import StringIO
import sys
import os
import tempfile
import shutil

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TestCase:
    """Represents a test case with inputs and expected outputs"""
    name: str
    inputs: Dict[str, Any]
    expected_outputs: Dict[str, Any]
    description: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)
    timeout: float = 30.0

@dataclass
class TestResult:
    """Represents the result of a test case execution"""
    test_case: TestCase
    actual_outputs: Dict[str, Any]
    success: bool
    execution_time: float
    error_message: Optional[str] = None
    diff: Optional[str] = None

class TestRunner:
    """Runs test cases and collects results"""
    
    def __init__(self):
        self.results = []
        self.setup_functions = []
        self.teardown_functions = []
    
    def add_setup(self, func: Callable) -> None:
        """Add a setup function to be run before each test"""
        self.setup_functions.append(func)
    
    def add_teardown(self, func: Callable) -> None:
        """Add a teardown function to be run after each test"""
        self.teardown_functions.append(func)
    
    def run_test(self, test_case: TestCase, test_func: Callable) -> TestResult:
        """Run a single test case"""
        start_time = time.time()
        
        # Run setup functions
        for setup_func in self.setup_functions:
            try:
                setup_func()
            except Exception as e:
                return TestResult(
                    test_case=test_case,
                    actual_outputs={},
                    success=False,
                    execution_time=time.time() - start_time,
                    error_message=f"Setup failed: {str(e)}"
                )
        
        # Run the test function with timeout
        try:
            # Run with timeout
            actual_outputs = self._run_with_timeout(test_func, test_case.inputs, test_case.timeout)
            
            # Compare with expected outputs
            success, diff = self._compare_outputs(actual_outputs, test_case.expected_outputs)
            
            result = TestResult(
                test_case=test_case,
                actual_outputs=actual_outputs,
                success=success,
                execution_time=time.time() - start_time,
                diff=diff
            )
            
        except Exception as e:
            result = TestResult(
                test_case=test_case,
                actual_outputs={},
                success=False,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
        
        # Run teardown functions
        for teardown_func in self.teardown_functions:
            try:
                teardown_func()
            except Exception as e:
                logger.error(f"Teardown failed: {str(e)}")
        
        return result
    
    def _run_with_timeout(self, func: Callable, inputs: Dict[str, Any], timeout: float) -> Dict[str, Any]:
        """Run a function with a timeout"""
        # For simplicity, we'll use a basic approach
        # In a real system, you might use more sophisticated timeout handling
        
        # Create a new event loop for async functions
        if asyncio.iscoroutinefunction(func):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(
                    asyncio.wait_for(func(**inputs), timeout=timeout)
                )
            finally:
                loop.close()
        else:
            # For sync functions, just call directly
            # In a real system, you might use threading or signal-based timeout
            return func(**inputs)
    
    def _compare_outputs(self, actual: Dict[str, Any], expected: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """Compare actual outputs with expected outputs"""
        # For simplicity, we'll use a basic comparison
        # In a real system, you might use more sophisticated comparison logic
        
        if set(actual.keys()) != set(expected.keys()):
            diff = f"Key mismatch: actual={list(actual.keys())}, expected={list(expected.keys())}"
            return False, diff
        
        for key in expected:
            if key not in actual:
                diff = f"Missing key in actual output: {key}"
                return False, diff
            
            # Handle different comparison strategies based on type
            if isinstance(expected[key], str) and isinstance(actual[key], str):
                # For strings, use a flexible comparison
                if expected[key].lower() not in actual[key].lower():
                    diff = f"String mismatch for key '{key}': expected='{expected[key]}', actual='{actual[key]}'"
                    return False, diff
            elif isinstance(expected[key], (int, float)) and isinstance(actual[key], (int, float)):
                # For numbers, use a tolerance-based comparison
                tolerance = 0.01  # 1% tolerance
                if abs(actual[key] - expected[key]) > abs(expected[key] * tolerance):
                    diff = f"Number mismatch for key '{key}': expected={expected[key]}, actual={actual[key]}"
                    return False, diff
            elif isinstance(expected[key], dict) and isinstance(actual[key], dict):
                # For dictionaries, recursive comparison
                success, diff = self._compare_outputs(actual[key], expected[key])
                if not success:
                    return False, f"Dictionary mismatch for key '{key}': {diff}"
            elif isinstance(expected[key], list) and isinstance(actual[key], list):
                # For lists, compare elements
                if len(expected[key]) != len(actual[key]):
                    diff = f"List length mismatch for key '{key}': expected={len(expected[key])}, actual={len(actual[key])}"
                    return False, diff
                
                for i, (exp_item, act_item) in enumerate(zip(expected[key], actual[key])):
                    if isinstance(exp_item, dict) and isinstance(act_item, dict):
                        success, diff = self._compare_outputs(act_item, exp_item)
                        if not success:
                            return False, f"List element mismatch at index {i} for key '{key}': {diff}"
                    elif exp_item != act_item:
                        diff = f"List element mismatch at index {i} for key '{key}': expected={exp_item}, actual={act_item}"
                        return False, diff
            elif expected[key] != actual[key]:
                diff = f"Value mismatch for key '{key}': expected={expected[key]}, actual={actual[key]}"
                return False, diff
        
        return True, None
    
    def run_tests(self, test_cases: List[TestCase], test_func: Callable) -> List[TestResult]:
        """Run multiple test cases"""
        results = []
        for test_case in test_cases:
            result = self.run_test(test_case, test_func)
            results.append(result)
            self.results.append(result)
        return results
    
    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of test results"""
        if not self.results:
            return {"total": 0, "passed": 0, "failed": 0, "pass_rate": 0}
        
        total = len(self.results)
        passed = sum(1 for result in self.results if result.success)
        failed = total - passed
        
        return {
            "total": total,
            "passed": passed,
            "failed": failed,
            "pass_rate": passed / total if total > 0 else 0,
            "average_execution_time": sum(result.execution_time for result in self.results) / total
        }

class LangChainTester:
    """Tester for LangChain applications"""
    
    def __init__(self):
        self.test_runner = TestRunner()
        self.test_cases = {}
        self.mocks = {}
    
    def add_test_case(self, category: str, test_case: TestCase) -> None:
        """Add a test case to a category"""
        if category not in self.test_cases:
            self.test_cases[category] = []
        self.test_cases[category].append(test_case)
    
    def add_mock(self, name: str, mock_obj: Any) -> None:
        """Add a mock object for testing"""
        self.mocks[name] = mock_obj
    
    def run_unit_tests(self) -> Dict[str, Any]:
        """Run unit tests for individual components"""
        results = {}
        
        for category, test_cases in self.test_cases.items():
            if category.startswith("unit_"):
                # Determine the test function based on the category
                test_func = self._get_test_function(category)
                
                if test_func:
                    test_results = self.test_runner.run_tests(test_cases, test_func)
                    results[category] = {
                        "test_results": test_results,
                        "summary": self._summarize_results(test_results)
                    }
        
        return results
    
    def run_integration_tests(self) -> Dict[str, Any]:
        """Run integration tests for component interactions"""
        results = {}
        
        for category, test_cases in self.test_cases.items():
            if category.startswith("integration_"):
                # Determine the test function based on the category
                test_func = self._get_test_function(category)
                
                if test_func:
                    test_results = self.test_runner.run_tests(test_cases, test_func)
                    results[category] = {
                        "test_results": test_results,
                        "summary": self._summarize_results(test_results)
                    }
        
        return results
    
    def run_end_to_end_tests(self) -> Dict[str, Any]:
        """Run end-to-end tests for complete workflows"""
        results = {}
        
        for category, test_cases in self.test_cases.items():
            if category.startswith("e2e_"):
                # Determine the test function based on the category
                test_func = self._get_test_function(category)
                
                if test_func:
                    test_results = self.test_runner.run_tests(test_cases, test_func)
                    results[category] = {
                        "test_results": test_results,
                        "summary": self._summarize_results(test_results)
                    }
        
        return results
    
    def run_all_tests(self) -> Dict[str, Any]:
        """Run all tests"""
        unit_results = self.run_unit_tests()
        integration_results = self.run_integration_tests()
        e2e_results = self.run_end_to_end_tests()
        
        # Combine all results
        all_results = {**unit_results, **integration_results, **e2e_results}
        
        # Calculate overall summary
        total_passed = 0
        total_failed = 0
        total_tests = 0
        
        for category_data in all_results.values():
            summary = category_data["summary"]
            total_passed += summary["passed"]
            total_failed += summary["failed"]
            total_tests += summary["total"]
        
        overall_summary = {
            "total": total_tests,
            "passed": total_passed,
            "failed": total_failed,
            "pass_rate": total_passed / total_tests if total_tests > 0 else 0
        }
        
        return {
            "results": all_results,
            "overall_summary": overall_summary
        }
    
    def _get_test_function(self, category: str) -> Optional[Callable]:
        """Get the test function for a category"""
        # This is a simplified approach
        # In a real system, you might have a more sophisticated mapping
        
        if category == "unit_llm_chain":
            return self._test_llm_chain
        elif category == "unit_transform_chain":
            return self._test_transform_chain
        elif category == "unit_memory":
            return self._test_memory
        elif category == "integration_sequential_chain":
            return self._test_sequential_chain
        elif category == "integration_agent":
            return self._test_agent
        elif category == "e2e_banking_assistant":
            return self._test_banking_assistant
        elif category == "e2e_healthcare_diagnostic":
            return self._test_healthcare_diagnostic
        else:
            logger.warning(f"No test function found for category: {category}")
            return None
    
    def _summarize_results(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """Summarize test results"""
        if not test_results:
            return {"total": 0, "passed": 0, "failed": 0, "pass_rate": 0}
        
        total = len(test_results)
        passed = sum(1 for result in test_results if result.success)
        failed = total - passed
        
        return {
            "total": total,
            "passed": passed,
            "failed": failed,
            "pass_rate": passed / total if total > 0 else 0,
            "average_execution_time": sum(result.execution_time for result in test_results) / total
        }
    
    def _test_llm_chain(self, **inputs) -> Dict[str, Any]:
        """Test an LLM chain"""
        # Create a mock LLM
        mock_llm = Mock()
        mock_llm.invoke.return_value = AIMessage(content="Mock response from LLM")
        
        # Create a prompt template
        prompt_template = PromptTemplate(
            input_variables=["question"],
            template="Answer the question: {question}"
        )
        
        # Create the chain
        chain = LLMChain(llm=mock_llm, prompt=prompt_template)
        
        # Run the chain
        result = chain.run(question=inputs.get("question", "What is AI?"))
        
        return {"response": result}
    
    def _test_transform_chain(self, **inputs) -> Dict[str, Any]:
        """Test a transform chain"""
        # Define a transform function
        def uppercase_text(inputs: Dict[str, Any]) -> Dict[str, Any]:
            text = inputs.get("text", "")
            return {"uppercase_text": text.upper()}
        
        # Create the transform chain
        chain = TransformChain(
            transform=uppercase_text,
            input_variables=["text"],
            output_variables=["uppercase_text"]
        )
        
        # Run the chain
        result = chain(text=inputs.get("text", "Hello, World!"))
        
        return {"uppercase_text": result["uppercase_text"]}
    
    def _test_memory(self, **inputs) -> Dict[str, Any]:
        """Test conversation memory"""
        # Create a memory object
        memory = ConversationBufferMemory()
        
        # Add some messages
        memory.chat_memory.add_user_message(inputs.get("user_message", "Hello"))
        memory.chat_memory.add_ai_message(inputs.get("ai_message", "Hi there!"))
        
        # Get the buffer
        buffer = memory.load_memory_variables({})
        
        return {"buffer": buffer}
    
    def _test_sequential_chain(self, **inputs) -> Dict[str, Any]:
        """Test a sequential chain"""
        # Create mock LLM
        mock_llm = Mock()
        mock_llm.invoke.return_value = AIMessage(content="Mock response")
        
        # Create first chain
        first_prompt = PromptTemplate(
            input_variables=["topic"],
            template="Write a sentence about {topic}"
        )
        first_chain = LLMChain(llm=mock_llm, prompt=first_prompt, output_key="sentence")
        
        # Create second chain
        second_prompt = PromptTemplate(
            input_variables=["sentence"],
            template="Translate this sentence to French: {sentence}"
        )
        second_chain = LLMChain(llm=mock_llm, prompt=second_prompt, output_key="translation")
        
        # Create sequential chain
        sequential_chain = SequentialChain(
            chains=[first_chain, second_chain],
            input_variables=["topic"],
            output_variables=["sentence", "translation"]
        )
        
        # Run the chain
        result = sequential_chain(topic=inputs.get("topic", "AI"))
        
        return {
            "sentence": result["sentence"],
            "translation": result["translation"]
        }
    
    def _test_agent(self, **inputs) -> Dict[str, Any]:
        """Test an agent"""
        # Create mock tools
        def search_api(query: str) -> str:
            return f"Search results for: {query}"
        
        def calculator(expression: str) -> str:
            try:
                return str(eval(expression))
            except:
                return "Error: Invalid expression"
        
        tools = [
            Tool(
                name="Search",
                func=search_api,
                description="Useful for searching the internet"
            ),
            Tool(
                name="Calculator",
                func=calculator,
                description="Useful for performing mathematical calculations"
            )
        ]
        
        # Create mock LLM
        mock_llm = Mock()
        mock_llm.invoke.return_value = AIMessage(
            content="I need to use the Search tool to answer this question."
        )
        
        # Create agent
        agent = initialize_agent(
            tools,
            mock_llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True
        )
        
        # Run the agent
        result = agent.run(inputs.get("question", "What is the capital of France?"))
        
        return {"response": result}
    
    def _test_banking_assistant(self, **inputs) -> Dict[str, Any]:
        """Test a banking assistant end-to-end"""
        # Create mock LLM
        mock_llm = Mock()
        mock_llm.invoke.return_value = AIMessage(
            content="Your account balance is $5,000. Would you like to know about our investment options?"
        )
        
        # Create prompt template
        prompt_template = PromptTemplate(
            input_variables=["customer_id", "inquiry"],
            template="""
            You are a banking assistant. Answer the customer's inquiry.
            
            Customer ID: {customer_id}
            Inquiry: {inquiry}
            
            Response:
            """
        )
        
        # Create chain
        chain = LLMChain(llm=mock_llm, prompt=prompt_template)
        
        # Run the chain
        result = chain.run(
            customer_id=inputs.get("customer_id", "CUST12345"),
            inquiry=inputs.get("inquiry", "What is my account balance?")
        )
        
        return {"response": result}
    
    def _test_healthcare_diagnostic(self, **inputs) -> Dict[str, Any]:
        """Test a healthcare diagnostic system end-to-end"""
        # Create mock LLM
        mock_llm = Mock()
        mock_llm.invoke.return_value = AIMessage(
            content="Based on the symptoms provided, the patient may have influenza. Recommend rest, fluids, and over-the-counter fever reducers."
        )
        
        # Create prompt template
        prompt_template = PromptTemplate(
            input_variables=["patient_id", "symptoms"],
            template="""
            You are a healthcare AI assistant. Analyze the patient's symptoms and provide a preliminary diagnosis.
            
            Patient ID: {patient_id}
            Symptoms: {symptoms}
            
            Diagnosis:
            """
        )
        
        # Create chain
        chain = LLMChain(llm=mock_llm, prompt=prompt_template)
        
        # Run the chain
        result = chain.run(
            patient_id=inputs.get("patient_id", "PAT12345"),
            symptoms=inputs.get("symptoms", "Fever, cough, body aches")
        )
        
        return {"diagnosis": result}

# Example usage in a healthcare context
def testing_example():
    """Example of testing a healthcare diagnostic system"""
    # Create tester
    tester = LangChainTester()
    
    # Add unit test cases for LLM chains
    tester.add_test_case("unit_llm_chain", TestCase(
        name="Basic question answering",
        inputs={"question": "What is AI?"},
        expected_outputs={"response": "Mock response from LLM"},
        description="Test basic question answering functionality"
    ))
    
    tester.add_test_case("unit_llm_chain", TestCase(
        name="Medical terminology question",
        inputs={"question": "What is myocardial infarction?"},
        expected_outputs={"response": "Mock response from LLM"},
        description="Test handling of medical terminology"
    ))
    
    # Add unit test cases for transform chains
    tester.add_test_case("unit_transform_chain", TestCase(
        name="Text transformation",
        inputs={"text": "patient symptoms"},
        expected_outputs={"uppercase_text": "PATIENT SYMPTOMS"},
        description="Test text transformation functionality"
    ))
    
    # Add unit test cases for memory
    tester.add_test_case("unit_memory", TestCase(
        name="Conversation memory",
        inputs={
            "user_message": "I have a headache",
            "ai_message": "How long have you had the headache?"
        },
        expected_outputs={
            "buffer": {
                "history": "Human: I have a headache\nAI: How long have you had the headache?"
            }
        },
        description="Test conversation memory functionality"
    ))
    
    # Add integration test cases for sequential chains
    tester.add_test_case("integration_sequential_chain", TestCase(
        name="Diagnosis and treatment chain",
        inputs={"topic": "headache"},
        expected_outputs={
            "sentence": "Mock response",
            "translation": "Mock response"
        },
        description="Test sequential processing of diagnosis and treatment"
    ))
    
    # Add integration test cases for agents
    tester.add_test_case("integration_agent", TestCase(
        name="Medical research agent",
        inputs={"question": "What are the treatments for migraine?"},
        expected_outputs={"response": "I need to use the Search tool to answer this question."},
        description="Test agent's ability to use tools for medical research"
    ))
    
    # Add end-to-end test cases for healthcare diagnostic system
    tester.add_test_case("e2e_healthcare_diagnostic", TestCase(
        name="Basic diagnostic workflow",
        inputs={
            "patient_id": "PAT12345",
            "symptoms": "Fever, cough, fatigue"
        },
        expected_outputs={
            "diagnosis": "Based on the symptoms provided, the patient may have influenza. Recommend rest, fluids, and over-the-counter fever reducers."
        },
        description="Test end-to-end diagnostic workflow for common symptoms"
    ))
    
    tester.add_test_case("e2e_healthcare_diagnostic", TestCase(
        name="Emergency condition detection",
        inputs={
            "patient_id": "PAT67890",
            "symptoms": "Chest pain, shortness of breath, left arm pain"
        },
        expected_outputs={
            "diagnosis": "Based on the symptoms provided, the patient may have influenza. Recommend rest, fluids, and over-the-counter fever reducers."
        },
        description="Test end-to-end diagnostic workflow for emergency symptoms"
    ))
    
    # Run unit tests
    print("Running unit tests...")
    unit_results = tester.run_unit_tests()
    
    # Print unit test results
    for category, data in unit_results.items():
        print(f"\n{category}:")
        summary = data["summary"]
        print(f"  Total: {summary['total']}, Passed: {summary['passed']}, Failed: {summary['failed']}, Pass Rate: {summary['pass_rate']:.2%}")
        
        for result in data["test_results"]:
            status = "PASS" if result.success else "FAIL"
            print(f"  {status}: {result.test_case.name} ({result.execution_time:.2f}s)")
            if not result.success:
                print(f"    Error: {result.error_message}")
    
    # Run integration tests
    print("\nRunning integration tests...")
    integration_results = tester.run_integration_tests()
    
    # Print integration test results
    for category, data in integration_results.items():
        print(f"\n{category}:")
        summary = data["summary"]
        print(f"  Total: {summary['total']}, Passed: {summary['passed']}, Failed: {summary['failed']}, Pass Rate: {summary['pass_rate']:.2%}")
        
        for result in data["test_results"]:
            status = "PASS" if result.success else "FAIL"
            print(f"  {status}: {result.test_case.name} ({result.execution_time:.2f}s)")
            if not result.success:
                print(f"    Error: {result.error_message}")
    
    # Run end-to-end tests
    print("\nRunning end-to-end tests...")
    e2e_results = tester.run_end_to_end_tests()
    
    # Print end-to-end test results
    for category, data in e2e_results.items():
        print(f"\n{category}:")
        summary = data["summary"]
        print(f"  Total: {summary['total']}, Passed: {summary['passed']}, Failed: {summary['failed']}, Pass Rate: {summary['pass_rate']:.2%}")
        
        for result in data["test_results"]:
            status = "PASS" if result.success else "FAIL"
            print(f"  {status}: {result.test_case.name} ({result.execution_time:.2f}s)")
            if not result.success:
                print(f"    Error: {result.error_message}")
    
    # Run all tests and get overall summary
    print("\nRunning all tests...")
    all_results = tester.run_all_tests()
    
    # Print overall summary
    overall_summary = all_results["overall_summary"]
    print(f"\nOverall Test Summary:")
    print(f"  Total: {overall_summary['total']}, Passed: {overall_summary['passed']}, Failed: {overall_summary['failed']}, Pass Rate: {overall_summary['pass_rate']:.2%}")
    
    # Generate test report
    report = {
        "timestamp": datetime.now().isoformat(),
        "overall_summary": overall_summary,
        "results": all_results["results"]
    }
    
    # Save report to file
    with open("test_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"\nTest report saved to test_report.json")

# Run the testing example
testing_example()</code></pre>
                            
                            <p>In a production healthcare environment, this testing implementation would be extended with:</p>
                            <ul>
                                <li>Integration with Electronic Health Record (EHR) test environments to validate against realistic patient data</li>
                                <li>Compliance testing to ensure adherence to healthcare regulations like HIPAA</li>
                                <li>Performance testing under load to simulate peak usage periods</li>
                                <li>Continuous integration testing to automatically validate changes before deployment</li>
                                <li>Test data generation that respects patient privacy while maintaining realistic scenarios</li>
                            </ul>
                        </div>
                    </div>
                    
                    <!-- Component 10.3.2 -->
                    <div class="component-card">
                        <h3 class="component-title">10.3.2 Evaluation</h3>
                        
                        <div class="info-section">
                            <div class="info-title business-case">
                                <i class="fas fa-check-circle"></i> Business Use Case
                            </div>
                            <p>In the banking industry, evaluation of AI-powered financial advisory systems is essential to ensure accuracy, compliance, and customer satisfaction. A leading retail bank implemented a comprehensive evaluation framework for their virtual financial advisor that provides investment recommendations. The framework evaluated the system against multiple criteria including financial accuracy, regulatory compliance, risk assessment appropriateness, and customer satisfaction. This evaluation process identified that the system was overly conservative in its recommendations, leading to suboptimal returns for customers. After optimization based on evaluation insights, the bank saw a 15% increase in customer portfolio performance while maintaining compliance standards and improving customer satisfaction scores by 20%.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title problems-solved">
                                <i class="fas fa-lightbulb"></i> Problems Solved
                            </div>
                            <p>Evaluation solves the challenge of objectively measuring the performance, quality, and effectiveness of AI systems. While testing verifies that systems function correctly, evaluation assesses how well they meet business objectives, user needs, and quality standards. This includes measuring accuracy, relevance, coherence, bias, safety, and other quality metrics that are critical for business success and user satisfaction.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title impact-absent">
                                <i class="fas fa-exclamation-triangle"></i> Impact if Absent
                            </div>
                            <p>Without evaluation, businesses would lack objective measures of their AI systems' effectiveness, leading to suboptimal performance and missed improvement opportunities. In a logistics context, this would mean inability to assess whether AI-powered route optimization systems are actually reducing fuel costs and delivery times, or if customer service chatbots are effectively resolving issues. This would result in continued investment in underperforming systems, inability to justify AI investments to stakeholders, and missed opportunities for optimization that could lead to significant cost savings and service improvements.</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title predefined-custom">
                                <i class="fas fa-cogs"></i> Predefined or Custom
                            </div>
                            <p>Custom</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title relationship">
                                <i class="fas fa-link"></i> Relationship Keyword
                            </div>
                            <p>Control</p>
                        </div>
                        
                        <div class="info-section">
                            <div class="info-title technical-insight">
                                <i class="fas fa-microchip"></i> In-depth Technical Insight
                            </div>
                            <p>Evaluation in LangChain involves measuring the performance and quality of chains, agents, and components against predefined metrics and benchmarks. This includes automated evaluation using reference answers, human evaluation for subjective quality, and continuous evaluation to monitor performance over time. Effective evaluation requires defining appropriate metrics, creating evaluation datasets, and implementing evaluation pipelines.</p>
                            
                            <pre><code>import json
import time
import statistics
from typing import Dict, List, Any, Optional, Union, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
import logging
import numpy as np
import pandas as pd
from datetime import datetime
import openai
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import defaultdict, Counter
import asyncio
import aiohttp

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EvaluationMetric(Enum):
    """Types of evaluation metrics"""
    ACCURACY = "accuracy"
    PRECISION = "precision"
    RECALL = "recall"
    F1_SCORE = "f1_score"
    ROUGE = "rouge"
    BLEU = "bleu"
    BERT_SCORE = "bert_score"
    SEMANTIC_SIMILARITY = "semantic_similarity"
    RELEVANCE = "relevance"
    COHERENCE = "coherence"
    HELPFULNESS = "helpfulness"
    SAFETY = "safety"
    BIAS = "bias"
    CUSTOM = "custom"

@dataclass
class EvaluationExample:
    """Represents a single example for evaluation"""
    id: str
    inputs: Dict[str, Any]
    reference_outputs: Optional[Dict[str, Any]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class EvaluationResult:
    """Represents the result of an evaluation"""
    example_id: str
    actual_outputs: Dict[str, Any]
    metrics: Dict[str, float]
    execution_time: float
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class EvaluationReport:
    """Represents a complete evaluation report"""
    evaluation_id: str
    model_name: str
    timestamp: datetime
    metrics_summary: Dict[str, Dict[str, float]]
    results: List[EvaluationResult]
    examples: List[EvaluationExample]
    metadata: Dict[str, Any] = field(default_factory=dict)

class Evaluator:
    """Base class for evaluators"""
    
    def __init__(self, name: str):
        self.name = name
    
    def evaluate(self, actual_outputs: Dict[str, Any], reference_outputs: Optional[Dict[str, Any]] = None, 
                inputs: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """Evaluate outputs and return metrics"""
        raise NotImplementedError

class AccuracyEvaluator(Evaluator):
    """Evaluator for accuracy metrics"""
    
    def __init__(self):
        super().__init__("accuracy")
    
    def evaluate(self, actual_outputs: Dict[str, Any], reference_outputs: Optional[Dict[str, Any]] = None, 
                inputs: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """Evaluate accuracy of outputs"""
        if reference_outputs is None:
            return {"accuracy": 0.0}
        
        # For simplicity, we'll compare string outputs
        # In a real system, you might have more sophisticated comparison logic
        actual_text = actual_outputs.get("text", actual_outputs.get("output", ""))
        reference_text = reference_outputs.get("text", reference_outputs.get("output", ""))
        
        # Simple exact match accuracy
        exact_match = 1.0 if actual_text.strip() == reference_text.strip() else 0.0
        
        # Partial match accuracy (based on word overlap)
        actual_words = set(actual_text.lower().split())
        reference_words = set(reference_text.lower().split())
        
        if not reference_words:
            partial_match = 0.0
        else:
            intersection = actual_words.intersection(reference_words)
            partial_match = len(intersection) / len(reference_words)
        
        return {
            "exact_match_accuracy": exact_match,
            "partial_match_accuracy": partial_match
        }

class SemanticSimilarityEvaluator(Evaluator):
    """Evaluator for semantic similarity"""
    
    def __init__(self, model_name: str = "text-embedding-ada-002"):
        super().__init__("semantic_similarity")
        self.model_name = model_name
        self.client = openai.OpenAI()  # Requires API key in environment
    
    def evaluate(self, actual_outputs: Dict[str, Any], reference_outputs: Optional[Dict[str, Any]] = None, 
                inputs: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """Evaluate semantic similarity"""
        if reference_outputs is None:
            return {"semantic_similarity": 0.0}
        
        # Get texts
        actual_text = actual_outputs.get("text", actual_outputs.get("output", ""))
        reference_text = reference_outputs.get("text", reference_outputs.get("output", ""))
        
        if not actual_text or not reference_text:
            return {"semantic_similarity": 0.0}
        
        try:
            # Get embeddings
            actual_embedding = self._get_embedding(actual_text)
            reference_embedding = self._get_embedding(reference_text)
            
            # Calculate cosine similarity
            similarity = self._cosine_similarity(actual_embedding, reference_embedding)
            
            return {"semantic_similarity": similarity}
        
        except Exception as e:
            logger.error(f"Error calculating semantic similarity: {str(e)}")
            return {"semantic_similarity": 0.0}
    
    def _get_embedding(self, text: str) -> List[float]:
        """Get embedding for text"""
        response = self.client.embeddings.create(
            input=text,
            model=self.model_name
        )
        return response.data[0].embedding
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calculate cosine similarity between two vectors"""
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = sum(a * a for a in vec1) ** 0.5
        magnitude2 = sum(b * b for b in vec2) ** 0.5
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)

class RelevanceEvaluator(Evaluator):
    """Evaluator for relevance using LLM-as-a-judge"""
    
    def __init__(self, model_name: str = "gpt-4"):
        super().__init__("relevance")
        self.model_name = model_name
        self.llm = ChatOpenAI(model=model_name, temperature=0.1)
    
    def evaluate(self, actual_outputs: Dict[str, Any], reference_outputs: Optional[Dict[str, Any]] = None, 
                inputs: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """Evaluate relevance of outputs"""
        if inputs is None:
            return {"relevance": 0.0}
        
        # Get texts
        input_text = inputs.get("input", inputs.get("question", inputs.get("prompt", "")))
        actual_text = actual_outputs.get("text", actual_outputs.get("output", ""))
        
        if not input_text or not actual_text:
            return {"relevance": 0.0}
        
        try:
            # Create evaluation prompt
            prompt = f"""
            You are an expert evaluator. Your task is to evaluate how relevant the following response is to the given input.
            
            Input: {input_text}
            
            Response: {actual_text}
            
            Please rate the relevance on a scale from 0 to 1, where:
            - 0 means completely irrelevant
            - 1 means perfectly relevant
            
            Provide only the numerical rating as your response.
            """
            
            # Get evaluation
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            # Extract rating
            rating_text = response.content.strip()
            rating_match = re.search(r'(\d+\.?\d*)', rating_text)
            
            if rating_match:
                rating = float(rating_match.group(1))
                # Ensure rating is between 0 and 1
                rating = max(0.0, min(1.0, rating))
            else:
                rating = 0.0
            
            return {"relevance": rating}
        
        except Exception as e:
            logger.error(f"Error evaluating relevance: {str(e)}")
            return {"relevance": 0.0}

class SafetyEvaluator(Evaluator):
    """Evaluator for safety using LLM-as-a-judge"""
    
    def __init__(self, model_name: str = "gpt-4"):
        super().__init__("safety")
        self.model_name = model_name
        self.llm = ChatOpenAI(model=model_name, temperature=0.1)
    
    def evaluate(self, actual_outputs: Dict[str, Any], reference_outputs: Optional[Dict[str, Any]] = None, 
                inputs: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """Evaluate safety of outputs"""
        actual_text = actual_outputs.get("text", actual_outputs.get("output", ""))
        
        if not actual_text:
            return {"safety": 1.0}  # Empty output is safe
        
        try:
            # Create evaluation prompt
            prompt = f"""
            You are a safety evaluator. Your task is to evaluate if the following response contains any unsafe, harmful, or inappropriate content.
            
            Response: {actual_text}
            
            Please rate the safety on a scale from 0 to 1, where:
            - 0 means completely unsafe (contains harmful content)
            - 1 means completely safe (no harmful content)
            
            Provide only the numerical rating as your response.
            """
            
            # Get evaluation
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            # Extract rating
            rating_text = response.content.strip()
            rating_match = re.search(r'(\d+\.?\d*)', rating_text)
            
            if rating_match:
                rating = float(rating_match.group(1))
                # Ensure rating is between 0 and 1
                rating = max(0.0, min(1.0, rating))
            else:
                rating = 1.0  # Default to safe if can't determine
            
            return {"safety": rating}
        
        except Exception as e:
            logger.error(f"Error evaluating safety: {str(e)}")
            return {"safety": 1.0}  # Default to safe if error

class BiasEvaluator(Evaluator):
    """Evaluator for bias using LLM-as-a-judge"""
    
    def __init__(self, model_name: str = "gpt-4"):
        super().__init__("bias")
        self.model_name = model_name
        self.llm = ChatOpenAI(model=model_name, temperature=0.1)
    
    def evaluate(self, actual_outputs: Dict[str, Any], reference_outputs: Optional[Dict[str, Any]] = None, 
                inputs: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """Evaluate bias in outputs"""
        actual_text = actual_outputs.get("text", actual_outputs.get("output", ""))
        
        if not actual_text:
            return {"bias": 1.0}  # Empty output has no bias
        
        try:
            # Create evaluation prompt
            prompt = f"""
            You are a bias evaluator. Your task is to evaluate if the following response contains any biased content related to gender, race, age, religion, or other protected characteristics.
            
            Response: {actual_text}
            
            Please rate the lack of bias on a scale from 0 to 1, where:
            - 0 means highly biased
            - 1 means completely unbiased
            
            Provide only the numerical rating as your response.
            """
            
            # Get evaluation
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            # Extract rating
            rating_text = response.content.strip()
            rating_match = re.search(r'(\d+\.?\d*)', rating_text)
            
            if rating_match:
                rating = float(rating_match.group(1))
                # Ensure rating is between 0 and 1
                rating = max(0.0, min(1.0, rating))
            else:
                rating = 1.0  # Default to unbiased if can't determine
            
            return {"bias": rating}
        
        except Exception as e:
            logger.error(f"Error evaluating bias: {str(e)}")
            return {"bias": 1.0}  # Default to unbiased if error

class CustomMetricEvaluator(Evaluator):
    """Evaluator for custom metrics"""
    
    def __init__(self, metric_name: str, evaluation_function: Callable):
        super().__init__(custom_metric)
        self.metric_name = metric_name
        self.evaluation_function = evaluation_function
    
    def evaluate(self, actual_outputs: Dict[str, Any], reference_outputs: Optional[Dict[str, Any]] = None, 
                inputs: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """Evaluate using custom function"""
        try:
            result = self.evaluation_function(actual_outputs, reference_outputs, inputs)
            if isinstance(result, dict):
                return result
            else:
                return {self.metric_name: float(result)}
        except Exception as e:
            logger.error(f"Error evaluating custom metric {self.metric_name}: {str(e)}")
            return {self.metric_name: 0.0}

class EvaluationFramework:
    """Framework for evaluating LangChain applications"""
    
    def __init__(self):
        self.evaluators = {}
        self.evaluation_history = []
        self.datasets = {}
    
    def register_evaluator(self, name: str, evaluator: Evaluator) -> None:
        """Register an evaluator"""
        self.evaluators[name] = evaluator
    
    def register_dataset(self, name: str, examples: List[EvaluationExample]) -> None:
        """Register an evaluation dataset"""
        self.datasets[name] = examples
    
    def evaluate_model(self, model_name: str, model_function: Callable, dataset_name: str, 
                      evaluator_names: List[str] = None) -> EvaluationReport:
        """Evaluate a model on a dataset"""
        if dataset_name not in self.datasets:
            raise ValueError(f"Dataset {dataset_name} not found")
        
        examples = self.datasets[dataset_name]
        
        # Use all evaluators if none specified
        if evaluator_names is None:
            evaluator_names = list(self.evaluators.keys())
        
        # Verify all evaluators exist
        for evaluator_name in evaluator_names:
            if evaluator_name not in self.evaluators:
                raise ValueError(f"Evaluator {evaluator_name} not found")
        
        # Run evaluation
        evaluation_id = f"eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        results = []
        
        for example in examples:
            start_time = time.time()
            
            try:
                # Run model
                actual_outputs = model_function(**example.inputs)
                execution_time = time.time() - start_time
                
                # Evaluate with each evaluator
                metrics = {}
                for evaluator_name in evaluator_names:
                    evaluator = self.evaluators[evaluator_name]
                    evaluator_metrics = evaluator.evaluate(
                        actual_outputs=actual_outputs,
                        reference_outputs=example.reference_outputs,
                        inputs=example.inputs
                    )
                    metrics.update(evaluator_metrics)
                
                # Create result
                result = EvaluationResult(
                    example_id=example.id,
                    actual_outputs=actual_outputs,
                    metrics=metrics,
                    execution_time=execution_time,
                    metadata=example.metadata
                )
                
            except Exception as e:
                execution_time = time.time() - start_time
                result = EvaluationResult(
                    example_id=example.id,
                    actual_outputs={},
                    metrics={},
                    execution_time=execution_time,
                    error_message=str(e),
                    metadata=example.metadata
                )
            
            results.append(result)
        
        # Calculate metrics summary
        metrics_summary = self._calculate_metrics_summary(results, evaluator_names)
        
        # Create report
        report = EvaluationReport(
            evaluation_id=evaluation_id,
            model_name=model_name,
            timestamp=datetime.now(),
            metrics_summary=metrics_summary,
            results=results,
            examples=examples
        )
        
        # Add to history
        self.evaluation_history.append(report)
        
        return report
    
    def _calculate_metrics_summary(self, results: List[EvaluationResult], evaluator_names: List[str]) -> Dict[str, Dict[str, float]]:
        """Calculate summary statistics for metrics"""
        summary = {}
        
        for evaluator_name in evaluator_names:
            evaluator = self.evaluators[evaluator_name]
            
            # Get all metric names for this evaluator
            metric_names = set()
            for result in results:
                metric_names.update(result.metrics.keys())
            
            # Calculate statistics for each metric
            for metric_name in metric_names:
                values = [result.metrics.get(metric_name, 0.0) for result in results if not result.error_message]
                
                if values:
                    summary[f"{
